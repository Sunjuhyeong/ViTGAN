{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViTGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c249ff12c86445d81b969fd7787c365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7125728271dc4c9aa2e91a1ebb6d5868",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_75eadf8853fe44499ce646c345da73f0",
              "IPY_MODEL_5dd755d08dd540f9a5385675d91570ff",
              "IPY_MODEL_12ed0b61e72148768a0ebeb1f87d735a"
            ]
          }
        },
        "7125728271dc4c9aa2e91a1ebb6d5868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75eadf8853fe44499ce646c345da73f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9bfd8c9db2de4c7ba858cef2fd630596",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e84253a44a3e4123af0285eebd47e0dc"
          }
        },
        "5dd755d08dd540f9a5385675d91570ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76a33e14e3df40b4a53eaaed05e47603",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ced60f3dde294142bf8ff1a129cb5e62"
          }
        },
        "12ed0b61e72148768a0ebeb1f87d735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eefec5578b734ceba93ba9743f10cf73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:11&lt;00:00, 16444105.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0764223243874c57b9d3448715e56fdb"
          }
        },
        "9bfd8c9db2de4c7ba858cef2fd630596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e84253a44a3e4123af0285eebd47e0dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76a33e14e3df40b4a53eaaed05e47603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ced60f3dde294142bf8ff1a129cb5e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eefec5578b734ceba93ba9743f10cf73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0764223243874c57b9d3448715e56fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunjuhyeong/ViTGAN/blob/main/ViTGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B-00ItEsd-_"
      },
      "source": [
        "CheckList\n",
        "---------------------\n",
        "\n",
        "1.   SN, G, D 떼고 확인\n",
        "\n",
        "    -> G_simple, D_simple에 들어가는 Batch Norm의  weight init std가 0.02가 아니라 1로 설정되어있어서 값이 너무 컸다. \n",
        "\n",
        "    -> 원래 G, D를 사용할 때는 StyleGAN 논문에 따라 값을 1 로 한다.\n",
        "\n",
        "    -> 코드에서 쓰이는 batch_size=B 이지만 b_size라는 값이 다르게 있다. 이를 잘못 사용했었다. 주의하기! \n",
        "\n",
        "2.  G 만 붙이고 확인\n",
        "\n",
        "    -> CUDA out of memory라서 Batch size와 Number of Hidden layer를 줄였다.\n",
        "    이게 영향이 있을까?\n",
        "    \n",
        "    -> modulated INR 가능하면 병렬처리 하기\n",
        "\n",
        "    ->  Generator가 학습이 안된다. 해결중\n",
        "\n",
        "    -> modulate가 안되는 것 같다. -> p.data를 붙여서 해결\n",
        "\n",
        "2. (2) G\n",
        "\n",
        "    -> Backpropagation 잘 되는지 확인\n",
        "\n",
        "    -> G 중에서도 ablation 할 수 있는지 확인\n",
        "    \n",
        "    -> init 떼기 ( 큰 영향 없나 확인. 요즘은 optimizer 기법 등이 발달해서 초기화가 크게 중요하지 않다고 하셨다.)\n",
        "\n",
        "    -> for loop으로 list를 쓸 게 아니고 ModuleList 라는 걸 써야한다.\n",
        "\n",
        "\n",
        "3.  D만 붙이고 확인\n",
        "\n",
        "    -> 근데, G와 D 모델의 크기가 좀 차이나는데 이런 건 영향 없나?\n",
        "\n",
        "    -> 잘 안된다. 왤까?\n",
        "    \n",
        "    -> overlap / decoder / MLP \n",
        "\n",
        "    -> for loop으로 list를 쓸 게 아니고 ModuleList 라는 걸 써야한다.\n",
        "\n",
        "\n",
        "4.  ISN 작동시키기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD_NamTJceuf"
      },
      "source": [
        "Question\n",
        "---------------\n",
        "1.   Spectral normalization 에서, 값이 30정도로 엄청 큰데 이러면 weight가 매번 너무 커지는 게 아닌가? 내가 initial을 이상한 곳에서 계산하고 있나?\n",
        "2.   linear layer weight에 초기 spetral norm 값을 곱하는 함수 자체도 working하지 않은 것 같다.\n",
        "\n",
        "\n",
        "  (1) with no_grad?\n",
        "  \n",
        "  (2) weight.data? weight? \n",
        "  \n",
        "  (3) .copy? \n",
        "  \n",
        "  (4) net.state_dict? \n",
        "  \n",
        "\n",
        "3.   \n",
        "\n",
        "\n",
        "Memo\n",
        "*      train dataset에는 50000의 이미지가 들어있고, 이를 10개의 batch로 나누면 len(dataloader) = 5000이다. 또한 128의 batch로 나누면 len(dataloader) = 391 이다.\n",
        "\n",
        "\n",
        "*   어째서 줘도 변화가 없을까? (multiply_initial_spectral_norm 함수 참고) \n",
        "      \n",
        "    -> initial_spectral_norm을 dict로 바꿔보자.\n",
        "\n",
        "\n",
        "*   Loss를 보고, 이게 acceptable한 Loss인지 어떻게 알 수 있을까?\n",
        "      \n",
        "    -> GAN은 Loss가 우상향 또는 우하향하면 안된다. 내시 균형을 향해 가야하기 때문이다. 따라서 Loss로 학습이 잘 되는 지 판단하기는 어렵고, FID score를 도입하거나 qualitative evaluation을 할 수 밖에 없다.\n",
        "\n",
        "\n",
        "*   D * L 에 맞추려면 D, L 차원을 펼쳐야 하나? 맞게 펼쳤나?\n",
        "    \n",
        "    -> 보통은 L이 들어가지 않는다. 왜냐하면 patch별로 다르게 MLP, Attention을 적용하는 게 아니기 때문에, L 차원을 따로 고려할 필요 없다. 항상 D를 받아 D를 내는 게 encoder. (vit-pytorch 참고) \n",
        "\n",
        "*   Discriminator classification layer에서 값이 0~1로 나오려면 sigmoid를 추가해야하나?\n",
        "    \n",
        "    -> sigmoid는 쓰지 않은 상태로 BCEWithLogitsLoss를 사용하기\n",
        "    -> 번외로, multi-class 일 때는 logit 별로 softmax를 사용해야 한다. \n",
        "\n",
        "*   왜 Optimizer가 아닌 netD를 직접 zeroGrad 하나? \n",
        "\n",
        "    -> optimizer에 등록된 weight만 업데이트하냐, 혹은 전체 netD를 업데이트 하냐의 차이이지만, 이 상황에서는 차이가 없다. \n",
        "    -> 만약 하나의 network에 여러가지 optimizer가 붙어있다면, 이때는 model.zero_grad()를 호출하는 게 좋다.\n",
        "\n",
        "*   초기 weight의 Spectral norm (Largest singular value) 계산하는 방법 있나?   \n",
        "    \n",
        "    -> spectral norm soruce 코드에서 찾기\n",
        "\n",
        "\n",
        "*   Training 부분에서 출력값들의 의미\n",
        "    \n",
        "    -> D(x)는 real_label(=1.)에, D_1(G(z)) 은 fake_label(=0.)에 가까워야 한다. Discriminator 학습할 때의 값이다.\n",
        "    \n",
        "    -> D_2(G(z))은 real_label(=1.)에 가까워야 한다. \n",
        "    Generator 학습할 때의 값이다.\n",
        "\n",
        "    -> Discriminator출력에 Sigmoid를 씌우고 생각하면 된다. D(x) 등에 음수가 나와도 이상한 것이 아니다. 단 Loss는 맞게 계산되어야 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlTtUTFAZfi3"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5gxEmO5w-5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1937c62a-201c-4729-98ad-dcd726351479"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-cWGVsalaJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f18df1-5a06-41f9-9c28-7487262da7b7"
      },
      "source": [
        "!pip install easydict\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2aC9oqoZfjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8caa616-35d8-4d34-b2ca-e62e003c34f4"
      },
      "source": [
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data \n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from einops import rearrange, repeat\n",
        "from easydict import EasyDict as edict\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5f16520cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNm51HguZfjC"
      },
      "source": [
        "Inputs\n",
        "------\n",
        "\n",
        "Let’s define some inputs for the run:\n",
        "\n",
        "-  **dataroot** - the path to the root of the dataset folder. We will\n",
        "   talk more about the dataset in the next section\n",
        "-  **workers** - the number of worker threads for loading the data with\n",
        "   the DataLoader\n",
        "-  **batch_size** - the batch size used in training. The DCGAN paper\n",
        "   uses a batch size of 128\n",
        "-  **image_size** - the spatial size of the images used for training.\n",
        "   This implementation defaults to 64x64. If another size is desired,\n",
        "   the structures of D and G must be changed. See\n",
        "   `here <https://github.com/pytorch/examples/issues/70>`__ for more\n",
        "   details\n",
        "-  **nc** - number of color channels in the input images. For color\n",
        "   images this is 3\n",
        "-  **nz** - length of latent vector\n",
        "-  **ngf** - relates to the depth of feature maps carried through the\n",
        "   generator\n",
        "-  **ndf** - sets the depth of feature maps propagated through the\n",
        "   discriminator\n",
        "-  **num_epochs** - number of training epochs to run. Training for\n",
        "   longer will probably lead to better results but will also take much\n",
        "   longer\n",
        "-  **lr** - learning rate for training. As described in the DCGAN paper,\n",
        "   this number should be 0.0002\n",
        "-  **beta1** - beta1 hyperparameter for Adam optimizers. As described in\n",
        "   paper, this number should be 0.5\n",
        "-  **ngpu** - number of GPUs available. If this is 0, code will run in\n",
        "   CPU mode. If this number is greater than 0 it will run on that number\n",
        "   of GPUs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRLyYgWZfjD"
      },
      "source": [
        "H = W = 32\n",
        "\n",
        "patch_h = patch_w = 4\n",
        "\n",
        "overlap_h = patch_h // 2\n",
        "overlap_w = patch_w // 2\n",
        "\n",
        "nph = H // patch_h\n",
        "\n",
        "npw = W // patch_w \n",
        "\n",
        "B =  1 # 128 in paper\n",
        "\n",
        "L = nph * npw  \n",
        "\n",
        "# Root directory for dataset\n",
        "dataroot = \"data/CIFAR10\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = B\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "#image_size = 64\n",
        "image_size = 32\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of w latent vector (i.e. size of generator input)\n",
        "nw = 100\n",
        "\n",
        "# Number of Head of Attention Layer\n",
        "num_heads = 6\n",
        "\n",
        "# embedding dimension\n",
        "D = 384\n",
        "\n",
        "# Size of hidden dimension in MLP\n",
        "n_hidden = 300 # 1536 in paper\n",
        "\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "# num_epochs = 5\n",
        "num_epochs = 1\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.0\n",
        "beta2 = 0.99\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1\n",
        "\n",
        "# depth\n",
        "depth = L // 8\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "# print(device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS53b6nh03pn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b46055d-28c1-4c19-8eef-e83f7bea24f6"
      },
      "source": [
        "h_basis = torch.linspace(0, nph-1, nph).div(nph-1).mul(2).sub(1)\n",
        "w_basis = torch.linspace(0, npw-1, npw).div(npw-1).mul(2).sub(1)\n",
        "coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "coords2d = torch.stack(coords2d, 0).reshape(2, nph*npw).t() \n",
        "# positional_embedding_patch_position = coords2d.unsqueeze(0).repeat(B, 1, 1)\n",
        "positional_embedding_patch_position = coords2d\n",
        "positional_embedding_patch_position = positional_embedding_patch_position.to(device)\n",
        "print(positional_embedding_patch_position.shape) # [patch_positions_in_an_image, D(=2)]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn9Ze-SP-KUi"
      },
      "source": [
        "# # original Fourier embedding\n",
        "\n",
        "# h_basis = torch.linspace(0, patch_h-1, patch_h).div(patch_h-1).mul(2).sub(1)\n",
        "# w_basis = torch.linspace(0, patch_w-1, patch_w).div(patch_w-1).mul(2).sub(1)\n",
        "# coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "# coords2d = torch.stack(coords2d, 0).reshape(2, patch_h*patch_w).t()\n",
        "# coords2d = coords2d.unsqueeze(0).repeat(B, 1, 1)\n",
        "# positional_embedding_patch_pixel = coords2d.unsqueeze(1).repeat(1, L, 1, 1)\n",
        "# positional_embedding_patch_pixel = positional_embedding_patch_pixel.to(device)\n",
        "# print(positional_embedding_patch_pixel.shape) #[B, number_of_patch, h*w, D(=2)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDT_ozejOvpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3817464-a019-48de-afa7-5c60c4f936ff"
      },
      "source": [
        "h_basis = torch.linspace(0, patch_h-1, patch_h).div(patch_h-1).mul(2).sub(1)\n",
        "w_basis = torch.linspace(0, patch_w-1, patch_w).div(patch_w-1).mul(2).sub(1)\n",
        "coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "coords2d = torch.stack(coords2d, 0).reshape(2, patch_h*patch_w).t()\n",
        "positional_embedding_patch_pixel = coords2d.to(device)\n",
        "print(positional_embedding_patch_pixel.shape) #[h*w, nc(=2)]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNKLROTyZfjE"
      },
      "source": [
        "Data\n",
        "----\n",
        "\n",
        "In this tutorial we will use the `Celeb-A Faces\n",
        "dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__ which can\n",
        "be downloaded at the linked site, or in `Google\n",
        "Drive <https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`__.\n",
        "The dataset will download as a file named *img_align_celeba.zip*. Once\n",
        "downloaded, create a directory named *celeba* and extract the zip file\n",
        "into that directory. Then, set the *dataroot* input for this notebook to\n",
        "the *celeba* directory you just created. The resulting directory\n",
        "structure should be:\n",
        "\n",
        "::\n",
        "\n",
        "   /path/to/celeba\n",
        "       -> img_align_celeba  \n",
        "           -> 188242.jpg\n",
        "           -> 173822.jpg\n",
        "           -> 284702.jpg\n",
        "           -> 537394.jpg\n",
        "              ...\n",
        "\n",
        "This is an important step because we will be using the ImageFolder\n",
        "dataset class, which requires there to be subdirectories in the\n",
        "dataset’s root folder. Now, we can create the dataset, create the\n",
        "dataloader, set the device to run on, and finally visualize some of the\n",
        "training data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIYU1lT2ZfjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "8c249ff12c86445d81b969fd7787c365",
            "7125728271dc4c9aa2e91a1ebb6d5868",
            "75eadf8853fe44499ce646c345da73f0",
            "5dd755d08dd540f9a5385675d91570ff",
            "12ed0b61e72148768a0ebeb1f87d735a",
            "9bfd8c9db2de4c7ba858cef2fd630596",
            "e84253a44a3e4123af0285eebd47e0dc",
            "76a33e14e3df40b4a53eaaed05e47603",
            "ced60f3dde294142bf8ff1a129cb5e62",
            "eefec5578b734ceba93ba9743f10cf73",
            "0764223243874c57b9d3448715e56fdb"
          ]
        },
        "outputId": "cc85da2a-cc46-47a5-9fa8-68ed0028b7a1"
      },
      "source": [
        "# # We can use an image folder dataset the way we have it setup.\n",
        "# # Create the dataset\n",
        "# dataset = dset.ImageFolder(root=dataroot,\n",
        "#                            transform=transforms.Compose([\n",
        "#                                transforms.Resize(image_size),\n",
        "#                                transforms.CenterCrop(image_size),\n",
        "#                                transforms.ToTensor(),\n",
        "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                            ]))\n",
        "# # Create the dataloader\n",
        "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "#                                          shuffle=True, num_workers=workers)\n",
        "\n",
        "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
        "training_data = dset.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
        "test_data = dset.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# 데이터로더를 생성합니다.\n",
        "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break\n",
        "\n",
        "dataloader = train_dataloader\n",
        "\n",
        "# Plot some training images\n",
        "# real_batch = next(iter(train_dataloader))\n",
        "# plt.figure(figsize=(8,8))\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Training Images\")\n",
        "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0))/\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c249ff12c86445d81b969fd7787c365",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Shape of X [N, C, H, W]:  torch.Size([1, 3, 32, 32])\n",
            "Shape of y:  torch.Size([1]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJfQGbOb7diQ"
      },
      "source": [
        "class PixelNormLayer(nn.Module):\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_W6GmCTZfjF"
      },
      "source": [
        "# Generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf9PqtO0ZfjG"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "# 초기화 방법이 나와있지 않다.\n",
        "# styleGan에선 N(0,1) 을 따르게 했다.\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        print(classname)\n",
        "        nn.init.normal_(m.weight.data, 0.0, (0.1 * (2 ** 0.5)))    \n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG8L6LVPWyd_"
      },
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "  def __init__(self, in_features, out_features, n_hidden):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "\n",
        "        self.pixel_norm = PixelNormLayer()\n",
        "        self.mapping_network = nn.Sequential(\n",
        "          nn.Linear(in_features=in_features, out_features=n_hidden, bias=True),\n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),    \n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=out_features, bias=True),    \n",
        "          # nn.LeakyReLU(negative_slope=0.01)\n",
        "        )\n",
        "        \n",
        "        # -------Code for check the value of layer parameters & z -----------\n",
        "        # self.linear1 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.linear2 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.linear3 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.linear4 = nn.Linear(in_features=in_features, out_features=n_hidden, bias=True)\n",
        "        # self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        # -------------------------------------------------------------------\n",
        "\n",
        "  def forward(self, input):\n",
        "        input = self.pixel_norm(input)\n",
        "\n",
        "        # -------Code for check the value of layer parameters & z -----------\n",
        "        # input = self.linear1(input)\n",
        "        # input = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # print(\"linear 1\", self.linear1.weight)\n",
        "        # input = self.linear2(input)\n",
        "        # input = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # input = self.linear3(input)\n",
        "        # input = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # input = self.linear4(input)\n",
        "        # output = self.relu(input)\n",
        "        # print(\"input\", input)\n",
        "        # -------------------------------------------------------------------\n",
        "        output = self.mapping_network(input)\n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jtRsFUMUsuJ"
      },
      "source": [
        "class SLN(nn.Module):\n",
        "  def __init__(self, w, fn=None):\n",
        "        super(SLN, self).__init__()\n",
        "        self.w = w\n",
        "        self.gamma = nn.Linear(in_features=nw, out_features=D, device=device)\n",
        "        self.beta = nn.Linear(in_features=nw, out_features=D, device=device) \n",
        "        self.fn = fn\n",
        "\n",
        "  def forward(self, input, **kwargs):\n",
        "        # input is positional embedding of patch position\n",
        "        mean = torch.mean(input, dim=-1, keepdim=True)\n",
        "        variance = torch.var(input, dim=-1, keepdim=True)\n",
        "        # print(\"input of SLN\", input.size()) # L D\n",
        "        self.w = self.w.unsqueeze(1) # B 1 nz\n",
        "        gamma = self.gamma(self.w) # B 1 D\n",
        "        beta = self.beta(self.w) # B 1 D\n",
        "\n",
        "        # print(\"input: \\n\", input.size())\n",
        "        # print(\"gamma: \\n\", gamma.size())\n",
        "        # print(\"beta: \\n\", beta.size())\n",
        "        # print(\"mean: \\n\", mean.size())\n",
        "        # print(\"variance: \\n\", variance.size())\n",
        "        output = gamma * ((input - mean) / variance) + beta\n",
        "        if self.fn != None:\n",
        "          return self.fn(output, **kwargs)\n",
        "        else:\n",
        "          return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpNyWsU4bikA"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "          nn.Linear(in_features=in_features, out_features=n_hidden, bias=True, device=device),\n",
        "          nn.LeakyReLU(negative_slope=0.01),\n",
        "          nn.Linear(in_features=n_hidden, out_features=out_features, bias=True, device=device),    \n",
        "          # nn.LeakyReLU(negative_slope=0.01)\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "        output = self.mlp(input)\n",
        "        return output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DauDkC8xaXuO"
      },
      "source": [
        "# class GeneratorEncoderBlock(nn.Module):\n",
        "#   def __init__(self, embed_dim, num_heads, w):\n",
        "#         super(GeneratorEncoderBlock, self).__init__()\n",
        "#         self.w = w\n",
        "#         self.sln1 = SLN(self.w)\n",
        "#         self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, device=device)\n",
        "#         self.sln2 = SLN(self.w)\n",
        "#         self.mlp = MLP(in_features=embed_dim, out_features=embed_dim).to(device)\n",
        "\n",
        "#   def forward(self, input):            \n",
        "#         # input is positional embedding of patch position\n",
        "#         # print(\"input\",input.size())\n",
        "#         temp = self.sln1(input)\n",
        "#         # print(\"sln1\",temp.size())\n",
        "#         temp, _ = self.attention.forward(query=temp, key=temp, value=temp)\n",
        "#         # print(\"attention\",temp[0].size())\n",
        "#         temp2 = temp + input\n",
        "#         output = self.sln2(temp2)\n",
        "#         # print(\"sln2\",temp.size())\n",
        "#         output = self.mlp(output)\n",
        "#         # print(\"mlp\",output.size())\n",
        "#         output = output + temp2\n",
        "#         return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDaupxPHdLJg"
      },
      "source": [
        "class GeneratorEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, depth, w):\n",
        "        super(GeneratorEncoder, self).__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "          self.encoder_blocks.append(\n",
        "              nn.ModuleList([\n",
        "                SLN(w, None),\n",
        "                nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, device=device),\n",
        "                SLN(w, MLP(in_features=embed_dim, out_features=embed_dim))\n",
        "            ])\n",
        "          )\n",
        "        self.sln = SLN(w=w, fn=None)\n",
        "\n",
        "  def forward(self, x):\n",
        "        for sln, attn, ff in self.encoder_blocks:\n",
        "            temp = sln(x)\n",
        "            temp, _ = attn(temp, temp, temp) \n",
        "            x = temp + x\n",
        "            x = ff(x) + x  \n",
        "        output = self.sln(x)\n",
        "        return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwU47cnEeX92"
      },
      "source": [
        "class PositionalEmbeddingLayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(PositionalEmbeddingLayer, self).__init__()\n",
        "        self.positional_embedding_layer = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "  def forward(self, input):\n",
        "        output = self.positional_embedding_layer(input)\n",
        "        return output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7MqEr5_h9x9"
      },
      "source": [
        "class FourierEmbeddingLayer(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(FourierEmbeddingLayer, self).__init__()\n",
        "        # self.fourier_embedding_layer = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        self.fourier_embedding_layer = nn.Parameter(torch.randn([in_features, out_features], device=device), requires_grad=True)\n",
        "  def forward(self, input):\n",
        "        # print(\"fourier input\", input)\n",
        "        output = input @ self.fourier_embedding_layer # 16, 2 -> 16, D\n",
        "        # print(\"fourier output\", output)\n",
        "        output = torch.sin(output)\n",
        "        # print(\"fourier sine output\", output)\n",
        "        return output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gUignvDll9R"
      },
      "source": [
        "INR\n",
        "----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0mB7aH_kN77"
      },
      "source": [
        "class ImplicitNeuralNetwork(nn.Module):\n",
        "  def __init__(self, embed_dim, n_hidden):\n",
        "        super(ImplicitNeuralNetwork, self).__init__()\n",
        "        self.y = y\n",
        "        self.linear1 = nn.Linear(in_features=embed_dim, out_features=n_hidden, device=device)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.02)\n",
        "        self.linear2 = nn.Linear(in_features=n_hidden, out_features=nc, device=device) \n",
        "        self.lrelu2 = nn.LeakyReLU(0.02)\n",
        "        self.implicit_neural_network = nn.Sequential(\n",
        "          self.linear1,\n",
        "          self.lrelu1,\n",
        "          self.linear2,\n",
        "          # self.lrelu2\n",
        "        )\n",
        "\n",
        "  def forward(self, input):\n",
        "        # print(\"input size\", input.size())\n",
        "        # output = self.implicit_neural_network(input)\n",
        "        # print(\"input of INR\", input)\n",
        "        input = self.linear1(input)\n",
        "        # print(\"linear1 weight\", self.linear1.weight.data.size(), self.linear1.weight.data)\n",
        "        input = self.lrelu1(input)\n",
        "        # print(\"after linear 1\", input)\n",
        "        output = self.linear2(input)\n",
        "        # output = self.lrelu2(input)\n",
        "        # print(\"linear2 weight\", self.linear2.weight.data.size(), self.linear2.weight.data)\n",
        "        # print(\"after linear 2\", output)\n",
        "        # print(\"output size\", output.size())\n",
        "        return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thVbRCgllqel"
      },
      "source": [
        "def modulate_inr (s1, s2, inr, epsilon):\n",
        "  for name, p in inr.named_parameters():\n",
        "    if name == 'linear1.weight':\n",
        "      # s1 = repeat(s1, 'B L D -> B L n_hidden D', B=B, L=L, D=D, n_hidden=n_hidden)\n",
        "      # s1 = repeat(s1, 'L D -> L n_hidden D', L=L, D=D, n_hidden=n_hidden)\n",
        "      s1 = repeat(s1, 'D -> n_hidden D', D=D, n_hidden=n_hidden)\n",
        "      # print(\"p size\", p.size())\n",
        "      # print(\"s1 size\", s1.size())\n",
        "      temp = torch.sum(s1*s1*p.data*p.data, dim=-1)\n",
        "      # print(\"temp\", temp)\n",
        "      # print(\"temp size\", temp.size())\n",
        "      # temp = repeat(temp, 'B L n_hidden -> B L  n_hidden D', B=B, L=L, D=D, n_hidden=n_hidden)\n",
        "      # temp = repeat(temp, 'L n_hidden -> L n_hidden D', L=L, D=D, n_hidden=n_hidden)\n",
        "      temp = repeat(temp, ' n_hidden -> n_hidden D', D=D, n_hidden=n_hidden)\n",
        "      # print(\"p\",p)\n",
        "      # print(\"s1\", s1)\n",
        "      # print(\"dividor\", torch.sqrt(epsilon + temp))\n",
        "      p.data = p.data * s1 / torch.sqrt(epsilon + temp)   \n",
        "      # print(\"p after\",p)   \n",
        "    elif  name == 'linear2.weight': \n",
        "      # s2 = repeat(s2, 'B L n_hidden -> B L nc n_hidden', B=B, L=L, nc=nc, n_hidden=n_hidden)\n",
        "      # s2 = repeat(s2, 'L n_hidden -> L nc n_hidden', L=L, nc=nc, n_hidden=n_hidden)\n",
        "      s2 = repeat(s2, 'n_hidden -> nc n_hidden', nc=nc, n_hidden=n_hidden)\n",
        "      temp = torch.sum(s2*s2*p.data*p.data, dim=-1)\n",
        "      # temp = repeat(temp, 'B L nc -> B L nc n_hidden', B=B, L=L, nc=nc, n_hidden=n_hidden)\n",
        "      # temp = repeat(temp,  'L nc -> L nc n_hidden', L=L, nc=nc, n_hidden=n_hidden)\n",
        "      temp = repeat(temp,  'nc -> nc n_hidden', nc=nc, n_hidden=n_hidden)\n",
        "      p.data = p.data * s2 / torch.sqrt(epsilon + temp)\n",
        "  return inr  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp0aS0yKlIe7"
      },
      "source": [
        "class ModulatedINR(nn.Module):\n",
        "  def __init__(self, embed_dim, n_hidden, patch_h, patch_w, y):\n",
        "        super(ModulatedINR, self).__init__()\n",
        "        # modulated MLP\n",
        "        self.y = y\n",
        "        self.epsilon = 1e-8\n",
        "        self.patch_h = patch_h\n",
        "        self.patch_w = patch_w\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_hidden = n_hidden\n",
        "        self.linear1 = nn.Linear(embed_dim, embed_dim, device=device)\n",
        "        self.linear2 = nn.Linear(embed_dim, n_hidden, device=device)\n",
        "        self.inr = ImplicitNeuralNetwork(embed_dim=embed_dim, n_hidden=n_hidden)\n",
        "        self.modulated_inr = copy.deepcopy(self.inr).to(device)\n",
        "        self.output = torch.zeros([B, L, (patch_h*patch_w), nc], device=device)\n",
        "\n",
        "  def forward(self, input): \n",
        "        # print(\"y\", self.y)\n",
        "        s1 = self.linear1(self.y) # D to D\n",
        "        # print(\"s1\", s1)\n",
        "\n",
        "        s2 = self.linear2(self.y) # D to n_hidden\n",
        "        # print(\"s2\", s2)\n",
        "\n",
        "        # print(s1.size()) # B L D\n",
        "        # print(s2.size()) # B L n_hidden\n",
        "        \n",
        "        # 일단 for loop으로 하고 나중에 병렬로 바꿔보기\n",
        "        for i in range(B): # B\n",
        "          for j in range(L): # L\n",
        "            self.modulated_inr = copy.deepcopy(self.inr).to(device)\n",
        "            new_modulated_inr = modulate_inr(s1[i][j], s2[i][j], self.modulated_inr, self.epsilon)\n",
        "            self.output[i][j] = new_modulated_inr(input)\n",
        "        # print(\"output\", self.output.size(), self.output)\n",
        "        return self.output     "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofPyfVWlYxc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b4b5ec-a130-4a32-810d-5fbcdd13a27b"
      },
      "source": [
        "a = torch.zeros([1])\n",
        "print(a)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0380PAP__fKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2059a6b-916c-427d-ef94-63b00c3a8308"
      },
      "source": [
        "class Test(nn.Module):\n",
        "  def __init__(self, y):\n",
        "      super(Test, self).__init__()\n",
        "      self.y = y\n",
        "      self.linear = nn.Linear(1, 1)\n",
        "      self.linear2 = nn.Linear(1, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.linear(self.y) # B L D\n",
        "    print(x.size())\n",
        "    y = self.linear2(input) # npwnph D\n",
        "    return x\n",
        "\n",
        "y = torch.ones([3,2,1]) # B L D\n",
        "a = Test(y)\n",
        "input = torch.ones([4, 1]) # B L npwnph D\n",
        "a(input)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0238],\n",
              "         [-0.0238]],\n",
              "\n",
              "        [[-0.0238],\n",
              "         [-0.0238]],\n",
              "\n",
              "        [[-0.0238],\n",
              "         [-0.0238]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSi83DUyZfjH"
      },
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.mapping_network = MappingNetwork(in_features=nz, out_features=nw, n_hidden=nz)\n",
        "        self.fourier_embedding_layer = FourierEmbeddingLayer(in_features=2, out_features=D)\n",
        "        self.positional_embedding_layer = PositionalEmbeddingLayer(in_features=2, out_features=D)\n",
        "        self.inr_list = []\n",
        "        self.output_patches = []\n",
        "\n",
        "    def forward(self, z):\n",
        "        # print(\"Z\", z)\n",
        "        w = self.mapping_network(z)\n",
        "        print(\"w\", w)\n",
        "        generator_encoder = GeneratorEncoder(embed_dim=D, num_heads=num_heads, depth=depth, w=w)\n",
        "        positional_embedding = self.positional_embedding_layer(positional_embedding_patch_position) # [L, D]\n",
        "        y = generator_encoder(positional_embedding) \n",
        "        print(\"y, y size\", y, y.size()) # [B, L, D]\n",
        "        fourier_embedding = self.fourier_embedding_layer(positional_embedding_patch_pixel) # [(patch_h patch_w), D]\n",
        "        # patch = torch.zeros([L, patch_h*patch_w, nc], device=device, requires_grad=False) # requires_grad=False가 맞을까? \n",
        "        # print(\"fourier_embedding , size \", fourier_embedding, fourier_embedding.size()) # [B, L, D]\n",
        "    \n",
        "        self.inr_list = ModulatedINR(embed_dim=D, n_hidden=n_hidden, patch_h=patch_h, patch_w=patch_w, y=y) \n",
        "        patch = self.inr_list(fourier_embedding) # phpw D\n",
        "        print(\"patch\", patch, patch.size()) #\n",
        "        \n",
        "        output = rearrange(patch, 'B (nph npw) (patch_h patch_w) nc -> B nc (nph patch_h) (npw patch_w)', B=B, nph=nph, npw=npw, patch_h=patch_h, patch_w=patch_w, nc=nc) # 맞게 rearrange 한걸까?\n",
        "        # B nc H W\n",
        "        print(\"output after rearrange\", output, output.size()) # [B, L, D]\n",
        "        # output = rearrange(patch, '(nph npw) (patch_h patch_w) nc -> nc (nph patch_h) (npw patch_w)', nph=nph, npw=npw, patch_h=patch_h, patch_w=patch_w, nc=nc) # 맞게 rearrange 한걸까?\n",
        "        output = torch.sigmoid(output)\n",
        "        print(\"output after sigmoid\", patch, patch.size()) # [B, L, D]\n",
        "        return output"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRuGISepZfjI"
      },
      "source": [
        "NetG\n",
        "---------------\n",
        "\n",
        ", we can instantiate the generator and apply the ``weights_init``\n",
        "function. Check out the printed model to see how the generator object is\n",
        "structured.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLvg3sLLEwrK"
      },
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator_Simple(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator_Simple, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            # nn.BatchNorm2d(nc),\n",
        "            # nn.ReLU(True),\n",
        "            nn.Tanh()\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            # nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        # print(\"output of generator\", output)\n",
        "        return output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiuVpVegZfjI"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "# netG = Generator_Simple(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "# netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "# print(netG)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzhMUNU6Ck5V"
      },
      "source": [
        "# ## Train with all-fake batch\n",
        "# # Generate batch of latent vectors\n",
        "# noise = torch.randn(B, nz, device=device) \n",
        "# # noise = torch.randn(b_size, nz, 1, 1, device=device)  # Noise for simple network\n",
        "# # Generate fake image batch with G\n",
        "# fake = netG(noise)\n",
        "# # print(fake.size())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHN0EbRHTuMP"
      },
      "source": [
        "# Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-DE5In3aayn"
      },
      "source": [
        "class ProjectingPatches(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "        super(ProjectingPatches, self).__init__()\n",
        "        self.project_layer = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "\n",
        "  def forward(self, input):\n",
        "        output = self.project_layer(input)\n",
        "        return output"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkpQMHfVoz55"
      },
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn=None):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        if self.fn != None:\n",
        "          return self.fn(self.norm(x), **kwargs)\n",
        "        else:\n",
        "          return self.norm(x)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFeZAJNCPEll"
      },
      "source": [
        "##### Transformer\n",
        "\n",
        "args = edict()\n",
        "MAX_LEN = 100\n",
        "\n",
        "class NewMultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked attention layer with a projection at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, device, mask=False):\n",
        "        super(NewMultiheadAttention, self).__init__()\n",
        "        # mask : whether to use \n",
        "        # key, query, value projections for all heads\n",
        "        args.nhid_tran = embed_dim\n",
        "        args.nhead = num_heads\n",
        "        self.key = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        self.query = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        self.value = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(args.nhid_tran, args.nhid_tran, device=device)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        if mask:\n",
        "            self.register_buffer(\"mask\", torch.tril(torch.ones(MAX_LEN, MAX_LEN, device=device)))\n",
        "        self.nhead = args.nhead\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        B = q.size(0)\n",
        "        T_q = q.size(1)\n",
        "        T = k.size(1)\n",
        "        nhead = self.nhead\n",
        "        hidden_size =  args.nhid_tran\n",
        "        d_k = hidden_size // nhead\n",
        "        q = self.query(q)\n",
        "        k = self.key(k)\n",
        "        v = self.value(v)\n",
        "        q = q.view(B, T_q, nhead, d_k)\n",
        "        k = k.view(B, T, nhead, d_k)\n",
        "        v = k.view(B, T, nhead, d_k)\n",
        "        q = torch.transpose(q, 1, 2)\n",
        "        k = torch.transpose(k, 1, 2)\n",
        "        v = torch.transpose(v, 1, 2)\n",
        "        \n",
        "        temp = torch.cdist(q.detach().clone(), k.detach().clone()) #(B,nhead,T_q,T) /  vectorized Euclidean L2 distances\n",
        "        temp = temp / (d_k ** 0.5)\n",
        "        if hasattr(self, 'mask'):\n",
        "          mask2 = self.mask[:T_q, :T].to(torch.bool)\n",
        "          mask2 = torch.unsqueeze(mask2, 0).expand(nhead, T_q, T)\n",
        "          mask2 = torch.unsqueeze(mask2, 0).expand(B, nhead, T_q, T)\n",
        "          temp = temp.masked_fill(mask2==0, float('-inf'))\n",
        "        if mask != None:\n",
        "          mask = torch.unsqueeze(mask, 1).expand(B, T_q, T)\n",
        "          mask = torch.unsqueeze(mask, 1).expand(B, nhead, T_q, T)\n",
        "          temp = temp.masked_fill(mask==0, float('-inf'))\n",
        "        temp = torch.nn.functional.softmax(temp, dim=-1)\n",
        "        outputs = torch.matmul(temp, v)\n",
        "        outputs = torch.transpose(outputs, 1, 2)\n",
        "        outputs = torch.reshape(outputs, (B, T_q, hidden_size))\n",
        "        outputs = self.proj(outputs)\n",
        "        return outputs"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkA_jTRfZ8JG"
      },
      "source": [
        "# class DiscriminatorEncoderBlock(nn.Module):\n",
        "#   def __init__(self, embed_dim):\n",
        "#         super(DiscriminatorEncoderBlock, self).__init__()\n",
        "#         self.ln1 = nn.LayerNorm(normalized_shape=D) \n",
        "#         self.attention = NewMultiheadAttention(embed_dim=embed_dim, num_heads=num_heads).to(device)\n",
        "#         self.ln2 = nn.LayerNorm(normalized_shape=D) \n",
        "#         self.mlp = MLP(in_features=embed_dim, out_features=embed_dim).to(device)\n",
        "\n",
        "#   def forward(self, input):                \n",
        "#         temp = self.ln1(input)\n",
        "#         temp = self.attention.forward(temp, temp, temp)\n",
        "#         temp = temp + input\n",
        "#         output = self.ln2(temp)\n",
        "#         output = self.mlp(output)\n",
        "#         output = output + temp\n",
        "#         return output        "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw4cizhwW5Eq"
      },
      "source": [
        "class DiscriminatorEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, depth):\n",
        "        super(DiscriminatorEncoder, self).__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "          self.encoder_blocks.append(\n",
        "              nn.ModuleList([\n",
        "                PreNorm(embed_dim),\n",
        "                NewMultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, device=device),\n",
        "                PreNorm(embed_dim, MLP(in_features=embed_dim, out_features=embed_dim))\n",
        "            ])\n",
        "          )\n",
        "\n",
        "  def forward(self, x):\n",
        "        for sln, attn, ff in self.encoder_blocks:\n",
        "            temp = sln(x)\n",
        "            temp = attn(temp, temp, temp) \n",
        "            x = temp + x\n",
        "            x = ff(x) + x  \n",
        "        return x"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iG_m8GSvSXM"
      },
      "source": [
        "def overlap_input(img):\n",
        "  # img = torch.zeros([B, nc, H, W]) -> 맞춰주기!\n",
        "  padding = (overlap_w, overlap_w, overlap_h, overlap_h)\n",
        "  img = nn.functional.pad(img, padding)\n",
        "  stride_h = patch_h\n",
        "  stride_w = patch_w\n",
        "  img_patches = img.unfold(2, patch_h+2*overlap_h, stride_h).unfold(3, patch_w+2*overlap_w, stride_w) \n",
        "  # [B, nc, nph, npw, p_h+2*o_h, p_w+2*o_w]\n",
        "  return img_patches\n",
        "\n",
        "# img = torch.zeros([B, nc, H, W]) \n",
        "# overlap_input(img).size()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3RZHtvjZfjK"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "\n",
        "        self.class_embedding = torch.zeros([1, 1, D], dtype=torch.float, requires_grad=True, device=device)\n",
        "        self.position_zero = torch.zeros([1, 1, D], dtype=torch.float, requires_grad=True, device=device)\n",
        "        self.class_embedding = repeat(self.class_embedding, '1 1 D -> B 1 D', B=B, D=D)\n",
        "        self.position_zero = repeat(self.position_zero, '1 1 D -> B 1 D', B=B, D=D)\n",
        "        self.projecting_patches = ProjectingPatches(in_features=(patch_h + 2*overlap_h) * (patch_w + 2*overlap_w) * nc, out_features=D).to(device)\n",
        "        self.positional_embedding_layer = PositionalEmbeddingLayer(in_features=2, out_features=D).to(device)\n",
        "        self.encoder = DiscriminatorEncoder(D, num_heads, depth).to(device)\n",
        "        self.output_layer = nn.Linear(in_features=D, out_features=1, device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input = rearrange(input, 'B H W nc -> B nc H W', B=B, nc=nc, H=H, W=W)\n",
        "        # print(\"input\", input)\n",
        "        patches = overlap_input(input) # [B, nc, nph, npw, p_h+2*o_h, p_w+2*o_w]\n",
        "        # print(\"patches\", patches)\n",
        "        \n",
        "        # for patch in patches:\n",
        "        flatten_patches = rearrange(patches, 'B nc nph npw patch_h patch_w -> B (nph npw) (patch_h patch_w nc)', B=patches.size(0), nph=nph, npw=npw, patch_h=patch_h+2*overlap_h, patch_w=patch_w+2*overlap_w, nc=nc)\n",
        "        # print(\"flatten_patches\", flatten_patches)\n",
        "        projected_flatten_patches = self.projecting_patches(flatten_patches) # [B, L, h*w*nc] -> [B, L, D]\n",
        "        # print(\"projected_flatten_patches\", projected_flatten_patches)\n",
        "        patch_embedding = torch.cat([self.class_embedding, projected_flatten_patches], dim=1) # [B, L, D] -> [B, L+1, D] \n",
        "        # print(\"patch_embedding\", patch_embedding)\n",
        "        \n",
        "        positional_embedding = self.positional_embedding_layer(positional_embedding_patch_position) # [B, L+1, D] 가 되어야하는데\n",
        "        positional_embedding = repeat(positional_embedding, 'L D -> B L D', B=B, L=L, D=D)\n",
        "        # print(\"positional_embedding\", positional_embedding)\n",
        "        positional_embedding = torch.cat([self.position_zero, positional_embedding], dim=1) # self.position_zero 맞게 init 했나? dim=1이 맞나?\n",
        "        # print(\"positional_embedding\", positional_embedding)\n",
        "        \n",
        "        tokens = positional_embedding + patch_embedding\n",
        "        # print(\"tokens\", tokens.size())\n",
        "        y = self.encoder(tokens) # [B, L+1, D]\n",
        "        # print(\"y\", y)\n",
        "        index = torch.tensor([0]).to(device)\n",
        "        print(\"y size\", y.size())\n",
        "        output_token = torch.index_select(y, dim=1, index=index) # [B, 1, D]\n",
        "        # print(\"output_token\", output_token)\n",
        "        output = self.output_layer(output_token) # [B, 1, 1]\n",
        "        # print(\"output in discriminator\", output)\n",
        "        # output = torch.sigmoid(output)\n",
        "        # print(\"output in discriminator after sigmoid\", output)\n",
        "        return output"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pbkzm1oNiqL"
      },
      "source": [
        "Spectral Normalization \n",
        "-----------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF_hJwZFyLDb"
      },
      "source": [
        "initial_spectral_norm = {}\n",
        "\n",
        "def spectral_normalization(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    nn.utils.parametrizations.spectral_norm(m)\n",
        "\n",
        "def calculate_initial_spectral_norm(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    global initial_spectral_norm\n",
        "    initial_spectral_norm[m.in_features] = torch.linalg.svdvals(m.weight.data)[0] \n",
        "\n",
        "def multiply_initial_spectral_norm(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    global initial_spectral_norm \n",
        "    print(\"weight before\", m.weight.data)\n",
        "    print(\"multiply this value\", initial_spectral_norm[m.in_features])\n",
        "    # with torch.no_grad():\n",
        "    m.weight.data = torch.mul(m.weight.data , initial_spectral_norm[m.in_features])\n",
        "    print(\"weight after\", m.weight.data)\n",
        "\n",
        "def print_weight(m):\n",
        "  if isinstance(m, (nn.Linear,)):\n",
        "    print(\"name, weight[0] of linear layer\", m.name, m.weight.data[0])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQMqlC1uEwrN"
      },
      "source": [
        "class Discriminator_Simple(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator_Simple, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # # input is (nc) x 64 x 64\n",
        "            # nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            # nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # print(input.size())\n",
        "        return self.main(input)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-mxs5ZMZfjL"
      },
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "# netD = Discriminator_Simple(ngpu).to(device)\n",
        "\n",
        "# Handle multi-gpu if desired\n",
        "if (device.type == 'cuda') and (ngpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "    \n",
        "# Apply the weights_init function to randomly initialize all weights\n",
        "#  to mean=0, stdev=0.2.\n",
        "# netD.apply(weights_init)\n",
        "\n",
        "# Spectral Normalization\n",
        "\n",
        "# netD.apply(calculate_initial_spectral_norm)\n",
        "# netD.apply(spectral_normalization)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC7H74H-ZfjL"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, as with the generator, we can create the discriminator, apply the\n",
        "``weights_init`` function, and print the model’s structure.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbj9hskJZfjM"
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(B, nz, device=device)\n",
        "# fixed_noise = torch.randn(64, nz, 1, 1, device=device) # Noise for simple network\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P60ZcFIofDa7"
      },
      "source": [
        "net1 = nn.Linear(10, 20)\n",
        "net2 = nn.Linear(10, 20)\n",
        "\n",
        "params = [\n",
        "    {'params':net1.parameters(), 'lr':0.1},\n",
        "    {'params':net2.parameters(), 'lr':0.01},\n",
        "    {'params':net2.parameters()},\n",
        "]\n",
        "\n",
        "# optimizer(params, lr=lr)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XMAoW2hOZfjP",
        "outputId": "ad319390-d388-4e8a-9d61-b1c399982e79"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "# Setting for debug\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "# torch.autograd.detect_anomaly\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        ###########################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0]\n",
        "        real_cpu = real_cpu.to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        \n",
        "        # Improved Spectral_Normalization\n",
        "        # netD.apply(multiply_initial_spectral_norm)\n",
        "        \n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        \n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, device=device) \n",
        "        # noise = torch.randn(b_size, nz, 1, 1, device=device)  # Noise for simple network\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "\n",
        "        # Improved Spectral_Normalization\n",
        "        # netD.apply(multiply_initial_spectral_norm)\n",
        "\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "\n",
        "        # Improved Spectral_Normalization\n",
        "        # netD.apply(multiply_initial_spectral_norm)\n",
        "\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        \n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            \n",
        "        iters += 1"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0054,  0.0309,  0.0566, -0.0019, -0.0023, -0.0564,  0.0458, -0.0420,\n",
            "          0.0926,  0.0021,  0.0883, -0.0879,  0.0798, -0.0356, -0.0578, -0.0790,\n",
            "         -0.0446, -0.0419,  0.0717,  0.0943, -0.0310, -0.0353,  0.0186,  0.0474,\n",
            "          0.0861, -0.0109, -0.0109,  0.0747,  0.0651,  0.0384,  0.0337, -0.0672,\n",
            "          0.1224, -0.0419,  0.0602,  0.1092, -0.1072,  0.0864, -0.0463,  0.0131,\n",
            "          0.0619, -0.0632,  0.0418,  0.0516,  0.0148,  0.0522, -0.1055,  0.0890,\n",
            "          0.0577,  0.0063,  0.0344, -0.0827, -0.0442, -0.0623,  0.0776,  0.0306,\n",
            "          0.0475,  0.0858,  0.0232,  0.0807,  0.0369, -0.0418,  0.0404,  0.0354,\n",
            "         -0.0833, -0.0109, -0.0394, -0.0771, -0.0833,  0.0055, -0.0109,  0.0578,\n",
            "          0.0280,  0.0514,  0.0317,  0.0022,  0.0847, -0.0906, -0.0008, -0.0813,\n",
            "          0.0426, -0.0580,  0.0480,  0.1131, -0.0685,  0.0744, -0.0547, -0.0879,\n",
            "          0.0295,  0.0209,  0.0440,  0.0138,  0.0443,  0.0761, -0.1088, -0.0896,\n",
            "          0.0247, -0.0110, -0.0774,  0.0158]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.1199,  0.0292,  0.0754,  ...,  0.1837, -0.1014,  0.1383],\n",
            "         [ 0.1132,  0.0263,  0.0756,  ...,  0.1835, -0.1120,  0.1239],\n",
            "         [ 0.1020,  0.0257,  0.0751,  ...,  0.1717, -0.1239,  0.0940],\n",
            "         ...,\n",
            "         [ 0.0658,  0.1702,  0.0341,  ..., -0.1110, -0.0854, -0.1792],\n",
            "         [ 0.0581,  0.1650,  0.0373,  ..., -0.1106, -0.0955, -0.1916],\n",
            "         [ 0.0555,  0.1589,  0.0411,  ..., -0.1013, -0.1030, -0.1836]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 4.2615e-01, -4.3235e-01,  2.4338e-01],\n",
            "          [ 2.7061e-01, -3.6225e-01,  4.5866e-01],\n",
            "          [ 2.9539e-01,  2.2266e-03,  5.6135e-01],\n",
            "          ...,\n",
            "          [ 6.5063e-01, -3.0302e-01, -2.8172e-01],\n",
            "          [ 2.4756e-01,  6.9122e-02,  4.4629e-01],\n",
            "          [ 4.3046e-02,  4.4949e-01,  3.0779e-02]],\n",
            "\n",
            "         [[ 4.8281e-01, -5.2169e-01,  2.1174e-01],\n",
            "          [ 3.4759e-01, -4.2217e-01,  4.3883e-01],\n",
            "          [ 3.6248e-01, -3.1037e-02,  5.4616e-01],\n",
            "          ...,\n",
            "          [ 7.3529e-01, -3.1398e-01, -2.2366e-01],\n",
            "          [ 4.3207e-01,  3.4243e-02,  4.4929e-01],\n",
            "          [ 2.7604e-01,  3.6906e-01, -4.7540e-03]],\n",
            "\n",
            "         [[ 4.9273e-01, -6.2897e-01,  8.7259e-02],\n",
            "          [ 4.3936e-01, -4.9963e-01,  3.6768e-01],\n",
            "          [ 4.4133e-01, -8.6695e-02,  5.2165e-01],\n",
            "          ...,\n",
            "          [ 8.3303e-01, -3.2499e-01, -1.8697e-01],\n",
            "          [ 6.6913e-01,  3.1529e-03,  3.8436e-01],\n",
            "          [ 5.3209e-01,  3.2837e-01, -5.5269e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.2623e-02, -1.7966e+00, -7.5558e-01],\n",
            "          [ 1.1711e-01, -7.5718e-01, -7.1333e-01],\n",
            "          [ 4.7642e-01,  9.8976e-02, -5.9192e-01],\n",
            "          ...,\n",
            "          [-3.3422e-01,  5.7449e-02, -1.5275e-01],\n",
            "          [-1.8565e-01,  4.1469e-01,  3.4705e-01],\n",
            "          [ 1.0211e-03,  2.5176e-01,  3.9265e-01]],\n",
            "\n",
            "         [[ 8.3589e-02, -1.8177e+00, -7.6806e-01],\n",
            "          [ 1.5067e-01, -7.8926e-01, -7.0905e-01],\n",
            "          [ 3.9725e-01, -1.1668e-02, -6.2019e-01],\n",
            "          ...,\n",
            "          [-2.9969e-01,  1.1473e-01, -1.9590e-01],\n",
            "          [-9.9216e-02,  4.8280e-01,  2.1396e-01],\n",
            "          [ 6.7529e-02,  2.8144e-01,  3.0408e-01]],\n",
            "\n",
            "         [[ 1.3308e-01, -1.8200e+00, -7.6593e-01],\n",
            "          [ 1.6803e-01, -8.2816e-01, -6.7760e-01],\n",
            "          [ 3.1394e-01, -1.1473e-01, -6.2386e-01],\n",
            "          ...,\n",
            "          [-2.4833e-01,  1.7565e-01, -2.5800e-01],\n",
            "          [-3.3578e-02,  5.3303e-01,  9.4656e-02],\n",
            "          [ 1.2046e-01,  2.9902e-01,  2.3946e-01]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 0.4261,  0.2706,  0.2954,  ...,  0.6000,  0.0392, -0.0043],\n",
            "          [ 0.3832,  0.1525,  0.2837,  ...,  0.4212,  0.0951,  0.2893],\n",
            "          [ 0.7859,  0.3796, -0.0159,  ...,  0.3753,  0.6619,  0.8703],\n",
            "          ...,\n",
            "          [ 0.1403, -0.1644, -0.0516,  ..., -0.0656,  0.2548,  0.4061],\n",
            "          [ 0.0452, -0.0377, -0.2923,  ..., -0.0305,  0.2044,  0.3838],\n",
            "          [ 0.2394,  0.0461, -0.3386,  ..., -0.2483, -0.0336,  0.1205]],\n",
            "\n",
            "         [[-0.4323, -0.3622,  0.0022,  ..., -0.7160, -0.4477, -0.7603],\n",
            "          [-0.1896, -0.1866,  0.1746,  ..., -0.5420,  0.1036, -0.0637],\n",
            "          [-0.3456, -0.2044,  0.1777,  ..., -0.1372,  0.0781,  0.1008],\n",
            "          ...,\n",
            "          [-0.9861, -0.4963,  0.0550,  ..., -0.8187,  0.2346,  0.0157],\n",
            "          [-0.8096, -0.3686,  0.1483,  ..., -0.0166,  0.2477,  0.0801],\n",
            "          [-0.6629, -0.2256,  0.2833,  ...,  0.1756,  0.5330,  0.2990]],\n",
            "\n",
            "         [[ 0.2434,  0.4587,  0.5614,  ..., -0.1652, -0.0189, -0.4057],\n",
            "          [-0.4906, -0.1731,  0.1529,  ..., -0.3699,  0.2202,  0.0980],\n",
            "          [-0.9468, -0.4858,  0.2944,  ..., -0.3992, -0.2557, -0.1530],\n",
            "          ...,\n",
            "          [-1.2473, -0.8044,  0.1000,  ..., -0.5826, -0.1599,  0.0716],\n",
            "          [-1.2099, -0.5305,  0.4046,  ..., -0.2811,  0.1074,  0.3344],\n",
            "          [-0.7078, -0.1984,  0.3298,  ..., -0.2580,  0.0947,  0.2395]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 4.2615e-01, -4.3235e-01,  2.4338e-01],\n",
            "          [ 2.7061e-01, -3.6225e-01,  4.5866e-01],\n",
            "          [ 2.9539e-01,  2.2266e-03,  5.6135e-01],\n",
            "          ...,\n",
            "          [ 6.5063e-01, -3.0302e-01, -2.8172e-01],\n",
            "          [ 2.4756e-01,  6.9122e-02,  4.4629e-01],\n",
            "          [ 4.3046e-02,  4.4949e-01,  3.0779e-02]],\n",
            "\n",
            "         [[ 4.8281e-01, -5.2169e-01,  2.1174e-01],\n",
            "          [ 3.4759e-01, -4.2217e-01,  4.3883e-01],\n",
            "          [ 3.6248e-01, -3.1037e-02,  5.4616e-01],\n",
            "          ...,\n",
            "          [ 7.3529e-01, -3.1398e-01, -2.2366e-01],\n",
            "          [ 4.3207e-01,  3.4243e-02,  4.4929e-01],\n",
            "          [ 2.7604e-01,  3.6906e-01, -4.7540e-03]],\n",
            "\n",
            "         [[ 4.9273e-01, -6.2897e-01,  8.7259e-02],\n",
            "          [ 4.3936e-01, -4.9963e-01,  3.6768e-01],\n",
            "          [ 4.4133e-01, -8.6695e-02,  5.2165e-01],\n",
            "          ...,\n",
            "          [ 8.3303e-01, -3.2499e-01, -1.8697e-01],\n",
            "          [ 6.6913e-01,  3.1529e-03,  3.8436e-01],\n",
            "          [ 5.3209e-01,  3.2837e-01, -5.5269e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.2623e-02, -1.7966e+00, -7.5558e-01],\n",
            "          [ 1.1711e-01, -7.5718e-01, -7.1333e-01],\n",
            "          [ 4.7642e-01,  9.8976e-02, -5.9192e-01],\n",
            "          ...,\n",
            "          [-3.3422e-01,  5.7449e-02, -1.5275e-01],\n",
            "          [-1.8565e-01,  4.1469e-01,  3.4705e-01],\n",
            "          [ 1.0211e-03,  2.5176e-01,  3.9265e-01]],\n",
            "\n",
            "         [[ 8.3589e-02, -1.8177e+00, -7.6806e-01],\n",
            "          [ 1.5067e-01, -7.8926e-01, -7.0905e-01],\n",
            "          [ 3.9725e-01, -1.1668e-02, -6.2019e-01],\n",
            "          ...,\n",
            "          [-2.9969e-01,  1.1473e-01, -1.9590e-01],\n",
            "          [-9.9216e-02,  4.8280e-01,  2.1396e-01],\n",
            "          [ 6.7529e-02,  2.8144e-01,  3.0408e-01]],\n",
            "\n",
            "         [[ 1.3308e-01, -1.8200e+00, -7.6593e-01],\n",
            "          [ 1.6803e-01, -8.2816e-01, -6.7760e-01],\n",
            "          [ 3.1394e-01, -1.1473e-01, -6.2386e-01],\n",
            "          ...,\n",
            "          [-2.4833e-01,  1.7565e-01, -2.5800e-01],\n",
            "          [-3.3578e-02,  5.3303e-01,  9.4656e-02],\n",
            "          [ 1.2046e-01,  2.9902e-01,  2.3946e-01]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "[0/1][0/50000]\tLoss_D: 1.3949\tLoss_G: 0.0000\tD(x): -0.1360\tD(G(z)): -0.1275 / 18.4630\n",
            "w tensor([[ 0.0065,  0.0323,  0.0547, -0.0023,  0.0021, -0.0579,  0.0448, -0.0427,\n",
            "          0.0938,  0.0025,  0.0881, -0.0904,  0.0795, -0.0362, -0.0588, -0.0794,\n",
            "         -0.0450, -0.0424,  0.0730,  0.0939, -0.0322, -0.0343,  0.0195,  0.0485,\n",
            "          0.0848, -0.0089, -0.0103,  0.0746,  0.0647,  0.0385,  0.0334, -0.0663,\n",
            "          0.1206, -0.0419,  0.0598,  0.1088, -0.1069,  0.0856, -0.0464,  0.0125,\n",
            "          0.0598, -0.0638,  0.0417,  0.0507,  0.0162,  0.0520, -0.1054,  0.0899,\n",
            "          0.0578,  0.0073,  0.0343, -0.0819, -0.0456, -0.0625,  0.0782,  0.0309,\n",
            "          0.0458,  0.0859,  0.0239,  0.0805,  0.0370, -0.0429,  0.0402,  0.0347,\n",
            "         -0.0841, -0.0097, -0.0374, -0.0780, -0.0815,  0.0032, -0.0123,  0.0579,\n",
            "          0.0281,  0.0510,  0.0310,  0.0031,  0.0853, -0.0912, -0.0012, -0.0813,\n",
            "          0.0430, -0.0590,  0.0486,  0.1137, -0.0669,  0.0732, -0.0558, -0.0896,\n",
            "          0.0316,  0.0198,  0.0429,  0.0136,  0.0449,  0.0759, -0.1074, -0.0887,\n",
            "          0.0242, -0.0116, -0.0773,  0.0148]], device='cuda:0')\n",
            "y, y size tensor([[[-0.1069,  0.1304, -0.0917,  ...,  0.0285,  0.0770, -0.0010],\n",
            "         [-0.0895,  0.1309, -0.1074,  ...,  0.0320,  0.1263, -0.0020],\n",
            "         [-0.0596,  0.1299, -0.1170,  ...,  0.0321,  0.1916, -0.0073],\n",
            "         ...,\n",
            "         [-0.0093,  0.0926,  0.1495,  ..., -0.0512,  0.1247, -0.1121],\n",
            "         [ 0.0175,  0.0908,  0.1393,  ..., -0.0582,  0.1530, -0.1146],\n",
            "         [ 0.0313,  0.0904,  0.1247,  ..., -0.0612,  0.1672, -0.1123]]],\n",
            "       device='cuda:0') torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 3.0562e-01, -4.1120e-01,  9.1743e-01],\n",
            "          [ 4.0171e-01, -1.1808e-02,  7.9388e-01],\n",
            "          [ 2.7780e-01,  8.7600e-01,  3.4997e-01],\n",
            "          ...,\n",
            "          [-3.4674e-01,  2.2637e-01, -4.1730e-01],\n",
            "          [-5.0794e-01,  6.4809e-01, -3.1034e-01],\n",
            "          [-2.8134e-01,  7.0285e-01, -3.6290e-01]],\n",
            "\n",
            "         [[ 2.6748e-01, -2.5821e-01,  9.0591e-01],\n",
            "          [ 3.6974e-01,  6.5112e-02,  7.5655e-01],\n",
            "          [ 3.6405e-01,  9.3424e-01,  2.9081e-01],\n",
            "          ...,\n",
            "          [-2.5059e-01,  2.1030e-01, -4.1042e-01],\n",
            "          [-5.1029e-01,  6.5440e-01, -2.9488e-01],\n",
            "          [-3.1880e-01,  7.1221e-01, -3.4313e-01]],\n",
            "\n",
            "         [[ 2.4460e-01,  3.9721e-03,  8.9831e-01],\n",
            "          [ 3.0172e-01,  2.1704e-01,  7.2352e-01],\n",
            "          [ 4.4835e-01,  1.0041e+00,  2.8206e-01],\n",
            "          ...,\n",
            "          [-1.3891e-01,  1.9952e-01, -4.0736e-01],\n",
            "          [-4.9601e-01,  6.4268e-01, -3.0025e-01],\n",
            "          [-2.8631e-01,  7.2111e-01, -2.9176e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1696e-01,  4.9330e-01,  9.7542e-01],\n",
            "          [-4.8340e-01,  5.2824e-01,  1.1165e+00],\n",
            "          [-2.2315e-01,  4.9638e-01,  8.4852e-01],\n",
            "          ...,\n",
            "          [-6.2306e-01,  2.0230e-02, -4.5833e-01],\n",
            "          [-2.8292e-01, -5.2446e-04,  1.8344e-01],\n",
            "          [ 2.3964e-01,  1.1568e-01,  5.3106e-01]],\n",
            "\n",
            "         [[-1.9855e-01,  5.5823e-01,  1.1099e+00],\n",
            "          [-4.9324e-01,  4.8674e-01,  1.1900e+00],\n",
            "          [-2.3216e-01,  3.9037e-01,  9.7598e-01],\n",
            "          ...,\n",
            "          [-8.6984e-01,  8.1083e-02, -5.0323e-01],\n",
            "          [-5.2412e-01, -1.8876e-02,  2.2671e-01],\n",
            "          [ 2.4853e-02,  1.3773e-01,  6.8573e-01]],\n",
            "\n",
            "         [[-1.7187e-01,  6.5389e-01,  1.2059e+00],\n",
            "          [-4.4391e-01,  4.4108e-01,  1.2193e+00],\n",
            "          [-1.9394e-01,  3.1625e-01,  1.1160e+00],\n",
            "          ...,\n",
            "          [-1.0626e+00,  1.4055e-01, -5.7023e-01],\n",
            "          [-7.0557e-01, -4.6462e-02,  1.8277e-01],\n",
            "          [-2.0089e-01,  1.4178e-01,  7.6945e-01]]]], device='cuda:0') torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 0.3056,  0.4017,  0.2778,  ...,  0.2875,  0.3840,  0.3721],\n",
            "          [ 0.2707,  0.1646,  0.0254,  ...,  0.0486,  0.1345,  0.1181],\n",
            "          [ 0.1723, -0.0697, -0.3378,  ..., -0.4769, -0.2362, -0.1924],\n",
            "          ...,\n",
            "          [-0.3844, -0.2409,  0.0709,  ..., -0.3776,  0.1678,  0.3725],\n",
            "          [-0.1450, -0.1940,  0.1693,  ..., -0.5910, -0.1229,  0.2505],\n",
            "          [ 0.2460,  0.0282,  0.2842,  ..., -1.0626, -0.7056, -0.2009]],\n",
            "\n",
            "         [[-0.4112, -0.0118,  0.8760,  ...,  0.9526,  0.9433,  0.9436],\n",
            "          [-0.8804, -0.4493,  0.6054,  ...,  0.4912,  0.3342,  0.3216],\n",
            "          [-0.6383, -0.1556,  0.4491,  ...,  0.1824,  0.1506,  0.1457],\n",
            "          ...,\n",
            "          [-0.2869, -0.0388,  0.2130,  ...,  0.2615,  0.3504,  0.5962],\n",
            "          [-0.3941, -0.4887,  0.0636,  ...,  0.1778,  0.0987,  0.4115],\n",
            "          [-0.3733, -0.3526,  0.0613,  ...,  0.1405, -0.0465,  0.1418]],\n",
            "\n",
            "         [[ 0.9174,  0.7939,  0.3500,  ...,  0.8000,  0.5987,  0.4436],\n",
            "          [ 0.7820,  0.5839, -0.0952,  ...,  0.4061,  0.2272,  0.2309],\n",
            "          [ 0.2697, -0.1545, -0.1010,  ..., -0.5512, -0.2463,  0.0079],\n",
            "          ...,\n",
            "          [ 0.2506,  0.3900,  0.4134,  ...,  0.5826,  0.8322,  1.3614],\n",
            "          [-0.1344, -0.4151, -0.2403,  ..., -0.4978,  0.4306,  1.0677],\n",
            "          [-0.1357, -0.6223, -0.5580,  ..., -0.5702,  0.1828,  0.7694]]]],\n",
            "       device='cuda:0') torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 3.0562e-01, -4.1120e-01,  9.1743e-01],\n",
            "          [ 4.0171e-01, -1.1808e-02,  7.9388e-01],\n",
            "          [ 2.7780e-01,  8.7600e-01,  3.4997e-01],\n",
            "          ...,\n",
            "          [-3.4674e-01,  2.2637e-01, -4.1730e-01],\n",
            "          [-5.0794e-01,  6.4809e-01, -3.1034e-01],\n",
            "          [-2.8134e-01,  7.0285e-01, -3.6290e-01]],\n",
            "\n",
            "         [[ 2.6748e-01, -2.5821e-01,  9.0591e-01],\n",
            "          [ 3.6974e-01,  6.5112e-02,  7.5655e-01],\n",
            "          [ 3.6405e-01,  9.3424e-01,  2.9081e-01],\n",
            "          ...,\n",
            "          [-2.5059e-01,  2.1030e-01, -4.1042e-01],\n",
            "          [-5.1029e-01,  6.5440e-01, -2.9488e-01],\n",
            "          [-3.1880e-01,  7.1221e-01, -3.4313e-01]],\n",
            "\n",
            "         [[ 2.4460e-01,  3.9721e-03,  8.9831e-01],\n",
            "          [ 3.0172e-01,  2.1704e-01,  7.2352e-01],\n",
            "          [ 4.4835e-01,  1.0041e+00,  2.8206e-01],\n",
            "          ...,\n",
            "          [-1.3891e-01,  1.9952e-01, -4.0736e-01],\n",
            "          [-4.9601e-01,  6.4268e-01, -3.0025e-01],\n",
            "          [-2.8631e-01,  7.2111e-01, -2.9176e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1696e-01,  4.9330e-01,  9.7542e-01],\n",
            "          [-4.8340e-01,  5.2824e-01,  1.1165e+00],\n",
            "          [-2.2315e-01,  4.9638e-01,  8.4852e-01],\n",
            "          ...,\n",
            "          [-6.2306e-01,  2.0230e-02, -4.5833e-01],\n",
            "          [-2.8292e-01, -5.2446e-04,  1.8344e-01],\n",
            "          [ 2.3964e-01,  1.1568e-01,  5.3106e-01]],\n",
            "\n",
            "         [[-1.9855e-01,  5.5823e-01,  1.1099e+00],\n",
            "          [-4.9324e-01,  4.8674e-01,  1.1900e+00],\n",
            "          [-2.3216e-01,  3.9037e-01,  9.7598e-01],\n",
            "          ...,\n",
            "          [-8.6984e-01,  8.1083e-02, -5.0323e-01],\n",
            "          [-5.2412e-01, -1.8876e-02,  2.2671e-01],\n",
            "          [ 2.4853e-02,  1.3773e-01,  6.8573e-01]],\n",
            "\n",
            "         [[-1.7187e-01,  6.5389e-01,  1.2059e+00],\n",
            "          [-4.4391e-01,  4.4108e-01,  1.2193e+00],\n",
            "          [-1.9394e-01,  3.1625e-01,  1.1160e+00],\n",
            "          ...,\n",
            "          [-1.0626e+00,  1.4055e-01, -5.7023e-01],\n",
            "          [-7.0557e-01, -4.6462e-02,  1.8277e-01],\n",
            "          [-2.0089e-01,  1.4178e-01,  7.6945e-01]]]], device='cuda:0') torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 5.4965e-03,  3.2172e-02,  5.6476e-02, -2.7576e-03, -1.9376e-03,\n",
            "         -5.7266e-02,  4.5686e-02, -4.3397e-02,  9.1958e-02,  2.3365e-03,\n",
            "          8.7322e-02, -8.8064e-02,  7.9663e-02, -3.5935e-02, -5.9074e-02,\n",
            "         -7.8893e-02, -4.4334e-02, -4.3029e-02,  7.2725e-02,  9.5073e-02,\n",
            "         -3.1173e-02, -3.4998e-02,  1.9356e-02,  4.7799e-02,  8.6129e-02,\n",
            "         -1.0959e-02, -1.0658e-02,  7.5144e-02,  6.4248e-02,  3.8479e-02,\n",
            "          3.4618e-02, -6.7936e-02,  1.2324e-01, -4.1977e-02,  6.0886e-02,\n",
            "          1.0956e-01, -1.0671e-01,  8.6760e-02, -4.6835e-02,  1.3583e-02,\n",
            "          6.0097e-02, -6.4302e-02,  4.2128e-02,  5.2446e-02,  1.5814e-02,\n",
            "          5.2902e-02, -1.0484e-01,  8.9883e-02,  5.7769e-02,  7.6954e-03,\n",
            "          3.4325e-02, -8.2555e-02, -4.4431e-02, -6.3453e-02,  7.7534e-02,\n",
            "          3.0028e-02,  4.6158e-02,  8.5213e-02,  2.4124e-02,  8.0416e-02,\n",
            "          3.7684e-02, -4.2188e-02,  4.0794e-02,  3.3534e-02, -8.2403e-02,\n",
            "         -9.6066e-03, -3.7790e-02, -7.8224e-02, -8.3793e-02,  3.9109e-03,\n",
            "         -1.1121e-02,  5.7549e-02,  2.8473e-02,  5.0812e-02,  3.1193e-02,\n",
            "          2.6780e-03,  8.6152e-02, -9.0330e-02,  7.9654e-05, -8.1514e-02,\n",
            "          4.3329e-02, -5.8107e-02,  4.8179e-02,  1.1348e-01, -6.7337e-02,\n",
            "          7.4385e-02, -5.3423e-02, -8.7704e-02,  3.0128e-02,  2.0739e-02,\n",
            "          4.3621e-02,  1.3716e-02,  4.4427e-02,  7.5686e-02, -1.0762e-01,\n",
            "         -8.8192e-02,  2.4071e-02, -1.1546e-02, -7.8521e-02,  1.4991e-02]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.1304, -0.0202,  0.0011,  ..., -0.1680,  0.0993,  0.0950],\n",
            "         [-0.0951, -0.0229,  0.0116,  ..., -0.1723,  0.0751,  0.0947],\n",
            "         [-0.0413, -0.0228,  0.0155,  ..., -0.1720,  0.0462,  0.0933],\n",
            "         ...,\n",
            "         [ 0.0487,  0.0702, -0.2765,  ..., -0.0903,  0.0878,  0.0453],\n",
            "         [ 0.0708,  0.0738, -0.2637,  ..., -0.0821,  0.0754,  0.0440],\n",
            "         [ 0.0744,  0.0739, -0.2435,  ..., -0.0792,  0.0701,  0.0445]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-0.2367,  0.8802,  0.3446],\n",
            "          [-0.2730,  0.8754,  0.4391],\n",
            "          [-0.3614,  0.3118,  0.4926],\n",
            "          ...,\n",
            "          [ 0.0074, -0.0987,  0.2550],\n",
            "          [ 0.4047,  0.1536,  0.4907],\n",
            "          [ 0.4331,  0.1520,  0.3024]],\n",
            "\n",
            "         [[-0.1913,  1.0277,  0.5983],\n",
            "          [-0.1960,  0.9769,  0.5994],\n",
            "          [-0.3850,  0.3146,  0.5601],\n",
            "          ...,\n",
            "          [ 0.0120, -0.1106,  0.1651],\n",
            "          [ 0.4128,  0.0925,  0.4472],\n",
            "          [ 0.3666,  0.1823,  0.2985]],\n",
            "\n",
            "         [[-0.1431,  1.2208,  0.8436],\n",
            "          [-0.1153,  1.0796,  0.7556],\n",
            "          [-0.4252,  0.3307,  0.6223],\n",
            "          ...,\n",
            "          [ 0.0045, -0.1876,  0.0178],\n",
            "          [ 0.3573, -0.0567,  0.3290],\n",
            "          [ 0.2208,  0.1535,  0.2262]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1031, -0.4480,  0.1961],\n",
            "          [-1.1092, -0.5305,  0.5802],\n",
            "          [-0.8399, -0.3933,  0.6477],\n",
            "          ...,\n",
            "          [-0.0595, -0.7869, -0.5368],\n",
            "          [-0.1264, -0.1880, -0.3492],\n",
            "          [-0.1981,  0.4824, -0.0019]],\n",
            "\n",
            "         [[-1.0725, -0.4252,  0.1468],\n",
            "          [-1.1332, -0.5402,  0.6579],\n",
            "          [-0.8857, -0.4219,  0.7600],\n",
            "          ...,\n",
            "          [ 0.0251, -0.8595, -0.7077],\n",
            "          [-0.1054, -0.1792, -0.4654],\n",
            "          [-0.2401,  0.4323, -0.0891]],\n",
            "\n",
            "         [[-0.9955, -0.3891,  0.0839],\n",
            "          [-1.1042, -0.4976,  0.6955],\n",
            "          [-0.8582, -0.3803,  0.8243],\n",
            "          ...,\n",
            "          [ 0.0894, -0.9083, -0.8446],\n",
            "          [-0.1056, -0.2068, -0.5302],\n",
            "          [-0.2921,  0.3199, -0.1533]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-0.2367, -0.2730, -0.3614,  ..., -0.3529, -0.6117, -0.9289],\n",
            "          [ 0.4420,  0.1352, -0.1204,  ...,  0.0945, -0.3472, -0.8457],\n",
            "          [ 0.1592, -0.1841,  0.2659,  ...,  0.1304, -0.2360, -0.6496],\n",
            "          ...,\n",
            "          [-0.1777, -0.1859,  0.0695,  ..., -0.4418, -0.2038,  0.1261],\n",
            "          [-0.2189, -0.2742, -0.0349,  ..., -0.0854,  0.2597,  0.4449],\n",
            "          [-0.3701, -0.3116, -0.3755,  ...,  0.0894, -0.1056, -0.2921]],\n",
            "\n",
            "         [[ 0.8802,  0.8754,  0.3118,  ...,  0.8192,  0.4353, -0.0352],\n",
            "          [ 0.4848,  0.4888, -0.0041,  ...,  0.5390,  0.1758, -0.5339],\n",
            "          [-0.1803, -0.1056,  0.0167,  ..., -0.1266, -0.6467, -0.6049],\n",
            "          ...,\n",
            "          [-0.4086,  0.0041,  0.2913,  ..., -0.4534,  0.1144,  0.2698],\n",
            "          [-0.6296, -0.4106,  0.3230,  ..., -0.7895,  0.2594,  0.5590],\n",
            "          [-0.2011, -0.4897, -0.1397,  ..., -0.9083, -0.2068,  0.3199]],\n",
            "\n",
            "         [[ 0.3446,  0.4391,  0.4926,  ...,  0.7750,  0.7605,  0.6880],\n",
            "          [-0.1616, -0.0222,  0.1700,  ...,  0.2400,  0.3434,  0.4629],\n",
            "          [-0.4252, -0.0285,  0.2853,  ..., -0.3306,  0.0301,  0.1991],\n",
            "          ...,\n",
            "          [-0.4844, -0.2280, -0.1821,  ...,  0.2168,  0.5177,  0.8048],\n",
            "          [-0.1150,  0.0273,  0.3345,  ..., -0.5359,  0.1793,  0.7096],\n",
            "          [ 0.2249,  0.3611,  0.3481,  ..., -0.8446, -0.5302, -0.1533]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-0.2367,  0.8802,  0.3446],\n",
            "          [-0.2730,  0.8754,  0.4391],\n",
            "          [-0.3614,  0.3118,  0.4926],\n",
            "          ...,\n",
            "          [ 0.0074, -0.0987,  0.2550],\n",
            "          [ 0.4047,  0.1536,  0.4907],\n",
            "          [ 0.4331,  0.1520,  0.3024]],\n",
            "\n",
            "         [[-0.1913,  1.0277,  0.5983],\n",
            "          [-0.1960,  0.9769,  0.5994],\n",
            "          [-0.3850,  0.3146,  0.5601],\n",
            "          ...,\n",
            "          [ 0.0120, -0.1106,  0.1651],\n",
            "          [ 0.4128,  0.0925,  0.4472],\n",
            "          [ 0.3666,  0.1823,  0.2985]],\n",
            "\n",
            "         [[-0.1431,  1.2208,  0.8436],\n",
            "          [-0.1153,  1.0796,  0.7556],\n",
            "          [-0.4252,  0.3307,  0.6223],\n",
            "          ...,\n",
            "          [ 0.0045, -0.1876,  0.0178],\n",
            "          [ 0.3573, -0.0567,  0.3290],\n",
            "          [ 0.2208,  0.1535,  0.2262]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1031, -0.4480,  0.1961],\n",
            "          [-1.1092, -0.5305,  0.5802],\n",
            "          [-0.8399, -0.3933,  0.6477],\n",
            "          ...,\n",
            "          [-0.0595, -0.7869, -0.5368],\n",
            "          [-0.1264, -0.1880, -0.3492],\n",
            "          [-0.1981,  0.4824, -0.0019]],\n",
            "\n",
            "         [[-1.0725, -0.4252,  0.1468],\n",
            "          [-1.1332, -0.5402,  0.6579],\n",
            "          [-0.8857, -0.4219,  0.7600],\n",
            "          ...,\n",
            "          [ 0.0251, -0.8595, -0.7077],\n",
            "          [-0.1054, -0.1792, -0.4654],\n",
            "          [-0.2401,  0.4323, -0.0891]],\n",
            "\n",
            "         [[-0.9955, -0.3891,  0.0839],\n",
            "          [-1.1042, -0.4976,  0.6955],\n",
            "          [-0.8582, -0.3803,  0.8243],\n",
            "          ...,\n",
            "          [ 0.0894, -0.9083, -0.8446],\n",
            "          [-0.1056, -0.2068, -0.5302],\n",
            "          [-0.2921,  0.3199, -0.1533]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0053,  0.0325,  0.0566, -0.0029, -0.0036, -0.0561,  0.0466, -0.0436,\n",
            "          0.0924,  0.0032,  0.0863, -0.0877,  0.0800, -0.0353, -0.0581, -0.0786,\n",
            "         -0.0440, -0.0428,  0.0724,  0.0950, -0.0312, -0.0361,  0.0201,  0.0481,\n",
            "          0.0862, -0.0126, -0.0098,  0.0762,  0.0643,  0.0388,  0.0348, -0.0689,\n",
            "          0.1246, -0.0410,  0.0618,  0.1098, -0.1068,  0.0863, -0.0473,  0.0135,\n",
            "          0.0603, -0.0646,  0.0418,  0.0529,  0.0154,  0.0528, -0.1041,  0.0896,\n",
            "          0.0567,  0.0085,  0.0344, -0.0826, -0.0443, -0.0647,  0.0777,  0.0301,\n",
            "          0.0457,  0.0848,  0.0243,  0.0797,  0.0379, -0.0423,  0.0411,  0.0328,\n",
            "         -0.0813, -0.0097, -0.0375, -0.0776, -0.0836,  0.0043, -0.0102,  0.0575,\n",
            "          0.0286,  0.0525,  0.0316,  0.0031,  0.0878, -0.0904,  0.0002, -0.0808,\n",
            "          0.0427, -0.0576,  0.0474,  0.1136, -0.0674,  0.0753, -0.0528, -0.0876,\n",
            "          0.0293,  0.0213,  0.0440,  0.0138,  0.0443,  0.0759, -0.1064, -0.0881,\n",
            "          0.0242, -0.0110, -0.0789,  0.0153]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.0283, -0.1556, -0.0520,  ...,  0.0998,  0.0501,  0.1585],\n",
            "         [ 0.0190, -0.1595, -0.0562,  ...,  0.0971,  0.0240,  0.1542],\n",
            "         [ 0.0014, -0.1567, -0.0555,  ...,  0.0920, -0.0070,  0.1429],\n",
            "         ...,\n",
            "         [ 0.0104,  0.1205,  0.1238,  ..., -0.0013,  0.0514, -0.0409],\n",
            "         [-0.0244,  0.1120,  0.1238,  ...,  0.0018,  0.0420, -0.0410],\n",
            "         [-0.0499,  0.0975,  0.1169,  ...,  0.0059,  0.0370, -0.0335]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 0.0191,  0.2740, -0.3214],\n",
            "          [ 0.1287,  0.5311, -0.2486],\n",
            "          [ 0.1843,  0.2824, -0.2789],\n",
            "          ...,\n",
            "          [ 0.0468, -0.4511, -0.6827],\n",
            "          [-0.0839, -0.6401, -0.8875],\n",
            "          [-0.3651, -0.6217, -0.9240]],\n",
            "\n",
            "         [[-0.0500,  0.2663, -0.5050],\n",
            "          [ 0.1378,  0.5277, -0.2788],\n",
            "          [ 0.2185,  0.2480, -0.2149],\n",
            "          ...,\n",
            "          [ 0.1099, -0.4759, -0.7691],\n",
            "          [-0.0833, -0.6882, -0.8426],\n",
            "          [-0.4487, -0.6749, -0.9816]],\n",
            "\n",
            "         [[-0.1186,  0.2818, -0.6695],\n",
            "          [ 0.1011,  0.4826, -0.2934],\n",
            "          [ 0.2841,  0.1954, -0.1620],\n",
            "          ...,\n",
            "          [ 0.2146, -0.4785, -0.7911],\n",
            "          [-0.0886, -0.7658, -0.7103],\n",
            "          [-0.4889, -0.7193, -0.9714]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3695, -0.7706, -0.0457],\n",
            "          [ 0.3231, -0.6117,  0.7319],\n",
            "          [ 0.3373, -0.9775,  0.8739],\n",
            "          ...,\n",
            "          [ 0.1569, -0.6146, -0.7397],\n",
            "          [-0.2365, -1.3082, -1.0510],\n",
            "          [-0.3358, -1.4371, -1.1585]],\n",
            "\n",
            "         [[ 0.4250, -0.6075, -0.0359],\n",
            "          [ 0.3917, -0.4828,  0.6209],\n",
            "          [ 0.4259, -0.9618,  0.8331],\n",
            "          ...,\n",
            "          [ 0.0510, -0.5935, -0.6892],\n",
            "          [-0.3483, -1.3188, -1.0777],\n",
            "          [-0.4073, -1.4721, -1.1657]],\n",
            "\n",
            "         [[ 0.4876, -0.4414, -0.0375],\n",
            "          [ 0.4389, -0.3469,  0.5148],\n",
            "          [ 0.4629, -0.8792,  0.7738],\n",
            "          ...,\n",
            "          [ 0.0170, -0.4916, -0.6313],\n",
            "          [-0.3910, -1.3056, -1.0758],\n",
            "          [-0.4362, -1.4786, -1.1496]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 0.0191,  0.1287,  0.1843,  ...,  0.0863,  0.4046,  0.2169],\n",
            "          [-0.0331, -0.0778,  0.1976,  ...,  0.0271,  0.2052, -0.2706],\n",
            "          [ 0.0383, -0.0398, -0.2402,  ...,  0.1523, -0.3217, -0.5429],\n",
            "          ...,\n",
            "          [-0.1380,  0.0139, -0.2120,  ...,  0.2255,  0.0513, -0.1837],\n",
            "          [ 0.0617,  0.2403,  0.1896,  ...,  0.1527, -0.3701, -0.5058],\n",
            "          [ 0.4604,  0.5484,  0.4915,  ...,  0.0170, -0.3910, -0.4362]],\n",
            "\n",
            "         [[ 0.2740,  0.5311,  0.2824,  ...,  0.3910, -0.1445, -0.6500],\n",
            "          [ 0.2081,  0.2219,  0.1870,  ...,  0.3225, -0.4276, -0.9500],\n",
            "          [ 0.1209, -0.1235, -0.3332,  ...,  0.0121, -0.7902, -0.9805],\n",
            "          ...,\n",
            "          [-0.6229, -0.3277, -0.3786,  ..., -0.0808, -0.7562, -1.4494],\n",
            "          [-0.0592,  0.0822, -0.4513,  ..., -0.0286, -1.0786, -1.6422],\n",
            "          [ 0.0631, -0.2635, -0.7814,  ..., -0.4916, -1.3056, -1.4786]],\n",
            "\n",
            "         [[-0.3214, -0.2486, -0.2789,  ..., -0.2757, -0.0530, -0.1018],\n",
            "          [-0.5639, -0.2580, -0.0426,  ..., -0.4600,  0.0112, -0.0925],\n",
            "          [-0.7501, -0.6322, -0.5474,  ..., -0.2282, -0.1337, -0.3950],\n",
            "          ...,\n",
            "          [-0.1291, -0.0123,  0.0800,  ...,  0.3469,  0.3879,  0.1591],\n",
            "          [-0.5511, -0.5348, -0.8110,  ..., -0.1747, -0.4903, -0.5275],\n",
            "          [-0.1583, -0.5415, -1.0797,  ..., -0.6313, -1.0758, -1.1496]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 0.0191,  0.2740, -0.3214],\n",
            "          [ 0.1287,  0.5311, -0.2486],\n",
            "          [ 0.1843,  0.2824, -0.2789],\n",
            "          ...,\n",
            "          [ 0.0468, -0.4511, -0.6827],\n",
            "          [-0.0839, -0.6401, -0.8875],\n",
            "          [-0.3651, -0.6217, -0.9240]],\n",
            "\n",
            "         [[-0.0500,  0.2663, -0.5050],\n",
            "          [ 0.1378,  0.5277, -0.2788],\n",
            "          [ 0.2185,  0.2480, -0.2149],\n",
            "          ...,\n",
            "          [ 0.1099, -0.4759, -0.7691],\n",
            "          [-0.0833, -0.6882, -0.8426],\n",
            "          [-0.4487, -0.6749, -0.9816]],\n",
            "\n",
            "         [[-0.1186,  0.2818, -0.6695],\n",
            "          [ 0.1011,  0.4826, -0.2934],\n",
            "          [ 0.2841,  0.1954, -0.1620],\n",
            "          ...,\n",
            "          [ 0.2146, -0.4785, -0.7911],\n",
            "          [-0.0886, -0.7658, -0.7103],\n",
            "          [-0.4889, -0.7193, -0.9714]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3695, -0.7706, -0.0457],\n",
            "          [ 0.3231, -0.6117,  0.7319],\n",
            "          [ 0.3373, -0.9775,  0.8739],\n",
            "          ...,\n",
            "          [ 0.1569, -0.6146, -0.7397],\n",
            "          [-0.2365, -1.3082, -1.0510],\n",
            "          [-0.3358, -1.4371, -1.1585]],\n",
            "\n",
            "         [[ 0.4250, -0.6075, -0.0359],\n",
            "          [ 0.3917, -0.4828,  0.6209],\n",
            "          [ 0.4259, -0.9618,  0.8331],\n",
            "          ...,\n",
            "          [ 0.0510, -0.5935, -0.6892],\n",
            "          [-0.3483, -1.3188, -1.0777],\n",
            "          [-0.4073, -1.4721, -1.1657]],\n",
            "\n",
            "         [[ 0.4876, -0.4414, -0.0375],\n",
            "          [ 0.4389, -0.3469,  0.5148],\n",
            "          [ 0.4629, -0.8792,  0.7738],\n",
            "          ...,\n",
            "          [ 0.0170, -0.4916, -0.6313],\n",
            "          [-0.3910, -1.3056, -1.0758],\n",
            "          [-0.4362, -1.4786, -1.1496]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0053,  0.0322,  0.0557, -0.0030, -0.0039, -0.0566,  0.0466, -0.0431,\n",
            "          0.0924,  0.0024,  0.0864, -0.0871,  0.0799, -0.0358, -0.0574, -0.0780,\n",
            "         -0.0433, -0.0422,  0.0712,  0.0947, -0.0312, -0.0355,  0.0210,  0.0482,\n",
            "          0.0852, -0.0124, -0.0096,  0.0765,  0.0636,  0.0372,  0.0347, -0.0675,\n",
            "          0.1239, -0.0415,  0.0635,  0.1092, -0.1073,  0.0864, -0.0471,  0.0128,\n",
            "          0.0587, -0.0649,  0.0415,  0.0520,  0.0169,  0.0530, -0.1034,  0.0895,\n",
            "          0.0568,  0.0079,  0.0340, -0.0819, -0.0434, -0.0639,  0.0777,  0.0281,\n",
            "          0.0462,  0.0858,  0.0248,  0.0804,  0.0367, -0.0420,  0.0395,  0.0324,\n",
            "         -0.0815, -0.0088, -0.0373, -0.0786, -0.0832,  0.0043, -0.0105,  0.0571,\n",
            "          0.0286,  0.0522,  0.0315,  0.0036,  0.0864, -0.0898, -0.0012, -0.0804,\n",
            "          0.0436, -0.0574,  0.0476,  0.1134, -0.0666,  0.0754, -0.0530, -0.0877,\n",
            "          0.0295,  0.0220,  0.0437,  0.0142,  0.0454,  0.0759, -0.1076, -0.0882,\n",
            "          0.0238, -0.0114, -0.0789,  0.0144]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.0495, -0.1681,  0.0561,  ..., -0.1205,  0.0098, -0.0796],\n",
            "         [ 0.0616, -0.1807,  0.0585,  ..., -0.1277,  0.0480, -0.0816],\n",
            "         [ 0.0805, -0.1797,  0.0601,  ..., -0.1170,  0.0904, -0.0861],\n",
            "         ...,\n",
            "         [ 0.1511,  0.3542, -0.0016,  ...,  0.2654,  0.0386, -0.1589],\n",
            "         [ 0.1702,  0.3433,  0.0039,  ...,  0.2851,  0.0446, -0.1603],\n",
            "         [ 0.1800,  0.3237,  0.0095,  ...,  0.2847,  0.0468, -0.1588]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-0.3468,  0.0024,  0.1377],\n",
            "          [-0.4311,  0.0970,  0.0211],\n",
            "          [-0.3101,  0.1786, -0.2553],\n",
            "          ...,\n",
            "          [ 0.1164,  0.0269,  0.2421],\n",
            "          [-0.2633, -0.3002,  0.3393],\n",
            "          [-0.5198, -0.3011, -0.0855]],\n",
            "\n",
            "         [[-0.3679, -0.1411,  0.2225],\n",
            "          [-0.3649,  0.0398,  0.0918],\n",
            "          [-0.1774,  0.1733, -0.2519],\n",
            "          ...,\n",
            "          [ 0.0748, -0.0762,  0.1764],\n",
            "          [-0.3110, -0.3227,  0.2691],\n",
            "          [-0.6210, -0.3118, -0.1105]],\n",
            "\n",
            "         [[-0.3784, -0.2540,  0.3108],\n",
            "          [-0.2617,  0.0178,  0.1310],\n",
            "          [-0.0550,  0.1439, -0.2247],\n",
            "          ...,\n",
            "          [ 0.0350, -0.1862,  0.1168],\n",
            "          [-0.3265, -0.3296,  0.2043],\n",
            "          [-0.6887, -0.2995, -0.1203]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.5829,  0.4509,  0.4408],\n",
            "          [-0.7901,  0.3406,  0.5418],\n",
            "          [-0.8596,  0.0276,  0.6321],\n",
            "          ...,\n",
            "          [-0.0163,  0.0504,  0.1710],\n",
            "          [-0.2182,  0.1303, -0.0287],\n",
            "          [-0.4238,  0.3692, -0.3220]],\n",
            "\n",
            "         [[-0.6433,  0.4990,  0.5446],\n",
            "          [-0.8920,  0.2761,  0.7233],\n",
            "          [-0.9393, -0.0784,  0.7703],\n",
            "          ...,\n",
            "          [-0.0019,  0.0257,  0.0416],\n",
            "          [-0.1835,  0.0992, -0.1037],\n",
            "          [-0.3642,  0.4062, -0.3243]],\n",
            "\n",
            "         [[-0.6759,  0.5233,  0.5980],\n",
            "          [-0.9413,  0.2292,  0.8360],\n",
            "          [-0.9544, -0.1611,  0.8368],\n",
            "          ...,\n",
            "          [ 0.0173,  0.0280, -0.0500],\n",
            "          [-0.1617,  0.1197, -0.1112],\n",
            "          [-0.3479,  0.4151, -0.3135]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-0.3468, -0.4311, -0.3101,  ..., -0.4295, -0.2904, -0.1029],\n",
            "          [-0.6333, -0.4039,  0.1058,  ..., -0.4062, -0.2640, -0.1660],\n",
            "          [-0.1874,  0.1629, -0.1828,  ..., -0.1798, -0.2572, -0.3334],\n",
            "          ...,\n",
            "          [-0.5509, -0.2508, -0.0829,  ..., -0.6081, -0.7204, -0.8077],\n",
            "          [-0.2037, -0.0206,  0.0387,  ..., -0.1165, -0.0854, -0.2935],\n",
            "          [ 0.2619,  0.1636, -0.0352,  ...,  0.0173, -0.1617, -0.3479]],\n",
            "\n",
            "         [[ 0.0024,  0.0970,  0.1786,  ...,  0.1163, -0.0608, -0.3827],\n",
            "          [-0.1651, -0.2136,  0.3379,  ...,  0.0397,  0.0143, -0.0346],\n",
            "          [ 0.2146,  0.2381, -0.2077,  ...,  0.1339,  0.0467,  0.2657],\n",
            "          ...,\n",
            "          [ 0.3041,  0.1185,  0.0946,  ...,  0.1917, -0.1187,  0.0086],\n",
            "          [ 0.3462,  0.2590,  0.1401,  ...,  0.1848,  0.1504,  0.1891],\n",
            "          [ 0.5839,  0.5540,  0.3198,  ...,  0.0280,  0.1197,  0.4151]],\n",
            "\n",
            "         [[ 0.1377,  0.0211, -0.2553,  ...,  0.5260,  0.2794,  0.3101],\n",
            "          [-0.0900,  0.0552, -0.3101,  ...,  0.3568,  0.0991,  0.0024],\n",
            "          [-0.2754,  0.1059,  0.0548,  ...,  0.0795, -0.1178, -0.1833],\n",
            "          ...,\n",
            "          [-0.3647, -0.4153, -0.0672,  ...,  0.3246,  0.5927,  0.6637],\n",
            "          [-0.1857,  0.1745,  0.3540,  ...,  0.0305, -0.0819, -0.0594],\n",
            "          [ 0.3136,  0.7285,  0.6062,  ..., -0.0500, -0.1112, -0.3135]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-0.3468,  0.0024,  0.1377],\n",
            "          [-0.4311,  0.0970,  0.0211],\n",
            "          [-0.3101,  0.1786, -0.2553],\n",
            "          ...,\n",
            "          [ 0.1164,  0.0269,  0.2421],\n",
            "          [-0.2633, -0.3002,  0.3393],\n",
            "          [-0.5198, -0.3011, -0.0855]],\n",
            "\n",
            "         [[-0.3679, -0.1411,  0.2225],\n",
            "          [-0.3649,  0.0398,  0.0918],\n",
            "          [-0.1774,  0.1733, -0.2519],\n",
            "          ...,\n",
            "          [ 0.0748, -0.0762,  0.1764],\n",
            "          [-0.3110, -0.3227,  0.2691],\n",
            "          [-0.6210, -0.3118, -0.1105]],\n",
            "\n",
            "         [[-0.3784, -0.2540,  0.3108],\n",
            "          [-0.2617,  0.0178,  0.1310],\n",
            "          [-0.0550,  0.1439, -0.2247],\n",
            "          ...,\n",
            "          [ 0.0350, -0.1862,  0.1168],\n",
            "          [-0.3265, -0.3296,  0.2043],\n",
            "          [-0.6887, -0.2995, -0.1203]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.5829,  0.4509,  0.4408],\n",
            "          [-0.7901,  0.3406,  0.5418],\n",
            "          [-0.8596,  0.0276,  0.6321],\n",
            "          ...,\n",
            "          [-0.0163,  0.0504,  0.1710],\n",
            "          [-0.2182,  0.1303, -0.0287],\n",
            "          [-0.4238,  0.3692, -0.3220]],\n",
            "\n",
            "         [[-0.6433,  0.4990,  0.5446],\n",
            "          [-0.8920,  0.2761,  0.7233],\n",
            "          [-0.9393, -0.0784,  0.7703],\n",
            "          ...,\n",
            "          [-0.0019,  0.0257,  0.0416],\n",
            "          [-0.1835,  0.0992, -0.1037],\n",
            "          [-0.3642,  0.4062, -0.3243]],\n",
            "\n",
            "         [[-0.6759,  0.5233,  0.5980],\n",
            "          [-0.9413,  0.2292,  0.8360],\n",
            "          [-0.9544, -0.1611,  0.8368],\n",
            "          ...,\n",
            "          [ 0.0173,  0.0280, -0.0500],\n",
            "          [-0.1617,  0.1197, -0.1112],\n",
            "          [-0.3479,  0.4151, -0.3135]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0048,  0.0316,  0.0571, -0.0024, -0.0021, -0.0563,  0.0466, -0.0438,\n",
            "          0.0922,  0.0029,  0.0867, -0.0875,  0.0798, -0.0362, -0.0592, -0.0791,\n",
            "         -0.0439, -0.0428,  0.0734,  0.0951, -0.0323, -0.0340,  0.0209,  0.0474,\n",
            "          0.0854, -0.0098, -0.0113,  0.0750,  0.0645,  0.0382,  0.0348, -0.0686,\n",
            "          0.1247, -0.0412,  0.0607,  0.1101, -0.1069,  0.0863, -0.0487,  0.0147,\n",
            "          0.0588, -0.0646,  0.0416,  0.0529,  0.0158,  0.0533, -0.1056,  0.0911,\n",
            "          0.0574,  0.0081,  0.0359, -0.0829, -0.0451, -0.0651,  0.0783,  0.0305,\n",
            "          0.0461,  0.0841,  0.0246,  0.0801,  0.0375, -0.0417,  0.0426,  0.0321,\n",
            "         -0.0809, -0.0088, -0.0370, -0.0790, -0.0840,  0.0034, -0.0099,  0.0585,\n",
            "          0.0280,  0.0531,  0.0320,  0.0035,  0.0871, -0.0906, -0.0005, -0.0809,\n",
            "          0.0442, -0.0587,  0.0476,  0.1121, -0.0677,  0.0739, -0.0524, -0.0880,\n",
            "          0.0312,  0.0213,  0.0447,  0.0137,  0.0441,  0.0747, -0.1069, -0.0879,\n",
            "          0.0242, -0.0104, -0.0794,  0.0165]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.2951, -0.0405, -0.0110,  ..., -0.1082, -0.0580,  0.0722],\n",
            "         [-0.2774, -0.0348, -0.0143,  ..., -0.1093, -0.0504,  0.0722],\n",
            "         [-0.2459, -0.0203, -0.0168,  ..., -0.1050, -0.0416,  0.0714],\n",
            "         ...,\n",
            "         [-0.1368,  0.1685,  0.0899,  ...,  0.0401, -0.0566,  0.0274],\n",
            "         [-0.0676,  0.1687,  0.0773,  ...,  0.0815, -0.0531,  0.0291],\n",
            "         [-0.0165,  0.1633,  0.0665,  ...,  0.1139, -0.0514,  0.0316]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-8.3030e-02, -8.8696e-02,  5.6405e-02],\n",
            "          [-1.1761e-01, -5.9256e-02, -1.1711e-01],\n",
            "          [ 1.5644e-01,  2.1725e-01, -4.2496e-02],\n",
            "          ...,\n",
            "          [-7.6615e-01, -7.9065e-02, -1.0021e-01],\n",
            "          [-3.9480e-01, -3.4098e-01,  1.4657e-01],\n",
            "          [ 7.9221e-02,  1.5815e-02,  3.2712e-01]],\n",
            "\n",
            "         [[-5.7224e-02, -3.9745e-02,  4.2127e-02],\n",
            "          [-1.5437e-01, -6.8803e-02, -1.6909e-01],\n",
            "          [ 3.2035e-02,  2.0074e-01, -7.4395e-02],\n",
            "          ...,\n",
            "          [-7.9900e-01, -2.2357e-02, -1.9122e-01],\n",
            "          [-3.4681e-01, -2.6733e-01, -2.9074e-02],\n",
            "          [ 2.1267e-01,  1.8531e-01,  2.0572e-01]],\n",
            "\n",
            "         [[-5.6018e-02,  6.9011e-02,  2.7102e-02],\n",
            "          [-1.8008e-01, -2.3340e-02, -2.1959e-01],\n",
            "          [-3.4238e-02,  2.1588e-01, -4.7555e-02],\n",
            "          ...,\n",
            "          [-8.4588e-01,  6.3420e-02, -2.8718e-01],\n",
            "          [-3.5363e-01, -1.7783e-01, -2.7945e-01],\n",
            "          [ 3.0633e-01,  3.6795e-01,  2.2346e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8192e-01,  2.3636e-01, -3.7996e-01],\n",
            "          [-1.9759e-01,  9.0726e-02, -1.3230e-01],\n",
            "          [-1.0535e-01,  5.8996e-02,  3.4760e-01],\n",
            "          ...,\n",
            "          [-6.8156e-01,  1.6348e-01,  3.2892e-01],\n",
            "          [-2.7718e-01,  8.5146e-02,  2.8324e-01],\n",
            "          [-3.8680e-02,  3.3151e-02,  1.7670e-01]],\n",
            "\n",
            "         [[-1.9208e-01,  1.3622e-01, -4.0673e-01],\n",
            "          [-1.8033e-01,  9.0927e-02, -2.3512e-01],\n",
            "          [-2.2193e-01,  1.1962e-01,  2.3800e-01],\n",
            "          ...,\n",
            "          [-7.0516e-01,  7.3707e-02,  1.7365e-01],\n",
            "          [-2.5602e-01,  7.3233e-02,  1.3094e-01],\n",
            "          [-4.5423e-02,  6.8452e-03, -5.4642e-03]],\n",
            "\n",
            "         [[-2.6147e-01,  3.4860e-02, -3.8473e-01],\n",
            "          [-2.1793e-01,  2.7919e-02, -2.9259e-01],\n",
            "          [-3.8993e-01,  1.0602e-01,  1.2340e-01],\n",
            "          ...,\n",
            "          [-7.3379e-01, -3.9182e-02, -8.0695e-03],\n",
            "          [-2.4009e-01,  1.1575e-02, -3.1418e-04],\n",
            "          [-6.3659e-02, -4.3287e-02, -1.2410e-01]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-8.3030e-02, -1.1761e-01,  1.5644e-01,  ..., -2.2864e-01,\n",
            "            2.4534e-01, -6.1989e-02],\n",
            "          [-3.1318e-01, -2.1359e-01,  2.3457e-01,  ..., -2.8356e-01,\n",
            "            2.6426e-01,  1.0627e-01],\n",
            "          [-4.8006e-01, -5.2334e-01,  1.2584e-02,  ..., -3.4748e-01,\n",
            "           -1.0096e-01,  2.0827e-01],\n",
            "          ...,\n",
            "          [-6.4088e-01, -4.4632e-01,  3.0969e-01,  ..., -1.6443e-01,\n",
            "           -1.7530e-01, -3.5478e-01],\n",
            "          [-6.6675e-01, -3.5067e-01, -6.6261e-02,  ..., -6.0911e-01,\n",
            "           -1.3219e-01, -7.1414e-02],\n",
            "          [-5.1754e-01, -5.0882e-01, -4.7178e-01,  ..., -7.3379e-01,\n",
            "           -2.4009e-01, -6.3659e-02]],\n",
            "\n",
            "         [[-8.8696e-02, -5.9256e-02,  2.1725e-01,  ...,  3.3292e-01,\n",
            "            5.7585e-01,  5.1606e-01],\n",
            "          [ 1.1911e-01,  1.3090e-02,  3.2918e-01,  ...,  1.3990e-01,\n",
            "            4.1052e-01,  4.9614e-01],\n",
            "          [ 3.5875e-01,  3.5564e-01, -3.8676e-01,  ..., -5.7702e-02,\n",
            "           -4.8286e-02,  2.0855e-01],\n",
            "          ...,\n",
            "          [ 2.1864e-01,  7.0406e-03,  1.3729e-01,  ..., -9.5379e-02,\n",
            "            8.1632e-02,  1.7727e-01],\n",
            "          [ 4.5607e-01,  3.9267e-01, -2.8396e-01,  ..., -1.9163e-02,\n",
            "           -1.4668e-01, -1.0843e-01],\n",
            "          [ 4.6485e-01,  1.8233e-01, -2.9858e-01,  ..., -3.9182e-02,\n",
            "            1.1575e-02, -4.3287e-02]],\n",
            "\n",
            "         [[ 5.6405e-02, -1.1711e-01, -4.2496e-02,  ..., -2.9503e-01,\n",
            "           -1.6695e-01, -2.2305e-01],\n",
            "          [-3.8693e-01, -1.3794e-01,  2.2633e-02,  ..., -1.8218e-01,\n",
            "           -1.1214e-01, -1.3276e-01],\n",
            "          [-3.2084e-01,  4.6578e-02,  2.3543e-01,  ...,  6.5072e-02,\n",
            "           -2.1065e-01, -2.6329e-01],\n",
            "          ...,\n",
            "          [-2.4197e-01,  3.9471e-02,  4.0516e-01,  ..., -3.1215e-01,\n",
            "            2.2095e-01,  3.2704e-02],\n",
            "          [-1.1955e-01,  8.5163e-02,  3.6556e-01,  ...,  2.1741e-03,\n",
            "            1.1368e-01,  3.7567e-02],\n",
            "          [ 2.1025e-01,  3.2076e-01,  5.6665e-01,  ..., -8.0695e-03,\n",
            "           -3.1418e-04, -1.2410e-01]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-8.3030e-02, -8.8696e-02,  5.6405e-02],\n",
            "          [-1.1761e-01, -5.9256e-02, -1.1711e-01],\n",
            "          [ 1.5644e-01,  2.1725e-01, -4.2496e-02],\n",
            "          ...,\n",
            "          [-7.6615e-01, -7.9065e-02, -1.0021e-01],\n",
            "          [-3.9480e-01, -3.4098e-01,  1.4657e-01],\n",
            "          [ 7.9221e-02,  1.5815e-02,  3.2712e-01]],\n",
            "\n",
            "         [[-5.7224e-02, -3.9745e-02,  4.2127e-02],\n",
            "          [-1.5437e-01, -6.8803e-02, -1.6909e-01],\n",
            "          [ 3.2035e-02,  2.0074e-01, -7.4395e-02],\n",
            "          ...,\n",
            "          [-7.9900e-01, -2.2357e-02, -1.9122e-01],\n",
            "          [-3.4681e-01, -2.6733e-01, -2.9074e-02],\n",
            "          [ 2.1267e-01,  1.8531e-01,  2.0572e-01]],\n",
            "\n",
            "         [[-5.6018e-02,  6.9011e-02,  2.7102e-02],\n",
            "          [-1.8008e-01, -2.3340e-02, -2.1959e-01],\n",
            "          [-3.4238e-02,  2.1588e-01, -4.7555e-02],\n",
            "          ...,\n",
            "          [-8.4588e-01,  6.3420e-02, -2.8718e-01],\n",
            "          [-3.5363e-01, -1.7783e-01, -2.7945e-01],\n",
            "          [ 3.0633e-01,  3.6795e-01,  2.2346e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8192e-01,  2.3636e-01, -3.7996e-01],\n",
            "          [-1.9759e-01,  9.0726e-02, -1.3230e-01],\n",
            "          [-1.0535e-01,  5.8996e-02,  3.4760e-01],\n",
            "          ...,\n",
            "          [-6.8156e-01,  1.6348e-01,  3.2892e-01],\n",
            "          [-2.7718e-01,  8.5146e-02,  2.8324e-01],\n",
            "          [-3.8680e-02,  3.3151e-02,  1.7670e-01]],\n",
            "\n",
            "         [[-1.9208e-01,  1.3622e-01, -4.0673e-01],\n",
            "          [-1.8033e-01,  9.0927e-02, -2.3512e-01],\n",
            "          [-2.2193e-01,  1.1962e-01,  2.3800e-01],\n",
            "          ...,\n",
            "          [-7.0516e-01,  7.3707e-02,  1.7365e-01],\n",
            "          [-2.5602e-01,  7.3233e-02,  1.3094e-01],\n",
            "          [-4.5423e-02,  6.8452e-03, -5.4642e-03]],\n",
            "\n",
            "         [[-2.6147e-01,  3.4860e-02, -3.8473e-01],\n",
            "          [-2.1793e-01,  2.7919e-02, -2.9259e-01],\n",
            "          [-3.8993e-01,  1.0602e-01,  1.2340e-01],\n",
            "          ...,\n",
            "          [-7.3379e-01, -3.9182e-02, -8.0695e-03],\n",
            "          [-2.4009e-01,  1.1575e-02, -3.1418e-04],\n",
            "          [-6.3659e-02, -4.3287e-02, -1.2410e-01]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0043,  0.0311,  0.0571, -0.0019, -0.0028, -0.0565,  0.0467, -0.0437,\n",
            "          0.0927,  0.0033,  0.0866, -0.0874,  0.0798, -0.0360, -0.0586, -0.0794,\n",
            "         -0.0441, -0.0425,  0.0735,  0.0951, -0.0316, -0.0337,  0.0206,  0.0467,\n",
            "          0.0854, -0.0099, -0.0113,  0.0746,  0.0648,  0.0382,  0.0351, -0.0692,\n",
            "          0.1247, -0.0400,  0.0607,  0.1107, -0.1074,  0.0861, -0.0492,  0.0149,\n",
            "          0.0590, -0.0648,  0.0417,  0.0532,  0.0156,  0.0536, -0.1061,  0.0905,\n",
            "          0.0579,  0.0075,  0.0360, -0.0829, -0.0455, -0.0648,  0.0781,  0.0308,\n",
            "          0.0462,  0.0837,  0.0244,  0.0795,  0.0376, -0.0413,  0.0430,  0.0325,\n",
            "         -0.0805, -0.0084, -0.0373, -0.0787, -0.0839,  0.0035, -0.0093,  0.0584,\n",
            "          0.0280,  0.0535,  0.0327,  0.0038,  0.0872, -0.0904, -0.0009, -0.0803,\n",
            "          0.0445, -0.0582,  0.0469,  0.1123, -0.0678,  0.0753, -0.0527, -0.0883,\n",
            "          0.0308,  0.0213,  0.0451,  0.0135,  0.0433,  0.0750, -0.1072, -0.0881,\n",
            "          0.0241, -0.0105, -0.0797,  0.0171]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.0757, -0.0871,  0.0493,  ..., -0.0845,  0.1014, -0.1078],\n",
            "         [-0.0678, -0.0895,  0.0470,  ..., -0.0882,  0.1112, -0.1009],\n",
            "         [-0.0523, -0.0909,  0.0382,  ..., -0.0850,  0.1231, -0.0897],\n",
            "         ...,\n",
            "         [-0.0094, -0.0376, -0.2542,  ...,  0.0689,  0.0989,  0.0853],\n",
            "         [ 0.0069, -0.0368, -0.2246,  ...,  0.0825,  0.1039,  0.0707],\n",
            "         [ 0.0155, -0.0376, -0.1960,  ...,  0.0879,  0.1070,  0.0535]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 0.3932,  0.2011,  0.1780],\n",
            "          [ 0.4478,  0.0976,  0.0461],\n",
            "          [ 0.2977,  0.0685, -0.2577],\n",
            "          ...,\n",
            "          [ 0.1801,  0.5616, -0.1896],\n",
            "          [ 0.2494,  0.5540, -0.1127],\n",
            "          [ 0.4337,  0.4102,  0.2147]],\n",
            "\n",
            "         [[ 0.3972,  0.2694,  0.2530],\n",
            "          [ 0.5074,  0.1449,  0.0549],\n",
            "          [ 0.4421,  0.1265, -0.2866],\n",
            "          ...,\n",
            "          [ 0.1159,  0.6399, -0.2181],\n",
            "          [ 0.1823,  0.5531, -0.1367],\n",
            "          [ 0.3863,  0.4114,  0.2010]],\n",
            "\n",
            "         [[ 0.3868,  0.3128,  0.2591],\n",
            "          [ 0.5660,  0.1686,  0.0057],\n",
            "          [ 0.5404,  0.2005, -0.3043],\n",
            "          ...,\n",
            "          [ 0.0040,  0.7447, -0.2171],\n",
            "          [ 0.1272,  0.5548, -0.1747],\n",
            "          [ 0.3343,  0.4034,  0.1675]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.7045,  0.8745,  0.2109],\n",
            "          [ 0.8668,  0.5725, -0.1727],\n",
            "          [ 0.8081,  0.8913, -0.3596],\n",
            "          ...,\n",
            "          [-0.5360,  1.1668, -0.6673],\n",
            "          [-0.2619,  1.4085, -0.5574],\n",
            "          [-0.0758,  1.1660, -0.7661]],\n",
            "\n",
            "         [[ 0.7122,  0.9141,  0.3813],\n",
            "          [ 0.8516,  0.5617,  0.0158],\n",
            "          [ 0.8409,  0.8717, -0.2892],\n",
            "          ...,\n",
            "          [-0.6500,  0.9948, -0.5875],\n",
            "          [-0.3778,  1.1839, -0.4232],\n",
            "          [-0.0633,  1.1162, -0.6480]],\n",
            "\n",
            "         [[ 0.6675,  0.8583,  0.4361],\n",
            "          [ 0.8294,  0.5278,  0.1023],\n",
            "          [ 0.8419,  0.8178, -0.2348],\n",
            "          ...,\n",
            "          [-0.6947,  0.8512, -0.4383],\n",
            "          [-0.3950,  1.0367, -0.2679],\n",
            "          [-0.0149,  1.0600, -0.5372]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 0.3932,  0.4478,  0.2977,  ...,  0.5942,  0.5891,  0.2541],\n",
            "          [ 0.4147,  0.2605, -0.0546,  ...,  0.2126,  0.2616,  0.2556],\n",
            "          [ 0.4576,  0.2519,  0.2923,  ..., -0.1271,  0.2221,  0.1766],\n",
            "          ...,\n",
            "          [ 0.4800,  0.1764, -0.3479,  ...,  0.4391,  0.3999,  0.1663],\n",
            "          [ 0.5958,  0.4028,  0.1816,  ..., -0.1908,  0.0019,  0.1036],\n",
            "          [ 0.4849,  0.5739,  0.6767,  ..., -0.6947, -0.3950, -0.0149]],\n",
            "\n",
            "         [[ 0.2011,  0.0976,  0.0685,  ...,  0.3240,  0.4924,  1.0576],\n",
            "          [-0.0753, -0.0223,  0.1692,  ...,  0.2899,  0.3900,  0.8213],\n",
            "          [ 0.1841,  0.2776,  0.3292,  ...,  0.4240,  0.4698,  0.6024],\n",
            "          ...,\n",
            "          [-0.0281, -0.0532,  0.2181,  ...,  0.5314,  0.6007,  0.8534],\n",
            "          [ 0.2859,  0.5238,  0.7081,  ...,  0.5071,  0.5145,  0.6634],\n",
            "          [ 0.5584,  1.0243,  1.4408,  ...,  0.8512,  1.0367,  1.0600]],\n",
            "\n",
            "         [[ 0.1780,  0.0461, -0.2577,  ..., -0.0439, -0.0692, -0.2397],\n",
            "          [ 0.3258,  0.0826, -0.1681,  ..., -0.2538, -0.1326, -0.1945],\n",
            "          [ 0.1116, -0.1952, -0.1469,  ..., -0.0368,  0.1973,  0.1420],\n",
            "          ...,\n",
            "          [-0.6393, -0.4568, -0.2646,  ..., -0.1156, -0.1267, -0.2179],\n",
            "          [-0.3882, -0.1931, -0.1628,  ..., -0.3340, -0.0068, -0.1856],\n",
            "          [-0.1639, -0.0421, -0.0435,  ..., -0.4383, -0.2679, -0.5372]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 0.3932,  0.2011,  0.1780],\n",
            "          [ 0.4478,  0.0976,  0.0461],\n",
            "          [ 0.2977,  0.0685, -0.2577],\n",
            "          ...,\n",
            "          [ 0.1801,  0.5616, -0.1896],\n",
            "          [ 0.2494,  0.5540, -0.1127],\n",
            "          [ 0.4337,  0.4102,  0.2147]],\n",
            "\n",
            "         [[ 0.3972,  0.2694,  0.2530],\n",
            "          [ 0.5074,  0.1449,  0.0549],\n",
            "          [ 0.4421,  0.1265, -0.2866],\n",
            "          ...,\n",
            "          [ 0.1159,  0.6399, -0.2181],\n",
            "          [ 0.1823,  0.5531, -0.1367],\n",
            "          [ 0.3863,  0.4114,  0.2010]],\n",
            "\n",
            "         [[ 0.3868,  0.3128,  0.2591],\n",
            "          [ 0.5660,  0.1686,  0.0057],\n",
            "          [ 0.5404,  0.2005, -0.3043],\n",
            "          ...,\n",
            "          [ 0.0040,  0.7447, -0.2171],\n",
            "          [ 0.1272,  0.5548, -0.1747],\n",
            "          [ 0.3343,  0.4034,  0.1675]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.7045,  0.8745,  0.2109],\n",
            "          [ 0.8668,  0.5725, -0.1727],\n",
            "          [ 0.8081,  0.8913, -0.3596],\n",
            "          ...,\n",
            "          [-0.5360,  1.1668, -0.6673],\n",
            "          [-0.2619,  1.4085, -0.5574],\n",
            "          [-0.0758,  1.1660, -0.7661]],\n",
            "\n",
            "         [[ 0.7122,  0.9141,  0.3813],\n",
            "          [ 0.8516,  0.5617,  0.0158],\n",
            "          [ 0.8409,  0.8717, -0.2892],\n",
            "          ...,\n",
            "          [-0.6500,  0.9948, -0.5875],\n",
            "          [-0.3778,  1.1839, -0.4232],\n",
            "          [-0.0633,  1.1162, -0.6480]],\n",
            "\n",
            "         [[ 0.6675,  0.8583,  0.4361],\n",
            "          [ 0.8294,  0.5278,  0.1023],\n",
            "          [ 0.8419,  0.8178, -0.2348],\n",
            "          ...,\n",
            "          [-0.6947,  0.8512, -0.4383],\n",
            "          [-0.3950,  1.0367, -0.2679],\n",
            "          [-0.0149,  1.0600, -0.5372]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0034,  0.0318,  0.0561, -0.0021, -0.0010, -0.0578,  0.0463, -0.0428,\n",
            "          0.0931,  0.0023,  0.0879, -0.0877,  0.0783, -0.0354, -0.0578, -0.0790,\n",
            "         -0.0441, -0.0420,  0.0730,  0.0938, -0.0319, -0.0327,  0.0213,  0.0475,\n",
            "          0.0860, -0.0094, -0.0099,  0.0735,  0.0645,  0.0381,  0.0346, -0.0686,\n",
            "          0.1225, -0.0406,  0.0610,  0.1110, -0.1078,  0.0871, -0.0472,  0.0145,\n",
            "          0.0599, -0.0655,  0.0406,  0.0519,  0.0165,  0.0538, -0.1053,  0.0895,\n",
            "          0.0582,  0.0063,  0.0343, -0.0821, -0.0448, -0.0620,  0.0773,  0.0301,\n",
            "          0.0473,  0.0847,  0.0235,  0.0803,  0.0379, -0.0414,  0.0412,  0.0337,\n",
            "         -0.0821, -0.0093, -0.0366, -0.0791, -0.0826,  0.0039, -0.0104,  0.0574,\n",
            "          0.0287,  0.0520,  0.0324,  0.0028,  0.0851, -0.0902, -0.0023, -0.0806,\n",
            "          0.0445, -0.0571,  0.0476,  0.1134, -0.0662,  0.0747, -0.0540, -0.0888,\n",
            "          0.0300,  0.0215,  0.0436,  0.0144,  0.0441,  0.0759, -0.1077, -0.0888,\n",
            "          0.0243, -0.0111, -0.0783,  0.0154]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.0463,  0.1257, -0.0945,  ..., -0.0367,  0.0660, -0.0968],\n",
            "         [-0.0582,  0.1314, -0.0909,  ..., -0.0370,  0.0885, -0.0966],\n",
            "         [-0.0758,  0.1341, -0.0879,  ..., -0.0372,  0.1160, -0.0892],\n",
            "         ...,\n",
            "         [-0.0903, -0.0589, -0.1500,  ..., -0.0304,  0.0676,  0.1799],\n",
            "         [-0.1098, -0.0542, -0.1492,  ..., -0.0305,  0.0829,  0.1751],\n",
            "         [-0.1211, -0.0477, -0.1474,  ..., -0.0307,  0.0895,  0.1649]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-0.3529,  0.0136,  1.1165],\n",
            "          [-1.2033,  0.1861,  1.1179],\n",
            "          [-0.9853,  0.0584,  0.5094],\n",
            "          ...,\n",
            "          [ 0.9402,  0.0346,  0.4472],\n",
            "          [ 0.7209,  0.4075, -0.0963],\n",
            "          [ 0.4505,  0.8867, -0.3536]],\n",
            "\n",
            "         [[-0.2432,  0.0851,  1.1516],\n",
            "          [-1.0977,  0.2356,  1.2208],\n",
            "          [-0.9066,  0.1730,  0.5418],\n",
            "          ...,\n",
            "          [ 1.0086, -0.0901,  0.3920],\n",
            "          [ 0.7986,  0.3758, -0.1132],\n",
            "          [ 0.4129,  0.8971, -0.4068]],\n",
            "\n",
            "         [[-0.0970,  0.2159,  1.1619],\n",
            "          [-0.9018,  0.2912,  1.2790],\n",
            "          [-0.7560,  0.2779,  0.5693],\n",
            "          ...,\n",
            "          [ 1.0302, -0.2107,  0.3188],\n",
            "          [ 0.8601,  0.3322, -0.1094],\n",
            "          [ 0.3759,  0.8834, -0.3734]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3907, -0.0041,  0.8408],\n",
            "          [-0.5311, -0.0917,  0.6937],\n",
            "          [ 0.0055,  0.1428,  0.4271],\n",
            "          ...,\n",
            "          [-0.2831,  0.5826, -0.1907],\n",
            "          [ 0.1961,  0.5460, -0.3340],\n",
            "          [ 0.2457,  0.7240, -0.0126]],\n",
            "\n",
            "         [[-0.3908, -0.0060,  0.6709],\n",
            "          [-0.5184, -0.0874,  0.5866],\n",
            "          [ 0.0331,  0.1872,  0.3804],\n",
            "          ...,\n",
            "          [-0.2649,  0.5170, -0.1774],\n",
            "          [ 0.2396,  0.5322, -0.2513],\n",
            "          [ 0.3173,  0.6681, -0.0327]],\n",
            "\n",
            "         [[-0.3506,  0.0403,  0.5201],\n",
            "          [-0.4532, -0.0380,  0.4910],\n",
            "          [ 0.0949,  0.2512,  0.3744],\n",
            "          ...,\n",
            "          [-0.2855,  0.4579, -0.1798],\n",
            "          [ 0.2427,  0.4988, -0.1634],\n",
            "          [ 0.3561,  0.6160, -0.0293]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-0.3529, -1.2033, -0.9853,  ..., -0.3540,  0.2839,  1.2449],\n",
            "          [ 0.3172, -0.3677, -0.2911,  ..., -0.2140,  0.2666,  0.9762],\n",
            "          [ 0.7620,  0.6699,  0.3448,  ...,  0.1338,  0.2958,  0.6152],\n",
            "          ...,\n",
            "          [ 0.3070,  0.0374,  0.0172,  ..., -0.3094,  0.1330,  0.8331],\n",
            "          [-0.4002, -0.3721, -0.0093,  ..., -0.4051,  0.3639,  0.8185],\n",
            "          [-0.7879, -0.6105, -0.2327,  ..., -0.2855,  0.2427,  0.3561]],\n",
            "\n",
            "         [[ 0.0136,  0.1861,  0.0584,  ...,  0.4697,  0.6206,  0.8079],\n",
            "          [ 0.3532,  0.1794,  0.2552,  ...,  0.1788,  0.5343,  0.6721],\n",
            "          [ 0.2495, -0.0342,  0.1602,  ..., -0.0625,  0.1287,  0.4834],\n",
            "          ...,\n",
            "          [ 0.5505,  0.1906, -0.3155,  ..., -0.1543,  0.2232,  0.3178],\n",
            "          [ 0.9789,  0.3855,  0.1775,  ...,  0.1605,  0.5883,  0.7673],\n",
            "          [ 1.2940,  0.7911,  0.3592,  ...,  0.4579,  0.4988,  0.6160]],\n",
            "\n",
            "         [[ 1.1165,  1.1179,  0.5094,  ...,  0.9548,  0.5543,  0.3624],\n",
            "          [ 1.1803,  0.7789,  0.3006,  ...,  0.6488,  0.5478,  0.7483],\n",
            "          [ 0.9055,  0.4601, -0.1027,  ...,  0.1086, -0.0536,  0.1516],\n",
            "          ...,\n",
            "          [ 1.0201,  0.6929,  0.1159,  ...,  0.3279,  0.2239, -0.1310],\n",
            "          [ 0.6473,  0.3169, -0.2000,  ...,  0.0394, -0.1747, -0.3084],\n",
            "          [ 0.3081,  0.1508, -0.0575,  ..., -0.1798, -0.1634, -0.0293]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-0.3529,  0.0136,  1.1165],\n",
            "          [-1.2033,  0.1861,  1.1179],\n",
            "          [-0.9853,  0.0584,  0.5094],\n",
            "          ...,\n",
            "          [ 0.9402,  0.0346,  0.4472],\n",
            "          [ 0.7209,  0.4075, -0.0963],\n",
            "          [ 0.4505,  0.8867, -0.3536]],\n",
            "\n",
            "         [[-0.2432,  0.0851,  1.1516],\n",
            "          [-1.0977,  0.2356,  1.2208],\n",
            "          [-0.9066,  0.1730,  0.5418],\n",
            "          ...,\n",
            "          [ 1.0086, -0.0901,  0.3920],\n",
            "          [ 0.7986,  0.3758, -0.1132],\n",
            "          [ 0.4129,  0.8971, -0.4068]],\n",
            "\n",
            "         [[-0.0970,  0.2159,  1.1619],\n",
            "          [-0.9018,  0.2912,  1.2790],\n",
            "          [-0.7560,  0.2779,  0.5693],\n",
            "          ...,\n",
            "          [ 1.0302, -0.2107,  0.3188],\n",
            "          [ 0.8601,  0.3322, -0.1094],\n",
            "          [ 0.3759,  0.8834, -0.3734]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3907, -0.0041,  0.8408],\n",
            "          [-0.5311, -0.0917,  0.6937],\n",
            "          [ 0.0055,  0.1428,  0.4271],\n",
            "          ...,\n",
            "          [-0.2831,  0.5826, -0.1907],\n",
            "          [ 0.1961,  0.5460, -0.3340],\n",
            "          [ 0.2457,  0.7240, -0.0126]],\n",
            "\n",
            "         [[-0.3908, -0.0060,  0.6709],\n",
            "          [-0.5184, -0.0874,  0.5866],\n",
            "          [ 0.0331,  0.1872,  0.3804],\n",
            "          ...,\n",
            "          [-0.2649,  0.5170, -0.1774],\n",
            "          [ 0.2396,  0.5322, -0.2513],\n",
            "          [ 0.3173,  0.6681, -0.0327]],\n",
            "\n",
            "         [[-0.3506,  0.0403,  0.5201],\n",
            "          [-0.4532, -0.0380,  0.4910],\n",
            "          [ 0.0949,  0.2512,  0.3744],\n",
            "          ...,\n",
            "          [-0.2855,  0.4579, -0.1798],\n",
            "          [ 0.2427,  0.4988, -0.1634],\n",
            "          [ 0.3561,  0.6160, -0.0293]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0043,  0.0314,  0.0552, -0.0011, -0.0013, -0.0580,  0.0468, -0.0443,\n",
            "          0.0937,  0.0026,  0.0877, -0.0887,  0.0785, -0.0358, -0.0579, -0.0792,\n",
            "         -0.0446, -0.0417,  0.0737,  0.0944, -0.0316, -0.0327,  0.0207,  0.0473,\n",
            "          0.0857, -0.0096, -0.0110,  0.0729,  0.0638,  0.0378,  0.0354, -0.0695,\n",
            "          0.1235, -0.0411,  0.0609,  0.1123, -0.1084,  0.0872, -0.0481,  0.0141,\n",
            "          0.0587, -0.0660,  0.0406,  0.0515,  0.0159,  0.0536, -0.1061,  0.0898,\n",
            "          0.0588,  0.0062,  0.0358, -0.0829, -0.0464, -0.0616,  0.0772,  0.0310,\n",
            "          0.0465,  0.0854,  0.0238,  0.0802,  0.0371, -0.0413,  0.0422,  0.0336,\n",
            "         -0.0820, -0.0092, -0.0375, -0.0786, -0.0825,  0.0035, -0.0113,  0.0579,\n",
            "          0.0283,  0.0524,  0.0318,  0.0030,  0.0852, -0.0902, -0.0025, -0.0802,\n",
            "          0.0449, -0.0576,  0.0477,  0.1125, -0.0660,  0.0754, -0.0540, -0.0892,\n",
            "          0.0302,  0.0205,  0.0447,  0.0139,  0.0436,  0.0773, -0.1083, -0.0880,\n",
            "          0.0240, -0.0115, -0.0783,  0.0157]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.0422,  0.0183, -0.1044,  ...,  0.1385, -0.2115, -0.0183],\n",
            "         [ 0.0395,  0.0198, -0.1065,  ...,  0.1379, -0.2859, -0.0184],\n",
            "         [ 0.0356,  0.0201, -0.1048,  ...,  0.1328, -0.3699, -0.0198],\n",
            "         ...,\n",
            "         [ 0.0327, -0.0246,  0.0527,  ...,  0.0432, -0.1407, -0.0629],\n",
            "         [ 0.0291, -0.0257,  0.0453,  ...,  0.0315, -0.1839, -0.0613],\n",
            "         [ 0.0269, -0.0254,  0.0343,  ...,  0.0254, -0.2063, -0.0587]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-5.3276e-01,  4.6759e-01, -5.8171e-01],\n",
            "          [-5.8940e-01, -7.3531e-02, -6.6599e-01],\n",
            "          [-5.0968e-01, -2.6811e-01, -9.8189e-01],\n",
            "          ...,\n",
            "          [-3.1565e-01,  5.2112e-01, -4.3417e-01],\n",
            "          [-7.3129e-01,  3.6713e-01, -4.5763e-01],\n",
            "          [-6.8352e-01,  1.3976e-01, -1.3035e-01]],\n",
            "\n",
            "         [[-5.2250e-01,  4.5700e-01, -5.5229e-01],\n",
            "          [-5.2002e-01, -1.1314e-01, -7.1392e-01],\n",
            "          [-5.0159e-01, -3.0234e-01, -9.5111e-01],\n",
            "          ...,\n",
            "          [-2.9645e-01,  4.7974e-01, -4.6991e-01],\n",
            "          [-7.4547e-01,  2.6967e-01, -5.6762e-01],\n",
            "          [-7.0389e-01,  4.0586e-02, -1.7949e-01]],\n",
            "\n",
            "         [[-5.0522e-01,  4.0490e-01, -5.0612e-01],\n",
            "          [-4.7288e-01, -1.6752e-01, -7.6316e-01],\n",
            "          [-4.8900e-01, -3.2613e-01, -8.8109e-01],\n",
            "          ...,\n",
            "          [-2.7172e-01,  4.0888e-01, -5.0203e-01],\n",
            "          [-7.5637e-01,  1.4390e-01, -6.8339e-01],\n",
            "          [-6.6820e-01, -4.4702e-02, -1.9410e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.9134e-02,  7.7250e-02, -1.2619e+00],\n",
            "          [-3.8744e-01, -8.5281e-02, -9.0726e-01],\n",
            "          [-3.7576e-01,  2.5926e-02, -6.3901e-01],\n",
            "          ...,\n",
            "          [ 3.3789e-02,  6.3949e-01,  1.2061e-01],\n",
            "          [-7.2335e-02,  7.9743e-01,  5.1633e-01],\n",
            "          [-1.5380e-02,  3.7006e-01,  8.7579e-01]],\n",
            "\n",
            "         [[ 1.8006e-01,  7.7660e-02, -1.1862e+00],\n",
            "          [-3.0286e-01, -1.1448e-01, -8.5412e-01],\n",
            "          [-2.9608e-01, -1.5415e-02, -5.0047e-01],\n",
            "          ...,\n",
            "          [-2.4880e-03,  6.3264e-01, -6.7732e-02],\n",
            "          [-6.2157e-02,  9.0250e-01,  2.5640e-01],\n",
            "          [ 4.2669e-02,  4.9058e-01,  7.1719e-01]],\n",
            "\n",
            "         [[ 2.3840e-01,  1.4301e-01, -1.0993e+00],\n",
            "          [-2.2199e-01, -9.7722e-02, -8.0058e-01],\n",
            "          [-2.2078e-01,  1.6552e-02, -3.9621e-01],\n",
            "          ...,\n",
            "          [-2.7913e-02,  6.3236e-01, -2.0238e-01],\n",
            "          [-9.4461e-02,  9.3823e-01,  1.6708e-02],\n",
            "          [ 2.5427e-04,  5.9119e-01,  4.9948e-01]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-5.3276e-01, -5.8940e-01, -5.0968e-01,  ..., -3.4267e-02,\n",
            "           -4.2601e-02, -7.1265e-02],\n",
            "          [-2.4677e-01, -2.1997e-01, -2.9833e-01,  ..., -6.1202e-02,\n",
            "           -2.2050e-01, -4.1841e-01],\n",
            "          [-2.4203e-01, -7.2806e-02, -4.6038e-01,  ..., -1.9709e-02,\n",
            "           -5.4689e-01, -7.7438e-01],\n",
            "          ...,\n",
            "          [-2.8621e-01, -3.0049e-01, -2.9338e-01,  ..., -4.2711e-02,\n",
            "           -2.0742e-01, -1.9172e-02],\n",
            "          [-2.6372e-03, -3.4753e-02, -3.5588e-01,  ...,  3.6001e-01,\n",
            "            1.0549e-02, -1.0670e-01],\n",
            "          [-9.4883e-02, -3.9145e-01, -7.3106e-01,  ..., -2.7913e-02,\n",
            "           -9.4461e-02,  2.5427e-04]],\n",
            "\n",
            "         [[ 4.6759e-01, -7.3531e-02, -2.6811e-01,  ..., -1.0833e-01,\n",
            "            2.9541e-02,  8.6900e-02],\n",
            "          [ 3.5639e-01,  9.5709e-02, -2.2870e-01,  ...,  5.8232e-02,\n",
            "            1.8327e-02, -2.0305e-01],\n",
            "          [ 2.4077e-01,  2.5582e-01,  6.0876e-02,  ...,  8.7646e-02,\n",
            "           -1.6048e-01, -6.2851e-02],\n",
            "          ...,\n",
            "          [ 3.6428e-01,  2.7947e-01,  5.2150e-02,  ..., -1.4198e-02,\n",
            "            9.1438e-02,  2.8630e-01],\n",
            "          [ 1.2628e-02,  5.6714e-02,  1.4265e-02,  ...,  4.0607e-01,\n",
            "            3.2527e-01,  1.7536e-01],\n",
            "          [ 4.2822e-01,  2.9530e-01,  1.3977e-01,  ...,  6.3236e-01,\n",
            "            9.3823e-01,  5.9119e-01]],\n",
            "\n",
            "         [[-5.8171e-01, -6.6599e-01, -9.8189e-01,  ..., -6.7909e-01,\n",
            "           -4.4826e-01, -2.4913e-01],\n",
            "          [-2.3763e-01, -1.5874e-01, -7.3612e-01,  ..., -5.1706e-02,\n",
            "           -2.7017e-01, -3.8430e-01],\n",
            "          [-1.8477e-01, -1.9395e-01, -1.7195e-01,  ..., -2.5290e-01,\n",
            "           -5.1527e-01, -3.8289e-01],\n",
            "          ...,\n",
            "          [-3.5894e-01, -2.3820e-01, -8.7775e-01,  ..., -5.0774e-01,\n",
            "           -2.4560e-01, -1.3643e-01],\n",
            "          [ 3.9294e-02,  9.9711e-02,  4.3185e-01,  ..., -1.3273e-01,\n",
            "            4.8948e-03,  1.5666e-01],\n",
            "          [ 6.0456e-01,  3.4670e-01,  4.1853e-01,  ..., -2.0238e-01,\n",
            "            1.6708e-02,  4.9948e-01]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-5.3276e-01,  4.6759e-01, -5.8171e-01],\n",
            "          [-5.8940e-01, -7.3531e-02, -6.6599e-01],\n",
            "          [-5.0968e-01, -2.6811e-01, -9.8189e-01],\n",
            "          ...,\n",
            "          [-3.1565e-01,  5.2112e-01, -4.3417e-01],\n",
            "          [-7.3129e-01,  3.6713e-01, -4.5763e-01],\n",
            "          [-6.8352e-01,  1.3976e-01, -1.3035e-01]],\n",
            "\n",
            "         [[-5.2250e-01,  4.5700e-01, -5.5229e-01],\n",
            "          [-5.2002e-01, -1.1314e-01, -7.1392e-01],\n",
            "          [-5.0159e-01, -3.0234e-01, -9.5111e-01],\n",
            "          ...,\n",
            "          [-2.9645e-01,  4.7974e-01, -4.6991e-01],\n",
            "          [-7.4547e-01,  2.6967e-01, -5.6762e-01],\n",
            "          [-7.0389e-01,  4.0586e-02, -1.7949e-01]],\n",
            "\n",
            "         [[-5.0522e-01,  4.0490e-01, -5.0612e-01],\n",
            "          [-4.7288e-01, -1.6752e-01, -7.6316e-01],\n",
            "          [-4.8900e-01, -3.2613e-01, -8.8109e-01],\n",
            "          ...,\n",
            "          [-2.7172e-01,  4.0888e-01, -5.0203e-01],\n",
            "          [-7.5637e-01,  1.4390e-01, -6.8339e-01],\n",
            "          [-6.6820e-01, -4.4702e-02, -1.9410e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.9134e-02,  7.7250e-02, -1.2619e+00],\n",
            "          [-3.8744e-01, -8.5281e-02, -9.0726e-01],\n",
            "          [-3.7576e-01,  2.5926e-02, -6.3901e-01],\n",
            "          ...,\n",
            "          [ 3.3789e-02,  6.3949e-01,  1.2061e-01],\n",
            "          [-7.2335e-02,  7.9743e-01,  5.1633e-01],\n",
            "          [-1.5380e-02,  3.7006e-01,  8.7579e-01]],\n",
            "\n",
            "         [[ 1.8006e-01,  7.7660e-02, -1.1862e+00],\n",
            "          [-3.0286e-01, -1.1448e-01, -8.5412e-01],\n",
            "          [-2.9608e-01, -1.5415e-02, -5.0047e-01],\n",
            "          ...,\n",
            "          [-2.4880e-03,  6.3264e-01, -6.7732e-02],\n",
            "          [-6.2157e-02,  9.0250e-01,  2.5640e-01],\n",
            "          [ 4.2669e-02,  4.9058e-01,  7.1719e-01]],\n",
            "\n",
            "         [[ 2.3840e-01,  1.4301e-01, -1.0993e+00],\n",
            "          [-2.2199e-01, -9.7722e-02, -8.0058e-01],\n",
            "          [-2.2078e-01,  1.6552e-02, -3.9621e-01],\n",
            "          ...,\n",
            "          [-2.7913e-02,  6.3236e-01, -2.0238e-01],\n",
            "          [-9.4461e-02,  9.3823e-01,  1.6708e-02],\n",
            "          [ 2.5427e-04,  5.9119e-01,  4.9948e-01]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0049,  0.0320,  0.0561, -0.0026, -0.0024, -0.0569,  0.0459, -0.0440,\n",
            "          0.0924,  0.0027,  0.0865, -0.0876,  0.0799, -0.0364, -0.0589, -0.0788,\n",
            "         -0.0432, -0.0427,  0.0724,  0.0950, -0.0315, -0.0346,  0.0204,  0.0478,\n",
            "          0.0854, -0.0104, -0.0105,  0.0759,  0.0645,  0.0376,  0.0350, -0.0676,\n",
            "          0.1234, -0.0412,  0.0617,  0.1093, -0.1072,  0.0864, -0.0473,  0.0135,\n",
            "          0.0587, -0.0642,  0.0418,  0.0525,  0.0167,  0.0533, -0.1051,  0.0901,\n",
            "          0.0584,  0.0080,  0.0335, -0.0822, -0.0443, -0.0648,  0.0774,  0.0291,\n",
            "          0.0466,  0.0846,  0.0242,  0.0799,  0.0372, -0.0415,  0.0408,  0.0327,\n",
            "         -0.0816, -0.0081, -0.0366, -0.0794, -0.0831,  0.0034, -0.0103,  0.0575,\n",
            "          0.0285,  0.0516,  0.0316,  0.0035,  0.0866, -0.0899, -0.0013, -0.0798,\n",
            "          0.0446, -0.0579,  0.0470,  0.1139, -0.0676,  0.0751, -0.0539, -0.0886,\n",
            "          0.0304,  0.0216,  0.0440,  0.0138,  0.0445,  0.0750, -0.1070, -0.0880,\n",
            "          0.0236, -0.0111, -0.0791,  0.0148]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.1311,  0.0815, -0.0221,  ..., -0.1633, -0.1294, -0.1384],\n",
            "         [ 0.1402,  0.0812, -0.0188,  ..., -0.1708, -0.1205, -0.1207],\n",
            "         [ 0.1520,  0.0811, -0.0153,  ..., -0.1716, -0.1075, -0.0913],\n",
            "         ...,\n",
            "         [ 0.1586,  0.0878,  0.0604,  ...,  0.0277, -0.1094,  0.2031],\n",
            "         [ 0.1684,  0.0880,  0.0494,  ...,  0.0311, -0.0972,  0.1981],\n",
            "         [ 0.1736,  0.0879,  0.0379,  ...,  0.0315, -0.0914,  0.1811]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-0.6953,  0.0849,  0.1194],\n",
            "          [-0.5640, -0.2676, -0.1383],\n",
            "          [-0.3103, -0.1352, -0.4884],\n",
            "          ...,\n",
            "          [-0.6599,  0.6816, -0.1346],\n",
            "          [-0.5962,  0.4311,  0.0927],\n",
            "          [-0.6850,  0.4522, -0.1360]],\n",
            "\n",
            "         [[-0.5219,  0.1219,  0.2400],\n",
            "          [-0.5597, -0.2217, -0.0239],\n",
            "          [-0.2932, -0.0251, -0.3137],\n",
            "          ...,\n",
            "          [-0.7210,  0.7061, -0.0150],\n",
            "          [-0.6574,  0.4143,  0.1503],\n",
            "          [-0.6964,  0.4276, -0.0854]],\n",
            "\n",
            "         [[-0.3683,  0.1573,  0.3708],\n",
            "          [-0.5653, -0.1409,  0.1358],\n",
            "          [-0.2688,  0.1190, -0.1014],\n",
            "          ...,\n",
            "          [-0.7190,  0.7421,  0.1628],\n",
            "          [-0.6763,  0.3741,  0.2515],\n",
            "          [-0.6653,  0.3202, -0.0478]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3990,  0.2546,  1.4120],\n",
            "          [ 0.0140,  0.2574,  1.0134],\n",
            "          [-0.0198, -0.2974,  0.2153],\n",
            "          ...,\n",
            "          [ 0.1694,  0.7200, -0.0760],\n",
            "          [ 0.7356,  0.6204, -0.4625],\n",
            "          [ 0.5383,  0.5157, -0.0714]],\n",
            "\n",
            "         [[-0.3760,  0.1314,  1.4638],\n",
            "          [ 0.0452,  0.1733,  1.0339],\n",
            "          [ 0.1456, -0.2045,  0.3654],\n",
            "          ...,\n",
            "          [ 0.2010,  0.6613,  0.0969],\n",
            "          [ 0.7318,  0.5294, -0.3635],\n",
            "          [ 0.5614,  0.5596,  0.0596]],\n",
            "\n",
            "         [[-0.3714,  0.0257,  1.4531],\n",
            "          [ 0.0286,  0.0683,  0.9976],\n",
            "          [ 0.2573, -0.1248,  0.4739],\n",
            "          ...,\n",
            "          [ 0.2711,  0.6060,  0.2908],\n",
            "          [ 0.7036,  0.4307, -0.2162],\n",
            "          [ 0.5867,  0.5950,  0.1838]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-0.6953, -0.5640, -0.3103,  ..., -0.1136,  0.1742,  0.4836],\n",
            "          [-0.4124, -0.2482, -0.1290,  ..., -0.0726,  0.0884,  0.3505],\n",
            "          [-0.5764, -0.3855, -0.2213,  ...,  0.1671, -0.0233,  0.1089],\n",
            "          ...,\n",
            "          [ 0.0462,  0.0115, -0.2114,  ..., -0.0265,  0.3563,  0.3805],\n",
            "          [ 0.4772,  0.5164,  0.1649,  ...,  0.3125,  0.2679,  0.3914],\n",
            "          [ 0.2689,  0.4339,  0.1742,  ...,  0.2711,  0.7036,  0.5867]],\n",
            "\n",
            "         [[ 0.0849, -0.2676, -0.1352,  ...,  0.2311,  0.5133,  0.4631],\n",
            "          [ 0.4489, -0.0142, -0.2050,  ...,  0.1108,  0.2980,  0.4573],\n",
            "          [ 0.7106,  0.4490,  0.1211,  ...,  0.3967, -0.0258,  0.0099],\n",
            "          ...,\n",
            "          [ 0.5471, -0.0608, -0.4223,  ...,  0.2700,  0.2076,  0.0159],\n",
            "          [ 0.8556,  0.5909, -0.0051,  ...,  0.4791,  0.1925,  0.1923],\n",
            "          [ 0.7301,  0.5722,  0.2435,  ...,  0.6060,  0.4307,  0.5950]],\n",
            "\n",
            "         [[ 0.1194, -0.1383, -0.4884,  ...,  0.3289,  0.2961,  0.4207],\n",
            "          [-0.0481,  0.0896, -0.2208,  ...,  0.4595,  0.1111,  0.4143],\n",
            "          [-0.2219, -0.1014,  0.0771,  ...,  0.6192,  0.0644,  0.1553],\n",
            "          ...,\n",
            "          [ 0.2022,  0.1074, -0.2308,  ...,  0.8036,  0.4135,  0.4232],\n",
            "          [-0.1511,  0.0386,  0.0681,  ...,  0.5735,  0.0386,  0.4406],\n",
            "          [-0.4404, -0.1933, -0.1184,  ...,  0.2908, -0.2162,  0.1838]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-0.6953,  0.0849,  0.1194],\n",
            "          [-0.5640, -0.2676, -0.1383],\n",
            "          [-0.3103, -0.1352, -0.4884],\n",
            "          ...,\n",
            "          [-0.6599,  0.6816, -0.1346],\n",
            "          [-0.5962,  0.4311,  0.0927],\n",
            "          [-0.6850,  0.4522, -0.1360]],\n",
            "\n",
            "         [[-0.5219,  0.1219,  0.2400],\n",
            "          [-0.5597, -0.2217, -0.0239],\n",
            "          [-0.2932, -0.0251, -0.3137],\n",
            "          ...,\n",
            "          [-0.7210,  0.7061, -0.0150],\n",
            "          [-0.6574,  0.4143,  0.1503],\n",
            "          [-0.6964,  0.4276, -0.0854]],\n",
            "\n",
            "         [[-0.3683,  0.1573,  0.3708],\n",
            "          [-0.5653, -0.1409,  0.1358],\n",
            "          [-0.2688,  0.1190, -0.1014],\n",
            "          ...,\n",
            "          [-0.7190,  0.7421,  0.1628],\n",
            "          [-0.6763,  0.3741,  0.2515],\n",
            "          [-0.6653,  0.3202, -0.0478]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3990,  0.2546,  1.4120],\n",
            "          [ 0.0140,  0.2574,  1.0134],\n",
            "          [-0.0198, -0.2974,  0.2153],\n",
            "          ...,\n",
            "          [ 0.1694,  0.7200, -0.0760],\n",
            "          [ 0.7356,  0.6204, -0.4625],\n",
            "          [ 0.5383,  0.5157, -0.0714]],\n",
            "\n",
            "         [[-0.3760,  0.1314,  1.4638],\n",
            "          [ 0.0452,  0.1733,  1.0339],\n",
            "          [ 0.1456, -0.2045,  0.3654],\n",
            "          ...,\n",
            "          [ 0.2010,  0.6613,  0.0969],\n",
            "          [ 0.7318,  0.5294, -0.3635],\n",
            "          [ 0.5614,  0.5596,  0.0596]],\n",
            "\n",
            "         [[-0.3714,  0.0257,  1.4531],\n",
            "          [ 0.0286,  0.0683,  0.9976],\n",
            "          [ 0.2573, -0.1248,  0.4739],\n",
            "          ...,\n",
            "          [ 0.2711,  0.6060,  0.2908],\n",
            "          [ 0.7036,  0.4307, -0.2162],\n",
            "          [ 0.5867,  0.5950,  0.1838]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 5.3896e-03,  3.2436e-02,  5.5799e-02, -2.3216e-03, -1.9373e-03,\n",
            "         -5.7543e-02,  4.5449e-02, -4.2739e-02,  9.2263e-02,  2.9889e-03,\n",
            "          8.6883e-02, -8.8601e-02,  7.9087e-02, -3.5679e-02, -5.9219e-02,\n",
            "         -7.8906e-02, -4.4683e-02, -4.3042e-02,  7.2952e-02,  9.5288e-02,\n",
            "         -3.1066e-02, -3.4638e-02,  1.9484e-02,  4.7462e-02,  8.5389e-02,\n",
            "         -1.0958e-02, -1.0325e-02,  7.4535e-02,  6.4371e-02,  3.8623e-02,\n",
            "          3.4850e-02, -6.8319e-02,  1.2297e-01, -4.1130e-02,  6.1265e-02,\n",
            "          1.1048e-01, -1.0720e-01,  8.6604e-02, -4.7392e-02,  1.3633e-02,\n",
            "          5.9806e-02, -6.4955e-02,  4.2179e-02,  5.2369e-02,  1.5595e-02,\n",
            "          5.2368e-02, -1.0530e-01,  8.9694e-02,  5.8113e-02,  7.5941e-03,\n",
            "          3.4709e-02, -8.2028e-02, -4.4934e-02, -6.2846e-02,  7.7762e-02,\n",
            "          3.0503e-02,  4.6050e-02,  8.5447e-02,  2.4512e-02,  8.0031e-02,\n",
            "          3.7864e-02, -4.1697e-02,  4.0739e-02,  3.4142e-02, -8.2380e-02,\n",
            "         -9.6351e-03, -3.7358e-02, -7.7963e-02, -8.3545e-02,  3.5296e-03,\n",
            "         -1.1251e-02,  5.7611e-02,  2.8391e-02,  5.1078e-02,  3.1307e-02,\n",
            "          2.8667e-03,  8.5997e-02, -9.1042e-02, -4.9580e-05, -8.1357e-02,\n",
            "          4.3675e-02, -5.7848e-02,  4.7791e-02,  1.1345e-01, -6.6890e-02,\n",
            "          7.4902e-02, -5.2888e-02, -8.7716e-02,  2.9769e-02,  2.0445e-02,\n",
            "          4.3306e-02,  1.3579e-02,  4.4230e-02,  7.6133e-02, -1.0799e-01,\n",
            "         -8.8611e-02,  2.4332e-02, -1.1789e-02, -7.8525e-02,  1.4856e-02]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.0550,  0.0356,  0.0988,  ...,  0.1166, -0.0694,  0.0339],\n",
            "         [-0.0960,  0.0397,  0.1037,  ...,  0.1327, -0.0695,  0.0353],\n",
            "         [-0.1499,  0.0495,  0.1071,  ...,  0.1371, -0.0696,  0.0334],\n",
            "         ...,\n",
            "         [-0.1726,  0.2424, -0.0117,  ..., -0.1887, -0.0690, -0.0533],\n",
            "         [-0.1942,  0.2382, -0.0004,  ..., -0.1910, -0.0693, -0.0568],\n",
            "         [-0.2063,  0.2282,  0.0116,  ..., -0.1852, -0.0695, -0.0567]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-0.9929,  0.1971,  1.0617],\n",
            "          [-0.4082,  0.3371,  0.5799],\n",
            "          [ 0.1456,  0.0077,  0.1604],\n",
            "          ...,\n",
            "          [-0.3194,  0.6318,  0.2086],\n",
            "          [ 0.5017,  1.0858, -0.2017],\n",
            "          [ 0.7575,  0.3639, -0.8417]],\n",
            "\n",
            "         [[-0.9513,  0.1478,  1.0075],\n",
            "          [-0.3192,  0.3401,  0.4911],\n",
            "          [ 0.2527,  0.0858,  0.0772],\n",
            "          ...,\n",
            "          [-0.3980,  0.7353,  0.2197],\n",
            "          [ 0.4138,  1.0928, -0.2517],\n",
            "          [ 0.6654,  0.3513, -0.8410]],\n",
            "\n",
            "         [[-0.8968,  0.0661,  0.8885],\n",
            "          [-0.2549,  0.2681,  0.4011],\n",
            "          [ 0.3134,  0.0956,  0.0612],\n",
            "          ...,\n",
            "          [-0.4546,  0.8118,  0.2734],\n",
            "          [ 0.3584,  1.0091, -0.2481],\n",
            "          [ 0.5534,  0.3144, -0.7817]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0535, -0.7064, -0.1857],\n",
            "          [-0.5059, -0.4305, -0.3064],\n",
            "          [-0.9532, -0.2714, -0.0308],\n",
            "          ...,\n",
            "          [ 0.1085, -0.1295,  0.9189],\n",
            "          [ 0.1741,  0.0220,  0.6797],\n",
            "          [ 0.0022,  0.1332,  0.6034]],\n",
            "\n",
            "         [[-0.2150, -0.6434, -0.3005],\n",
            "          [-0.6078, -0.5343, -0.3295],\n",
            "          [-0.9504, -0.3698, -0.0246],\n",
            "          ...,\n",
            "          [ 0.1715, -0.0130,  0.8608],\n",
            "          [ 0.3044,  0.0354,  0.6375],\n",
            "          [ 0.1095,  0.2289,  0.5634]],\n",
            "\n",
            "         [[-0.3421, -0.5873, -0.3623],\n",
            "          [-0.6978, -0.5789, -0.3256],\n",
            "          [-0.9238, -0.4550,  0.0100],\n",
            "          ...,\n",
            "          [ 0.2220,  0.0647,  0.8059],\n",
            "          [ 0.4040,  0.0565,  0.5933],\n",
            "          [ 0.2130,  0.2849,  0.5442]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-0.9929, -0.4082,  0.1456,  ..., -0.4420, -0.3420, -0.1872],\n",
            "          [-1.2264, -0.4863,  0.2739,  ..., -0.4911, -0.0251,  0.0399],\n",
            "          [-1.4514, -0.7400,  0.3492,  ..., -0.3467,  0.3014,  0.1218],\n",
            "          ...,\n",
            "          [ 0.0409, -0.1048, -0.2883,  ..., -0.2562, -0.4570, -0.3498],\n",
            "          [-0.2745,  0.0245,  0.1446,  ...,  0.0725,  0.3352,  0.1973],\n",
            "          [-0.1454,  0.4516,  0.4918,  ...,  0.2220,  0.4040,  0.2130]],\n",
            "\n",
            "         [[ 0.1971,  0.3371,  0.0077,  ..., -0.5522, -0.4222, -0.7398],\n",
            "          [ 0.3032,  0.3065, -0.2682,  ..., -0.3373, -0.1848, -0.1194],\n",
            "          [ 0.2917,  0.2874,  0.5121,  ...,  0.2058,  0.3563,  0.4295],\n",
            "          ...,\n",
            "          [ 0.0147,  0.0290, -0.2831,  ..., -0.3169, -0.4264, -0.6851],\n",
            "          [-0.1692, -0.2923, -0.0602,  ..., -0.1133,  0.0564, -0.0582],\n",
            "          [-0.4859, -0.5641, -0.1209,  ...,  0.0647,  0.0565,  0.2849]],\n",
            "\n",
            "         [[ 1.0617,  0.5799,  0.1604,  ..., -0.0326,  0.1200, -0.0530],\n",
            "          [ 0.8689,  0.3153,  0.0712,  ..., -0.3356,  0.2298,  0.0718],\n",
            "          [ 0.8278,  0.3441, -0.1772,  ...,  0.2685,  0.1868,  0.2393],\n",
            "          ...,\n",
            "          [ 0.7092,  0.2296,  0.1546,  ..., -0.2960,  0.4834,  0.6129],\n",
            "          [ 0.6346,  0.3967,  0.5764,  ...,  0.4749,  0.4802,  0.8301],\n",
            "          [ 0.6481,  0.5851,  0.5341,  ...,  0.8059,  0.5933,  0.5442]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-0.9929,  0.1971,  1.0617],\n",
            "          [-0.4082,  0.3371,  0.5799],\n",
            "          [ 0.1456,  0.0077,  0.1604],\n",
            "          ...,\n",
            "          [-0.3194,  0.6318,  0.2086],\n",
            "          [ 0.5017,  1.0858, -0.2017],\n",
            "          [ 0.7575,  0.3639, -0.8417]],\n",
            "\n",
            "         [[-0.9513,  0.1478,  1.0075],\n",
            "          [-0.3192,  0.3401,  0.4911],\n",
            "          [ 0.2527,  0.0858,  0.0772],\n",
            "          ...,\n",
            "          [-0.3980,  0.7353,  0.2197],\n",
            "          [ 0.4138,  1.0928, -0.2517],\n",
            "          [ 0.6654,  0.3513, -0.8410]],\n",
            "\n",
            "         [[-0.8968,  0.0661,  0.8885],\n",
            "          [-0.2549,  0.2681,  0.4011],\n",
            "          [ 0.3134,  0.0956,  0.0612],\n",
            "          ...,\n",
            "          [-0.4546,  0.8118,  0.2734],\n",
            "          [ 0.3584,  1.0091, -0.2481],\n",
            "          [ 0.5534,  0.3144, -0.7817]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0535, -0.7064, -0.1857],\n",
            "          [-0.5059, -0.4305, -0.3064],\n",
            "          [-0.9532, -0.2714, -0.0308],\n",
            "          ...,\n",
            "          [ 0.1085, -0.1295,  0.9189],\n",
            "          [ 0.1741,  0.0220,  0.6797],\n",
            "          [ 0.0022,  0.1332,  0.6034]],\n",
            "\n",
            "         [[-0.2150, -0.6434, -0.3005],\n",
            "          [-0.6078, -0.5343, -0.3295],\n",
            "          [-0.9504, -0.3698, -0.0246],\n",
            "          ...,\n",
            "          [ 0.1715, -0.0130,  0.8608],\n",
            "          [ 0.3044,  0.0354,  0.6375],\n",
            "          [ 0.1095,  0.2289,  0.5634]],\n",
            "\n",
            "         [[-0.3421, -0.5873, -0.3623],\n",
            "          [-0.6978, -0.5789, -0.3256],\n",
            "          [-0.9238, -0.4550,  0.0100],\n",
            "          ...,\n",
            "          [ 0.2220,  0.0647,  0.8059],\n",
            "          [ 0.4040,  0.0565,  0.5933],\n",
            "          [ 0.2130,  0.2849,  0.5442]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0057,  0.0321,  0.0568, -0.0025, -0.0035, -0.0572,  0.0467, -0.0427,\n",
            "          0.0930,  0.0028,  0.0869, -0.0877,  0.0801, -0.0355, -0.0577, -0.0787,\n",
            "         -0.0442, -0.0429,  0.0720,  0.0950, -0.0314, -0.0362,  0.0196,  0.0479,\n",
            "          0.0853, -0.0119, -0.0095,  0.0762,  0.0644,  0.0376,  0.0347, -0.0684,\n",
            "          0.1236, -0.0417,  0.0622,  0.1090, -0.1066,  0.0851, -0.0465,  0.0124,\n",
            "          0.0590, -0.0641,  0.0419,  0.0525,  0.0161,  0.0526, -0.1040,  0.0893,\n",
            "          0.0570,  0.0074,  0.0340, -0.0817, -0.0450, -0.0636,  0.0776,  0.0294,\n",
            "          0.0458,  0.0857,  0.0241,  0.0800,  0.0374, -0.0419,  0.0404,  0.0330,\n",
            "         -0.0819, -0.0093, -0.0378, -0.0782, -0.0833,  0.0041, -0.0107,  0.0578,\n",
            "          0.0287,  0.0525,  0.0316,  0.0029,  0.0860, -0.0900, -0.0008, -0.0806,\n",
            "          0.0425, -0.0582,  0.0478,  0.1133, -0.0675,  0.0749, -0.0537, -0.0881,\n",
            "          0.0292,  0.0214,  0.0443,  0.0138,  0.0448,  0.0763, -0.1081, -0.0884,\n",
            "          0.0237, -0.0121, -0.0785,  0.0152]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.0285,  0.1930, -0.0137,  ...,  0.1984,  0.1049,  0.2639],\n",
            "         [ 0.0118,  0.2003, -0.0200,  ...,  0.2089,  0.0912,  0.2714],\n",
            "         [-0.0134,  0.2046, -0.0232,  ...,  0.2120,  0.0718,  0.2659],\n",
            "         ...,\n",
            "         [ 0.0074,  0.1024,  0.1764,  ..., -0.0122,  0.0855, -0.2512],\n",
            "         [-0.0268,  0.0884,  0.1647,  ..., -0.0188,  0.0688, -0.2424],\n",
            "         [-0.0506,  0.0797,  0.1486,  ..., -0.0209,  0.0595, -0.2233]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-1.0394, -0.8348, -0.3156],\n",
            "          [-1.0446, -0.5588, -0.2073],\n",
            "          [-0.5896, -0.3267, -0.0255],\n",
            "          ...,\n",
            "          [-0.1554, -0.7040,  0.6772],\n",
            "          [-0.0230, -0.7068,  0.7534],\n",
            "          [ 0.2067, -0.2180,  0.4241]],\n",
            "\n",
            "         [[-1.0617, -0.8240, -0.3430],\n",
            "          [-1.0932, -0.4766, -0.1649],\n",
            "          [-0.5853, -0.3525,  0.0478],\n",
            "          ...,\n",
            "          [-0.1570, -0.7635,  0.7537],\n",
            "          [-0.0841, -0.6709,  0.7579],\n",
            "          [ 0.1218, -0.1797,  0.3554]],\n",
            "\n",
            "         [[-1.0638, -0.7531, -0.3567],\n",
            "          [-1.1224, -0.3362, -0.1365],\n",
            "          [-0.5879, -0.2788,  0.1061],\n",
            "          ...,\n",
            "          [-0.1602, -0.7879,  0.7945],\n",
            "          [-0.1498, -0.5907,  0.7174],\n",
            "          [ 0.0181, -0.1304,  0.2479]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3198,  0.0909, -0.2323],\n",
            "          [-0.5030,  0.4093,  0.0579],\n",
            "          [-0.5803,  0.5210,  0.3042],\n",
            "          ...,\n",
            "          [-0.2887, -0.2956, -0.1031],\n",
            "          [-0.2352, -0.3114,  0.0202],\n",
            "          [ 0.1607, -0.0402,  0.1471]],\n",
            "\n",
            "         [[-0.3187,  0.1030, -0.3410],\n",
            "          [-0.4609,  0.4398, -0.0071],\n",
            "          [-0.5880,  0.5195,  0.2246],\n",
            "          ...,\n",
            "          [-0.3160, -0.2872, -0.0867],\n",
            "          [-0.3517, -0.3632, -0.0188],\n",
            "          [-0.0441, -0.1321,  0.0918]],\n",
            "\n",
            "         [[-0.3582,  0.1314, -0.4798],\n",
            "          [-0.4815,  0.4675, -0.0541],\n",
            "          [-0.5978,  0.5268,  0.1896],\n",
            "          ...,\n",
            "          [-0.3230, -0.2568, -0.0812],\n",
            "          [-0.4459, -0.3600, -0.0272],\n",
            "          [-0.2003, -0.1539,  0.0344]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-1.0394, -1.0446, -0.5896,  ..., -1.0158, -0.8915, -0.4787],\n",
            "          [-1.1069, -0.8432,  0.0641,  ..., -0.5526, -0.0953, -0.0132],\n",
            "          [-0.6628, -0.3628,  0.1436,  ..., -0.2545,  0.2051,  0.1658],\n",
            "          ...,\n",
            "          [-0.8106, -0.5684, -0.1872,  ..., -0.1131, -0.2604, -0.1361],\n",
            "          [-0.3903, -0.2679,  0.1485,  ..., -0.0270, -0.3779, -0.3236],\n",
            "          [-0.3773, -0.2134,  0.3609,  ..., -0.3230, -0.4459, -0.2003]],\n",
            "\n",
            "         [[-0.8348, -0.5588, -0.3267,  ...,  0.4425,  0.5093,  0.0712],\n",
            "          [-1.1186, -0.5278,  0.0330,  ...,  0.1088,  0.0558, -0.2316],\n",
            "          [-0.6332, -0.6985, -0.1087,  ..., -0.6402, -0.0541, -0.0746],\n",
            "          ...,\n",
            "          [-0.7162, -0.4299,  0.4869,  ...,  0.1966,  0.3292,  0.2895],\n",
            "          [-0.2953, -0.0764, -0.0205,  ..., -0.0492,  0.0262,  0.2648],\n",
            "          [-0.0412,  0.1398,  0.0612,  ..., -0.2568, -0.3600, -0.1539]],\n",
            "\n",
            "         [[-0.3156, -0.2073, -0.0255,  ...,  0.0692,  0.2849, -0.2424],\n",
            "          [-0.4358, -0.1807, -0.0340,  ..., -0.1195, -0.1186, -0.5628],\n",
            "          [-0.5187,  0.2292,  0.2465,  ..., -0.0387, -0.2166, -0.5559],\n",
            "          ...,\n",
            "          [-0.4731, -0.2528,  0.0646,  ..., -0.0967, -0.0353, -0.2793],\n",
            "          [-0.6086, -0.3910,  0.2549,  ..., -0.2312, -0.1195, -0.1781],\n",
            "          [-0.3252, -0.2172,  0.2587,  ..., -0.0812, -0.0272,  0.0344]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-1.0394, -0.8348, -0.3156],\n",
            "          [-1.0446, -0.5588, -0.2073],\n",
            "          [-0.5896, -0.3267, -0.0255],\n",
            "          ...,\n",
            "          [-0.1554, -0.7040,  0.6772],\n",
            "          [-0.0230, -0.7068,  0.7534],\n",
            "          [ 0.2067, -0.2180,  0.4241]],\n",
            "\n",
            "         [[-1.0617, -0.8240, -0.3430],\n",
            "          [-1.0932, -0.4766, -0.1649],\n",
            "          [-0.5853, -0.3525,  0.0478],\n",
            "          ...,\n",
            "          [-0.1570, -0.7635,  0.7537],\n",
            "          [-0.0841, -0.6709,  0.7579],\n",
            "          [ 0.1218, -0.1797,  0.3554]],\n",
            "\n",
            "         [[-1.0638, -0.7531, -0.3567],\n",
            "          [-1.1224, -0.3362, -0.1365],\n",
            "          [-0.5879, -0.2788,  0.1061],\n",
            "          ...,\n",
            "          [-0.1602, -0.7879,  0.7945],\n",
            "          [-0.1498, -0.5907,  0.7174],\n",
            "          [ 0.0181, -0.1304,  0.2479]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3198,  0.0909, -0.2323],\n",
            "          [-0.5030,  0.4093,  0.0579],\n",
            "          [-0.5803,  0.5210,  0.3042],\n",
            "          ...,\n",
            "          [-0.2887, -0.2956, -0.1031],\n",
            "          [-0.2352, -0.3114,  0.0202],\n",
            "          [ 0.1607, -0.0402,  0.1471]],\n",
            "\n",
            "         [[-0.3187,  0.1030, -0.3410],\n",
            "          [-0.4609,  0.4398, -0.0071],\n",
            "          [-0.5880,  0.5195,  0.2246],\n",
            "          ...,\n",
            "          [-0.3160, -0.2872, -0.0867],\n",
            "          [-0.3517, -0.3632, -0.0188],\n",
            "          [-0.0441, -0.1321,  0.0918]],\n",
            "\n",
            "         [[-0.3582,  0.1314, -0.4798],\n",
            "          [-0.4815,  0.4675, -0.0541],\n",
            "          [-0.5978,  0.5268,  0.1896],\n",
            "          ...,\n",
            "          [-0.3230, -0.2568, -0.0812],\n",
            "          [-0.4459, -0.3600, -0.0272],\n",
            "          [-0.2003, -0.1539,  0.0344]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0055,  0.0308,  0.0560, -0.0016, -0.0009, -0.0565,  0.0462, -0.0430,\n",
            "          0.0934,  0.0024,  0.0878, -0.0884,  0.0801, -0.0359, -0.0581, -0.0790,\n",
            "         -0.0450, -0.0425,  0.0727,  0.0940, -0.0320, -0.0342,  0.0201,  0.0469,\n",
            "          0.0854, -0.0095, -0.0108,  0.0739,  0.0650,  0.0384,  0.0340, -0.0679,\n",
            "          0.1221, -0.0405,  0.0595,  0.1098, -0.1072,  0.0858, -0.0474,  0.0138,\n",
            "          0.0603, -0.0635,  0.0418,  0.0522,  0.0151,  0.0528, -0.1058,  0.0892,\n",
            "          0.0579,  0.0069,  0.0353, -0.0825, -0.0456, -0.0630,  0.0786,  0.0312,\n",
            "          0.0460,  0.0849,  0.0234,  0.0800,  0.0368, -0.0416,  0.0415,  0.0343,\n",
            "         -0.0821, -0.0096, -0.0383, -0.0775, -0.0824,  0.0040, -0.0107,  0.0582,\n",
            "          0.0278,  0.0524,  0.0325,  0.0034,  0.0854, -0.0908, -0.0008, -0.0810,\n",
            "          0.0430, -0.0585,  0.0484,  0.1133, -0.0682,  0.0740, -0.0548, -0.0888,\n",
            "          0.0311,  0.0204,  0.0445,  0.0135,  0.0437,  0.0756, -0.1076, -0.0890,\n",
            "          0.0247, -0.0118, -0.0781,  0.0164]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.1348,  0.2393, -0.0193,  ...,  0.0497,  0.0138,  0.1724],\n",
            "         [-0.1221,  0.2551, -0.0217,  ...,  0.0527,  0.0140,  0.1774],\n",
            "         [-0.0992,  0.2661, -0.0246,  ...,  0.0532,  0.0142,  0.1724],\n",
            "         ...,\n",
            "         [-0.0816,  0.0485, -0.1014,  ..., -0.0179,  0.0139, -0.2484],\n",
            "         [-0.0187,  0.0375, -0.0902,  ..., -0.0184,  0.0141, -0.2374],\n",
            "         [ 0.0300,  0.0310, -0.0794,  ..., -0.0172,  0.0143, -0.2184]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 0.2541, -0.1683, -1.0158],\n",
            "          [ 0.5047, -0.3492, -0.9787],\n",
            "          [ 0.5859, -0.4202, -0.8205],\n",
            "          ...,\n",
            "          [ 0.5370,  0.2466,  0.1956],\n",
            "          [ 0.3885,  0.2297,  0.3381],\n",
            "          [ 0.1632,  0.3243, -0.0680]],\n",
            "\n",
            "         [[ 0.2156,  0.0214, -1.0220],\n",
            "          [ 0.3098, -0.2085, -0.9177],\n",
            "          [ 0.3542, -0.3673, -0.7450],\n",
            "          ...,\n",
            "          [ 0.5181,  0.2935,  0.2827],\n",
            "          [ 0.3216,  0.2775,  0.4323],\n",
            "          [ 0.1420,  0.2742,  0.0412]],\n",
            "\n",
            "         [[ 0.1411,  0.1899, -0.9469],\n",
            "          [ 0.0774, -0.0516, -0.7962],\n",
            "          [ 0.0484, -0.3172, -0.6640],\n",
            "          ...,\n",
            "          [ 0.3723,  0.3446,  0.3402],\n",
            "          [ 0.1758,  0.3151,  0.4741],\n",
            "          [ 0.0950,  0.1901,  0.1010]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.4714,  0.0172, -0.1182],\n",
            "          [ 0.3222, -0.4646, -0.0076],\n",
            "          [ 0.6914, -0.4797,  0.2743],\n",
            "          ...,\n",
            "          [-0.1078,  0.2251,  0.0786],\n",
            "          [-0.4850,  0.0438,  0.0548],\n",
            "          [-0.6097,  0.1100,  0.1428]],\n",
            "\n",
            "         [[-0.4802,  0.0141, -0.2145],\n",
            "          [ 0.2953, -0.5120, -0.1189],\n",
            "          [ 0.6157, -0.4993,  0.2194],\n",
            "          ...,\n",
            "          [-0.1154,  0.2552,  0.0792],\n",
            "          [-0.4686,  0.0963,  0.1461],\n",
            "          [-0.5508,  0.1633,  0.2802]],\n",
            "\n",
            "         [[-0.4437,  0.0569, -0.2671],\n",
            "          [ 0.2945, -0.5192, -0.1958],\n",
            "          [ 0.5004, -0.5243,  0.1369],\n",
            "          ...,\n",
            "          [-0.1416,  0.3102,  0.0477],\n",
            "          [-0.4351,  0.1539,  0.2345],\n",
            "          [-0.5079,  0.2000,  0.3634]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 2.5413e-01,  5.0474e-01,  5.8594e-01,  ..., -2.4707e-01,\n",
            "           -5.0587e-01, -6.2942e-01],\n",
            "          [-1.0386e-01,  5.8244e-02,  4.0766e-01,  ...,  4.6674e-02,\n",
            "           -5.0823e-01, -6.1343e-01],\n",
            "          [ 3.5874e-02,  9.8008e-02,  1.6488e-01,  ..., -4.3896e-01,\n",
            "           -4.7362e-01, -4.5898e-01],\n",
            "          ...,\n",
            "          [-3.8379e-02,  2.2490e-01,  7.2055e-01,  ...,  2.4271e-02,\n",
            "            5.0232e-02, -1.2530e-01],\n",
            "          [-1.1278e-01, -1.8178e-01, -5.4666e-02,  ..., -9.5012e-02,\n",
            "           -6.3743e-01, -7.0841e-01],\n",
            "          [ 3.2894e-02, -6.6910e-02,  1.4523e-01,  ..., -1.4162e-01,\n",
            "           -4.3512e-01, -5.0785e-01]],\n",
            "\n",
            "         [[-1.6834e-01, -3.4922e-01, -4.2021e-01,  ..., -1.0400e-01,\n",
            "           -3.0396e-01,  1.2281e-01],\n",
            "          [ 1.6637e-01,  5.4245e-04, -2.9638e-01,  ...,  6.2734e-03,\n",
            "           -4.5317e-01, -3.6183e-01],\n",
            "          [ 1.7255e-01,  8.9845e-03, -1.0286e-01,  ...,  1.6844e-01,\n",
            "           -1.1877e-02, -3.5297e-01],\n",
            "          ...,\n",
            "          [-1.7308e-03, -2.0940e-01, -2.1454e-01,  ..., -2.5854e-01,\n",
            "           -1.1252e-01,  1.2471e-01],\n",
            "          [ 1.2405e-01, -6.0063e-02,  4.1556e-02,  ...,  1.4766e-01,\n",
            "            3.2014e-02,  1.7843e-01],\n",
            "          [ 4.2486e-01,  2.1569e-01,  1.6398e-01,  ...,  3.1024e-01,\n",
            "            1.5391e-01,  2.0000e-01]],\n",
            "\n",
            "         [[-1.0158e+00, -9.7867e-01, -8.2049e-01,  ..., -2.6382e-01,\n",
            "           -2.8922e-01, -3.5494e-01],\n",
            "          [-7.4696e-01, -5.0051e-01, -1.3507e-01,  ..., -2.1909e-01,\n",
            "            3.3880e-02, -1.9966e-02],\n",
            "          [ 9.8595e-02,  4.8783e-02,  2.0180e-01,  ...,  8.9550e-02,\n",
            "            2.0189e-01,  5.2095e-02],\n",
            "          ...,\n",
            "          [ 1.2455e-01, -6.2602e-03, -4.2770e-01,  ..., -1.9275e-01,\n",
            "            9.8013e-02,  7.6095e-02],\n",
            "          [ 3.2763e-02, -2.1509e-01, -2.9953e-01,  ..., -8.3168e-02,\n",
            "            2.8870e-01,  3.9632e-01],\n",
            "          [-2.3929e-01, -5.1854e-01, -4.8588e-01,  ...,  4.7677e-02,\n",
            "            2.3454e-01,  3.6340e-01]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 0.2541, -0.1683, -1.0158],\n",
            "          [ 0.5047, -0.3492, -0.9787],\n",
            "          [ 0.5859, -0.4202, -0.8205],\n",
            "          ...,\n",
            "          [ 0.5370,  0.2466,  0.1956],\n",
            "          [ 0.3885,  0.2297,  0.3381],\n",
            "          [ 0.1632,  0.3243, -0.0680]],\n",
            "\n",
            "         [[ 0.2156,  0.0214, -1.0220],\n",
            "          [ 0.3098, -0.2085, -0.9177],\n",
            "          [ 0.3542, -0.3673, -0.7450],\n",
            "          ...,\n",
            "          [ 0.5181,  0.2935,  0.2827],\n",
            "          [ 0.3216,  0.2775,  0.4323],\n",
            "          [ 0.1420,  0.2742,  0.0412]],\n",
            "\n",
            "         [[ 0.1411,  0.1899, -0.9469],\n",
            "          [ 0.0774, -0.0516, -0.7962],\n",
            "          [ 0.0484, -0.3172, -0.6640],\n",
            "          ...,\n",
            "          [ 0.3723,  0.3446,  0.3402],\n",
            "          [ 0.1758,  0.3151,  0.4741],\n",
            "          [ 0.0950,  0.1901,  0.1010]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.4714,  0.0172, -0.1182],\n",
            "          [ 0.3222, -0.4646, -0.0076],\n",
            "          [ 0.6914, -0.4797,  0.2743],\n",
            "          ...,\n",
            "          [-0.1078,  0.2251,  0.0786],\n",
            "          [-0.4850,  0.0438,  0.0548],\n",
            "          [-0.6097,  0.1100,  0.1428]],\n",
            "\n",
            "         [[-0.4802,  0.0141, -0.2145],\n",
            "          [ 0.2953, -0.5120, -0.1189],\n",
            "          [ 0.6157, -0.4993,  0.2194],\n",
            "          ...,\n",
            "          [-0.1154,  0.2552,  0.0792],\n",
            "          [-0.4686,  0.0963,  0.1461],\n",
            "          [-0.5508,  0.1633,  0.2802]],\n",
            "\n",
            "         [[-0.4437,  0.0569, -0.2671],\n",
            "          [ 0.2945, -0.5192, -0.1958],\n",
            "          [ 0.5004, -0.5243,  0.1369],\n",
            "          ...,\n",
            "          [-0.1416,  0.3102,  0.0477],\n",
            "          [-0.4351,  0.1539,  0.2345],\n",
            "          [-0.5079,  0.2000,  0.3634]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0050,  0.0312,  0.0559, -0.0027, -0.0019, -0.0568,  0.0462, -0.0434,\n",
            "          0.0932,  0.0035,  0.0864, -0.0880,  0.0803, -0.0363, -0.0576, -0.0788,\n",
            "         -0.0440, -0.0427,  0.0720,  0.0948, -0.0314, -0.0352,  0.0203,  0.0478,\n",
            "          0.0850, -0.0119, -0.0096,  0.0763,  0.0647,  0.0381,  0.0341, -0.0673,\n",
            "          0.1231, -0.0412,  0.0617,  0.1092, -0.1072,  0.0861, -0.0473,  0.0133,\n",
            "          0.0596, -0.0645,  0.0419,  0.0518,  0.0153,  0.0517, -0.1043,  0.0894,\n",
            "          0.0567,  0.0077,  0.0348, -0.0822, -0.0452, -0.0640,  0.0781,  0.0296,\n",
            "          0.0461,  0.0851,  0.0241,  0.0804,  0.0365, -0.0425,  0.0405,  0.0330,\n",
            "         -0.0824, -0.0094, -0.0373, -0.0783, -0.0828,  0.0043, -0.0105,  0.0578,\n",
            "          0.0284,  0.0525,  0.0312,  0.0030,  0.0864, -0.0903, -0.0018, -0.0803,\n",
            "          0.0435, -0.0575,  0.0469,  0.1133, -0.0674,  0.0752, -0.0533, -0.0884,\n",
            "          0.0297,  0.0211,  0.0440,  0.0132,  0.0445,  0.0758, -0.1075, -0.0878,\n",
            "          0.0240, -0.0107, -0.0782,  0.0152]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.1085, -0.0181,  0.0696,  ...,  0.1667, -0.1033, -0.2033],\n",
            "         [-0.1061, -0.0238,  0.0791,  ...,  0.1740, -0.0957, -0.1973],\n",
            "         [-0.1012, -0.0264,  0.0868,  ...,  0.1745, -0.0865, -0.1843],\n",
            "         ...,\n",
            "         [-0.0846,  0.0839, -0.0698,  ..., -0.1083, -0.1116,  0.0680],\n",
            "         [-0.0766,  0.0913, -0.0678,  ..., -0.1238, -0.1056,  0.0567],\n",
            "         [-0.0719,  0.0954, -0.0613,  ..., -0.1291, -0.1012,  0.0391]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 0.0581, -0.0070, -0.6397],\n",
            "          [-0.3207, -0.5538, -0.3838],\n",
            "          [ 0.2514, -0.6205, -0.3186],\n",
            "          ...,\n",
            "          [ 0.0453,  0.2879,  0.2266],\n",
            "          [-0.2147,  0.3730,  0.1103],\n",
            "          [ 0.0222, -0.0884, -0.3824]],\n",
            "\n",
            "         [[-0.0439,  0.0252, -0.4499],\n",
            "          [-0.3509, -0.5636, -0.2863],\n",
            "          [ 0.1816, -0.5385, -0.3774],\n",
            "          ...,\n",
            "          [-0.0686,  0.3599,  0.2083],\n",
            "          [-0.3094,  0.4515,  0.1157],\n",
            "          [-0.1301, -0.0931, -0.4124]],\n",
            "\n",
            "         [[-0.1822,  0.1362, -0.2273],\n",
            "          [-0.3585, -0.5351, -0.2129],\n",
            "          [ 0.0918, -0.4254, -0.4604],\n",
            "          ...,\n",
            "          [-0.1762,  0.4334,  0.1710],\n",
            "          [-0.3807,  0.5139,  0.0683],\n",
            "          [-0.2885, -0.0905, -0.4753]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2443,  0.0525,  0.7517],\n",
            "          [-0.5028,  0.1268,  0.3311],\n",
            "          [-0.5529, -0.2438,  0.0799],\n",
            "          ...,\n",
            "          [-0.4571,  0.0494,  0.3834],\n",
            "          [-0.3645,  0.2672,  0.4049],\n",
            "          [ 0.0419,  0.4883,  0.3185]],\n",
            "\n",
            "         [[-0.2029,  0.0897,  0.7884],\n",
            "          [-0.4451,  0.1236,  0.2851],\n",
            "          [-0.5497, -0.3410, -0.0153],\n",
            "          ...,\n",
            "          [-0.3666,  0.0873,  0.3068],\n",
            "          [-0.3133,  0.2742,  0.3048],\n",
            "          [-0.0395,  0.4532,  0.2758]],\n",
            "\n",
            "         [[-0.2014,  0.1557,  0.7627],\n",
            "          [-0.4205,  0.1248,  0.2317],\n",
            "          [-0.5283, -0.4166, -0.0717],\n",
            "          ...,\n",
            "          [-0.2673,  0.1187,  0.2670],\n",
            "          [-0.2538,  0.2938,  0.2935],\n",
            "          [-0.1195,  0.4133,  0.2656]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 0.0581, -0.3207,  0.2514,  ..., -0.3852, -0.1476,  0.1675],\n",
            "          [-0.1050, -0.3037,  0.3711,  ..., -0.4672, -0.0954, -0.0110],\n",
            "          [-0.2451, -0.1110, -0.0341,  ..., -0.2884, -0.2271, -0.3480],\n",
            "          ...,\n",
            "          [-0.6406, -0.4035,  0.0268,  ..., -0.2194, -0.4621, -0.6391],\n",
            "          [-0.6050, -0.3287, -0.0852,  ..., -0.1034, -0.2061, -0.4472],\n",
            "          [-0.2525, -0.2119, -0.0625,  ..., -0.2673, -0.2538, -0.1195]],\n",
            "\n",
            "         [[-0.0070, -0.5538, -0.6205,  ...,  0.4234, -0.1476,  0.0620],\n",
            "          [ 0.1656, -0.0857, -0.5197,  ...,  0.4414, -0.1465,  0.1230],\n",
            "          [ 0.1384,  0.1926,  0.2752,  ...,  0.4713,  0.1637, -0.0378],\n",
            "          ...,\n",
            "          [ 0.0432,  0.0494, -0.3779,  ...,  0.2800, -0.3953, -0.4591],\n",
            "          [-0.0464, -0.0775,  0.3746,  ...,  0.0759,  0.2191,  0.1933],\n",
            "          [-0.1142, -0.1412,  0.2167,  ...,  0.1187,  0.2938,  0.4133]],\n",
            "\n",
            "         [[-0.6397, -0.3838, -0.3186,  ..., -0.0389, -0.4622, -0.5458],\n",
            "          [-0.2523, -0.0771, -0.2341,  ...,  0.1376, -0.2013, -0.4948],\n",
            "          [ 0.0398,  0.1127,  0.0392,  ...,  0.1487, -0.1115, -0.2952],\n",
            "          ...,\n",
            "          [-0.2680, -0.2207,  0.3635,  ...,  0.1304,  0.1272,  0.1653],\n",
            "          [ 0.4339,  0.4665,  0.4877,  ...,  0.1181,  0.0892, -0.0682],\n",
            "          [ 0.5436,  0.7286,  0.8118,  ...,  0.2670,  0.2935,  0.2656]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 0.0581, -0.0070, -0.6397],\n",
            "          [-0.3207, -0.5538, -0.3838],\n",
            "          [ 0.2514, -0.6205, -0.3186],\n",
            "          ...,\n",
            "          [ 0.0453,  0.2879,  0.2266],\n",
            "          [-0.2147,  0.3730,  0.1103],\n",
            "          [ 0.0222, -0.0884, -0.3824]],\n",
            "\n",
            "         [[-0.0439,  0.0252, -0.4499],\n",
            "          [-0.3509, -0.5636, -0.2863],\n",
            "          [ 0.1816, -0.5385, -0.3774],\n",
            "          ...,\n",
            "          [-0.0686,  0.3599,  0.2083],\n",
            "          [-0.3094,  0.4515,  0.1157],\n",
            "          [-0.1301, -0.0931, -0.4124]],\n",
            "\n",
            "         [[-0.1822,  0.1362, -0.2273],\n",
            "          [-0.3585, -0.5351, -0.2129],\n",
            "          [ 0.0918, -0.4254, -0.4604],\n",
            "          ...,\n",
            "          [-0.1762,  0.4334,  0.1710],\n",
            "          [-0.3807,  0.5139,  0.0683],\n",
            "          [-0.2885, -0.0905, -0.4753]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2443,  0.0525,  0.7517],\n",
            "          [-0.5028,  0.1268,  0.3311],\n",
            "          [-0.5529, -0.2438,  0.0799],\n",
            "          ...,\n",
            "          [-0.4571,  0.0494,  0.3834],\n",
            "          [-0.3645,  0.2672,  0.4049],\n",
            "          [ 0.0419,  0.4883,  0.3185]],\n",
            "\n",
            "         [[-0.2029,  0.0897,  0.7884],\n",
            "          [-0.4451,  0.1236,  0.2851],\n",
            "          [-0.5497, -0.3410, -0.0153],\n",
            "          ...,\n",
            "          [-0.3666,  0.0873,  0.3068],\n",
            "          [-0.3133,  0.2742,  0.3048],\n",
            "          [-0.0395,  0.4532,  0.2758]],\n",
            "\n",
            "         [[-0.2014,  0.1557,  0.7627],\n",
            "          [-0.4205,  0.1248,  0.2317],\n",
            "          [-0.5283, -0.4166, -0.0717],\n",
            "          ...,\n",
            "          [-0.2673,  0.1187,  0.2670],\n",
            "          [-0.2538,  0.2938,  0.2935],\n",
            "          [-0.1195,  0.4133,  0.2656]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0048,  0.0317,  0.0568, -0.0025, -0.0021, -0.0567,  0.0464, -0.0438,\n",
            "          0.0924,  0.0031,  0.0866, -0.0880,  0.0795, -0.0360, -0.0587, -0.0789,\n",
            "         -0.0447, -0.0429,  0.0734,  0.0948, -0.0322, -0.0340,  0.0209,  0.0473,\n",
            "          0.0855, -0.0103, -0.0108,  0.0749,  0.0641,  0.0385,  0.0348, -0.0687,\n",
            "          0.1240, -0.0411,  0.0612,  0.1101, -0.1069,  0.0862, -0.0485,  0.0148,\n",
            "          0.0591, -0.0652,  0.0414,  0.0521,  0.0157,  0.0531, -0.1053,  0.0907,\n",
            "          0.0573,  0.0076,  0.0358, -0.0830, -0.0450, -0.0645,  0.0779,  0.0305,\n",
            "          0.0464,  0.0847,  0.0241,  0.0803,  0.0376, -0.0418,  0.0415,  0.0327,\n",
            "         -0.0814, -0.0093, -0.0373, -0.0789, -0.0831,  0.0036, -0.0106,  0.0585,\n",
            "          0.0280,  0.0528,  0.0318,  0.0030,  0.0867, -0.0904, -0.0003, -0.0807,\n",
            "          0.0438, -0.0584,  0.0477,  0.1124, -0.0676,  0.0742, -0.0532, -0.0880,\n",
            "          0.0305,  0.0212,  0.0442,  0.0134,  0.0440,  0.0755, -0.1075, -0.0879,\n",
            "          0.0243, -0.0109, -0.0792,  0.0160]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 0.2188,  0.0311,  0.0531,  ..., -0.1379,  0.0687, -0.0778],\n",
            "         [ 0.2081,  0.0343,  0.0470,  ..., -0.1366,  0.0615, -0.0771],\n",
            "         [ 0.1854,  0.0346,  0.0375,  ..., -0.1250,  0.0519, -0.0663],\n",
            "         ...,\n",
            "         [ 0.0150, -0.1063, -0.2265,  ...,  0.0985,  0.0799,  0.1800],\n",
            "         [-0.0149, -0.1074, -0.1848,  ...,  0.0884,  0.0704,  0.1845],\n",
            "         [-0.0318, -0.1054, -0.1440,  ...,  0.0740,  0.0641,  0.1786]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[-0.0160, -0.0924,  0.4772],\n",
            "          [-0.2090, -0.5244,  0.3737],\n",
            "          [-0.3542, -0.3345, -0.1437],\n",
            "          ...,\n",
            "          [ 0.7404, -0.0925, -1.0260],\n",
            "          [ 0.5663, -0.0750, -1.0616],\n",
            "          [-0.1811, -0.4308, -0.9988]],\n",
            "\n",
            "         [[ 0.1578, -0.1709,  0.4610],\n",
            "          [-0.1531, -0.6482,  0.2887],\n",
            "          [-0.3144, -0.4880, -0.0565],\n",
            "          ...,\n",
            "          [ 0.5992, -0.1336, -1.0006],\n",
            "          [ 0.4985, -0.0940, -1.0548],\n",
            "          [-0.0796, -0.4069, -0.9933]],\n",
            "\n",
            "         [[ 0.2676, -0.2583,  0.4476],\n",
            "          [-0.1726, -0.7935,  0.2227],\n",
            "          [-0.3430, -0.6347,  0.0665],\n",
            "          ...,\n",
            "          [ 0.4239, -0.2342, -0.8766],\n",
            "          [ 0.3751, -0.1683, -0.9902],\n",
            "          [-0.0189, -0.3847, -0.9688]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1739,  0.3261,  0.7411],\n",
            "          [-0.4342,  0.1410,  0.0020],\n",
            "          [-0.3361, -0.0219, -0.6304],\n",
            "          ...,\n",
            "          [-0.2554,  0.2818, -0.1654],\n",
            "          [-0.1782,  0.0590,  0.0830],\n",
            "          [-0.2290, -0.3375,  0.0261]],\n",
            "\n",
            "         [[ 0.0366,  0.4145,  0.7315],\n",
            "          [-0.2155,  0.2318,  0.1387],\n",
            "          [-0.1377,  0.0489, -0.4159],\n",
            "          ...,\n",
            "          [-0.4150,  0.2204, -0.2269],\n",
            "          [-0.3044, -0.0537,  0.0882],\n",
            "          [-0.2342, -0.3759,  0.0847]],\n",
            "\n",
            "         [[ 0.1803,  0.5325,  0.7084],\n",
            "          [-0.0441,  0.3346,  0.2597],\n",
            "          [ 0.0160,  0.0990, -0.1758],\n",
            "          ...,\n",
            "          [-0.5078,  0.1504, -0.2502],\n",
            "          [-0.4021, -0.1506,  0.0524],\n",
            "          [-0.2165, -0.4266,  0.0795]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[-0.0160, -0.2090, -0.3542,  ...,  0.0769, -0.1046, -0.1324],\n",
            "          [ 0.6443,  0.2456, -0.2460,  ...,  0.1377,  0.1049, -0.0975],\n",
            "          [ 0.9620,  0.5009,  0.2609,  ...,  0.0469, -0.1596, -0.4803],\n",
            "          ...,\n",
            "          [-0.8241, -0.6040, -0.3215,  ...,  0.1408,  0.0309,  0.0931],\n",
            "          [-0.0456,  0.2029, -0.0489,  ..., -0.1556, -0.0918, -0.1264],\n",
            "          [ 0.1520,  0.4090,  0.4069,  ..., -0.5078, -0.4021, -0.2165]],\n",
            "\n",
            "         [[-0.0924, -0.5244, -0.3345,  ..., -0.4526, -0.3080,  0.1651],\n",
            "          [ 0.2573, -0.1479, -0.2049,  ..., -0.1072, -0.1738, -0.3084],\n",
            "          [ 0.0540, -0.1252, -0.1698,  ..., -0.3395, -0.5107, -0.7458],\n",
            "          ...,\n",
            "          [ 1.1033,  0.4989, -0.1954,  ...,  0.2431,  0.0551,  0.2881],\n",
            "          [ 0.8519,  0.4544, -0.1379,  ...,  0.2362, -0.0226,  0.0731],\n",
            "          [ 0.4523,  0.6150,  0.3975,  ...,  0.1504, -0.1506, -0.4266]],\n",
            "\n",
            "         [[ 0.4772,  0.3737, -0.1437,  ...,  0.5420,  0.5736,  0.4980],\n",
            "          [-0.0306,  0.1498, -0.3860,  ...,  0.1080,  0.2223,  0.1076],\n",
            "          [-0.7078, -0.4234, -0.6738,  ..., -0.0649, -0.1754, -0.2287],\n",
            "          ...,\n",
            "          [ 0.3695,  0.2968, -0.7594,  ...,  0.2576, -0.2042,  0.0962],\n",
            "          [-0.0175,  0.0654, -0.6790,  ..., -0.1614,  0.2395,  0.4743],\n",
            "          [-0.3494, -0.1522, -0.6746,  ..., -0.2502,  0.0524,  0.0795]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[-0.0160, -0.0924,  0.4772],\n",
            "          [-0.2090, -0.5244,  0.3737],\n",
            "          [-0.3542, -0.3345, -0.1437],\n",
            "          ...,\n",
            "          [ 0.7404, -0.0925, -1.0260],\n",
            "          [ 0.5663, -0.0750, -1.0616],\n",
            "          [-0.1811, -0.4308, -0.9988]],\n",
            "\n",
            "         [[ 0.1578, -0.1709,  0.4610],\n",
            "          [-0.1531, -0.6482,  0.2887],\n",
            "          [-0.3144, -0.4880, -0.0565],\n",
            "          ...,\n",
            "          [ 0.5992, -0.1336, -1.0006],\n",
            "          [ 0.4985, -0.0940, -1.0548],\n",
            "          [-0.0796, -0.4069, -0.9933]],\n",
            "\n",
            "         [[ 0.2676, -0.2583,  0.4476],\n",
            "          [-0.1726, -0.7935,  0.2227],\n",
            "          [-0.3430, -0.6347,  0.0665],\n",
            "          ...,\n",
            "          [ 0.4239, -0.2342, -0.8766],\n",
            "          [ 0.3751, -0.1683, -0.9902],\n",
            "          [-0.0189, -0.3847, -0.9688]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1739,  0.3261,  0.7411],\n",
            "          [-0.4342,  0.1410,  0.0020],\n",
            "          [-0.3361, -0.0219, -0.6304],\n",
            "          ...,\n",
            "          [-0.2554,  0.2818, -0.1654],\n",
            "          [-0.1782,  0.0590,  0.0830],\n",
            "          [-0.2290, -0.3375,  0.0261]],\n",
            "\n",
            "         [[ 0.0366,  0.4145,  0.7315],\n",
            "          [-0.2155,  0.2318,  0.1387],\n",
            "          [-0.1377,  0.0489, -0.4159],\n",
            "          ...,\n",
            "          [-0.4150,  0.2204, -0.2269],\n",
            "          [-0.3044, -0.0537,  0.0882],\n",
            "          [-0.2342, -0.3759,  0.0847]],\n",
            "\n",
            "         [[ 0.1803,  0.5325,  0.7084],\n",
            "          [-0.0441,  0.3346,  0.2597],\n",
            "          [ 0.0160,  0.0990, -0.1758],\n",
            "          ...,\n",
            "          [-0.5078,  0.1504, -0.2502],\n",
            "          [-0.4021, -0.1506,  0.0524],\n",
            "          [-0.2165, -0.4266,  0.0795]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0047,  0.0316,  0.0563, -0.0020, -0.0007, -0.0570,  0.0465, -0.0427,\n",
            "          0.0932,  0.0031,  0.0872, -0.0883,  0.0793, -0.0361, -0.0580, -0.0791,\n",
            "         -0.0453, -0.0425,  0.0733,  0.0944, -0.0326, -0.0335,  0.0210,  0.0474,\n",
            "          0.0853, -0.0096, -0.0102,  0.0742,  0.0647,  0.0383,  0.0341, -0.0682,\n",
            "          0.1229, -0.0409,  0.0607,  0.1101, -0.1069,  0.0854, -0.0483,  0.0142,\n",
            "          0.0597, -0.0647,  0.0408,  0.0518,  0.0157,  0.0528, -0.1054,  0.0905,\n",
            "          0.0569,  0.0067,  0.0360, -0.0827, -0.0455, -0.0634,  0.0783,  0.0313,\n",
            "          0.0462,  0.0852,  0.0239,  0.0800,  0.0375, -0.0419,  0.0416,  0.0334,\n",
            "         -0.0821, -0.0098, -0.0379, -0.0781, -0.0823,  0.0042, -0.0108,  0.0587,\n",
            "          0.0280,  0.0533,  0.0321,  0.0029,  0.0858, -0.0908, -0.0017, -0.0810,\n",
            "          0.0434, -0.0581,  0.0476,  0.1126, -0.0670,  0.0740, -0.0539, -0.0886,\n",
            "          0.0305,  0.0207,  0.0441,  0.0134,  0.0441,  0.0760, -0.1076, -0.0885,\n",
            "          0.0244, -0.0103, -0.0786,  0.0164]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[-0.0460, -0.2017, -0.0920,  ...,  0.0517, -0.0170, -0.2423],\n",
            "         [-0.0464, -0.2099, -0.0848,  ...,  0.0505, -0.0345, -0.2342],\n",
            "         [-0.0472, -0.2119, -0.0759,  ...,  0.0450, -0.0579, -0.2140],\n",
            "         ...,\n",
            "         [-0.0493,  0.0795, -0.0033,  ..., -0.0448, -0.0531,  0.0485],\n",
            "         [-0.0502,  0.0672, -0.0155,  ..., -0.0436, -0.0583,  0.0534],\n",
            "         [-0.0508,  0.0535, -0.0275,  ..., -0.0397, -0.0580,  0.0492]]],\n",
            "       device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 0.2133, -0.5365, -0.0289],\n",
            "          [-0.2273, -0.3802,  0.1339],\n",
            "          [-0.1068, -0.5620, -0.0620],\n",
            "          ...,\n",
            "          [ 0.6818, -1.3160,  0.5638],\n",
            "          [ 0.7603, -1.2059,  0.2555],\n",
            "          [ 0.7075, -0.9044,  0.3579]],\n",
            "\n",
            "         [[ 0.2665, -0.5825,  0.1029],\n",
            "          [-0.2406, -0.4110,  0.2312],\n",
            "          [-0.2246, -0.5281,  0.0167],\n",
            "          ...,\n",
            "          [ 0.6064, -1.3269,  0.5784],\n",
            "          [ 0.6404, -1.1965,  0.2009],\n",
            "          [ 0.5452, -0.9016,  0.3347]],\n",
            "\n",
            "         [[ 0.3384, -0.6303,  0.2081],\n",
            "          [-0.1793, -0.4223,  0.2986],\n",
            "          [-0.2754, -0.5304,  0.0506],\n",
            "          ...,\n",
            "          [ 0.5175, -1.3169,  0.6016],\n",
            "          [ 0.4865, -1.1872,  0.1803],\n",
            "          [ 0.3373, -0.9530,  0.3020]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.5479, -0.9826, -0.2640],\n",
            "          [ 0.4247, -0.7350, -0.1976],\n",
            "          [ 0.0621, -0.5313,  0.2029],\n",
            "          ...,\n",
            "          [ 0.0670, -0.6118,  0.2296],\n",
            "          [ 0.0855, -0.5112,  0.5487],\n",
            "          [ 0.1967, -0.5407,  0.6121]],\n",
            "\n",
            "         [[ 0.6230, -1.0489, -0.4289],\n",
            "          [ 0.4425, -0.7291, -0.3201],\n",
            "          [ 0.0768, -0.6077,  0.1505],\n",
            "          ...,\n",
            "          [ 0.0945, -0.6447,  0.2614],\n",
            "          [-0.0589, -0.5310,  0.5509],\n",
            "          [ 0.0806, -0.6247,  0.7073]],\n",
            "\n",
            "         [[ 0.6900, -1.0577, -0.5490],\n",
            "          [ 0.4724, -0.6807, -0.4165],\n",
            "          [ 0.0934, -0.6465,  0.0869],\n",
            "          ...,\n",
            "          [ 0.0918, -0.6814,  0.3549],\n",
            "          [-0.1721, -0.5614,  0.5954],\n",
            "          [-0.0326, -0.7548,  0.8109]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 2.1330e-01, -2.2732e-01, -1.0684e-01,  ...,  2.2845e-01,\n",
            "            3.1261e-01,  1.4304e-01],\n",
            "          [ 2.7989e-01, -1.4478e-01,  1.6263e-01,  ...,  2.0321e-01,\n",
            "            1.5392e-01, -5.7760e-02],\n",
            "          [ 3.6942e-01,  3.1364e-01,  4.6357e-01,  ...,  3.0145e-01,\n",
            "           -3.1911e-01, -3.2349e-01],\n",
            "          ...,\n",
            "          [ 5.8458e-03,  1.7186e-01,  2.6255e-01,  ...,  2.8454e-01,\n",
            "           -3.9200e-02, -3.8163e-01],\n",
            "          [-2.1942e-01,  9.2435e-03,  4.6288e-01,  ...,  6.7721e-02,\n",
            "           -3.4854e-01, -4.2249e-01],\n",
            "          [-3.4181e-01,  4.2761e-01,  9.6151e-01,  ...,  9.1783e-02,\n",
            "           -1.7207e-01, -3.2570e-02]],\n",
            "\n",
            "         [[-5.3648e-01, -3.8018e-01, -5.6198e-01,  ..., -3.6818e-01,\n",
            "           -6.4670e-01, -6.8054e-01],\n",
            "          [-5.6670e-01, -2.8741e-01, -3.8258e-01,  ..., -2.2422e-01,\n",
            "           -5.6019e-01, -1.0231e+00],\n",
            "          [-8.1913e-01, -7.0081e-01, -6.3252e-01,  ..., -2.6812e-01,\n",
            "           -8.4943e-01, -1.5034e+00],\n",
            "          ...,\n",
            "          [-2.8971e-01, -4.1280e-02, -1.7341e-01,  ..., -4.9060e-01,\n",
            "           -4.3430e-01, -4.0603e-01],\n",
            "          [-5.8948e-01, -5.4616e-01, -3.7002e-01,  ..., -6.1788e-01,\n",
            "           -2.4263e-01, -4.1657e-01],\n",
            "          [-8.0690e-01, -9.3264e-01, -9.1840e-01,  ..., -6.8137e-01,\n",
            "           -5.6139e-01, -7.5478e-01]],\n",
            "\n",
            "         [[-2.8898e-02,  1.3387e-01, -6.2015e-02,  ..., -2.6794e-01,\n",
            "           -4.6053e-01, -7.7973e-01],\n",
            "          [ 1.5775e-01,  1.0423e-01, -2.1647e-01,  ..., -1.8217e-01,\n",
            "           -4.1170e-01, -8.7523e-01],\n",
            "          [ 4.2043e-01,  4.1146e-01, -1.8523e-01,  ...,  1.9336e-01,\n",
            "            2.6604e-02, -1.2132e-01],\n",
            "          ...,\n",
            "          [ 1.2237e-01,  7.4766e-02,  6.5239e-02,  ..., -3.0384e-01,\n",
            "            3.7360e-01,  3.2111e-01],\n",
            "          [ 1.5627e-01,  4.1958e-01,  4.0341e-01,  ..., -3.9139e-04,\n",
            "            4.1067e-01,  8.1098e-01],\n",
            "          [ 1.9674e-01,  9.3657e-01,  8.2238e-01,  ...,  3.5493e-01,\n",
            "            5.9545e-01,  8.1088e-01]]]], device='cuda:0',\n",
            "       grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 0.2133, -0.5365, -0.0289],\n",
            "          [-0.2273, -0.3802,  0.1339],\n",
            "          [-0.1068, -0.5620, -0.0620],\n",
            "          ...,\n",
            "          [ 0.6818, -1.3160,  0.5638],\n",
            "          [ 0.7603, -1.2059,  0.2555],\n",
            "          [ 0.7075, -0.9044,  0.3579]],\n",
            "\n",
            "         [[ 0.2665, -0.5825,  0.1029],\n",
            "          [-0.2406, -0.4110,  0.2312],\n",
            "          [-0.2246, -0.5281,  0.0167],\n",
            "          ...,\n",
            "          [ 0.6064, -1.3269,  0.5784],\n",
            "          [ 0.6404, -1.1965,  0.2009],\n",
            "          [ 0.5452, -0.9016,  0.3347]],\n",
            "\n",
            "         [[ 0.3384, -0.6303,  0.2081],\n",
            "          [-0.1793, -0.4223,  0.2986],\n",
            "          [-0.2754, -0.5304,  0.0506],\n",
            "          ...,\n",
            "          [ 0.5175, -1.3169,  0.6016],\n",
            "          [ 0.4865, -1.1872,  0.1803],\n",
            "          [ 0.3373, -0.9530,  0.3020]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.5479, -0.9826, -0.2640],\n",
            "          [ 0.4247, -0.7350, -0.1976],\n",
            "          [ 0.0621, -0.5313,  0.2029],\n",
            "          ...,\n",
            "          [ 0.0670, -0.6118,  0.2296],\n",
            "          [ 0.0855, -0.5112,  0.5487],\n",
            "          [ 0.1967, -0.5407,  0.6121]],\n",
            "\n",
            "         [[ 0.6230, -1.0489, -0.4289],\n",
            "          [ 0.4425, -0.7291, -0.3201],\n",
            "          [ 0.0768, -0.6077,  0.1505],\n",
            "          ...,\n",
            "          [ 0.0945, -0.6447,  0.2614],\n",
            "          [-0.0589, -0.5310,  0.5509],\n",
            "          [ 0.0806, -0.6247,  0.7073]],\n",
            "\n",
            "         [[ 0.6900, -1.0577, -0.5490],\n",
            "          [ 0.4724, -0.6807, -0.4165],\n",
            "          [ 0.0934, -0.6465,  0.0869],\n",
            "          ...,\n",
            "          [ 0.0918, -0.6814,  0.3549],\n",
            "          [-0.1721, -0.5614,  0.5954],\n",
            "          [-0.0326, -0.7548,  0.8109]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n",
            "w tensor([[ 0.0055,  0.0327,  0.0564, -0.0027, -0.0051, -0.0572,  0.0475, -0.0427,\n",
            "          0.0924,  0.0025,  0.0868, -0.0880,  0.0797, -0.0357, -0.0574, -0.0779,\n",
            "         -0.0446, -0.0423,  0.0719,  0.0952, -0.0309, -0.0355,  0.0200,  0.0473,\n",
            "          0.0859, -0.0122, -0.0098,  0.0748,  0.0640,  0.0370,  0.0358, -0.0700,\n",
            "          0.1247, -0.0421,  0.0623,  0.1100, -0.1070,  0.0851, -0.0472,  0.0128,\n",
            "          0.0600, -0.0648,  0.0412,  0.0534,  0.0168,  0.0541, -0.1044,  0.0894,\n",
            "          0.0565,  0.0067,  0.0356, -0.0821, -0.0447, -0.0629,  0.0780,  0.0300,\n",
            "          0.0454,  0.0862,  0.0247,  0.0790,  0.0374, -0.0410,  0.0413,  0.0331,\n",
            "         -0.0814, -0.0097, -0.0395, -0.0771, -0.0837,  0.0042, -0.0115,  0.0587,\n",
            "          0.0279,  0.0537,  0.0323,  0.0027,  0.0857, -0.0903, -0.0006, -0.0814,\n",
            "          0.0428, -0.0570,  0.0483,  0.1130, -0.0664,  0.0756, -0.0528, -0.0875,\n",
            "          0.0286,  0.0203,  0.0446,  0.0148,  0.0450,  0.0777, -0.1082, -0.0881,\n",
            "          0.0246, -0.0117, -0.0791,  0.0153]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "y, y size tensor([[[ 6.1882e-02, -2.3392e-02,  5.5624e-02,  ..., -2.8414e-01,\n",
            "          -2.6469e-02, -8.1528e-02],\n",
            "         [ 5.5155e-02, -2.2957e-02,  5.5741e-02,  ..., -2.9104e-01,\n",
            "          -1.6361e-02, -7.3514e-02],\n",
            "         [ 4.3680e-02, -2.3853e-02,  5.6349e-02,  ..., -2.8796e-01,\n",
            "          -3.3279e-03, -5.9598e-02],\n",
            "         ...,\n",
            "         [ 1.8541e-04, -5.9988e-02,  8.7395e-02,  ...,  5.5243e-03,\n",
            "          -3.2699e-02,  1.7048e-01],\n",
            "         [-7.8432e-03, -6.1303e-02,  8.5096e-02,  ...,  1.2471e-02,\n",
            "          -2.5761e-02,  1.5620e-01],\n",
            "         [-1.0597e-02, -6.1241e-02,  8.2231e-02,  ...,  1.2696e-02,\n",
            "          -2.1381e-02,  1.4115e-01]]], device='cuda:0', grad_fn=<AddBackward0>) torch.Size([1, 64, 384])\n",
            "patch tensor([[[[ 0.5538, -0.3796,  0.0627],\n",
            "          [ 0.9759, -0.2538, -0.0534],\n",
            "          [ 1.0816, -0.1878, -0.0496],\n",
            "          ...,\n",
            "          [-0.0855, -0.1996,  0.0759],\n",
            "          [-0.0908, -0.2973, -0.1572],\n",
            "          [ 0.3174, -0.2440, -0.1792]],\n",
            "\n",
            "         [[ 0.4472, -0.4885,  0.0890],\n",
            "          [ 1.0066, -0.3959, -0.0024],\n",
            "          [ 1.0996, -0.3642, -0.0092],\n",
            "          ...,\n",
            "          [-0.1826, -0.2445,  0.0661],\n",
            "          [-0.1292, -0.3701, -0.1749],\n",
            "          [ 0.3168, -0.2972, -0.2725]],\n",
            "\n",
            "         [[ 0.3250, -0.5562,  0.1317],\n",
            "          [ 1.0391, -0.4761,  0.0527],\n",
            "          [ 1.1239, -0.5715,  0.0500],\n",
            "          ...,\n",
            "          [-0.2870, -0.2107, -0.0217],\n",
            "          [-0.1413, -0.3733, -0.2302],\n",
            "          [ 0.3189, -0.3415, -0.3976]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2740, -0.2853,  0.0966],\n",
            "          [-0.1882, -0.7167,  0.3106],\n",
            "          [-0.2212, -0.5750,  0.5435],\n",
            "          ...,\n",
            "          [-0.2932,  0.3738, -0.1103],\n",
            "          [-0.5459,  0.4671, -0.3205],\n",
            "          [-0.4849,  0.4284, -0.6329]],\n",
            "\n",
            "         [[-0.2942, -0.2581,  0.2141],\n",
            "          [-0.1435, -0.7427,  0.4779],\n",
            "          [-0.2313, -0.7514,  0.6727],\n",
            "          ...,\n",
            "          [-0.3763,  0.4820, -0.1118],\n",
            "          [-0.4738,  0.5965, -0.3125],\n",
            "          [-0.4325,  0.5312, -0.7553]],\n",
            "\n",
            "         [[-0.3189, -0.1801,  0.3463],\n",
            "          [-0.1309, -0.7201,  0.6378],\n",
            "          [-0.2544, -0.8780,  0.7460],\n",
            "          ...,\n",
            "          [-0.4251,  0.5706, -0.1650],\n",
            "          [-0.3884,  0.7268, -0.3389],\n",
            "          [-0.3734,  0.6164, -0.8761]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "output after rearrange tensor([[[[ 0.5538,  0.9759,  1.0816,  ...,  0.6709,  1.0439,  1.1925],\n",
            "          [ 0.3476,  0.5003,  0.5572,  ...,  0.3129,  0.5611,  0.9532],\n",
            "          [ 0.0300, -0.0383,  0.0910,  ..., -0.3856,  0.0794,  0.5690],\n",
            "          ...,\n",
            "          [ 0.2071,  0.1141, -0.2000,  ...,  0.0393, -0.1481, -0.1040],\n",
            "          [-0.0385, -0.0482,  0.0441,  ...,  0.0020, -0.2459, -0.2244],\n",
            "          [-0.1434,  0.0579,  0.1820,  ..., -0.4251, -0.3884, -0.3734]],\n",
            "\n",
            "         [[-0.3796, -0.2538, -0.1878,  ..., -0.1998, -0.8386, -0.8744],\n",
            "          [-0.1760, -0.1600, -0.0853,  ...,  0.1912, -0.6310, -0.8674],\n",
            "          [-0.1962, -0.0973, -0.2711,  ...,  0.3415,  0.2663, -0.2715],\n",
            "          ...,\n",
            "          [ 0.0778, -0.0244,  0.3301,  ..., -0.3727, -0.3879, -0.2078],\n",
            "          [-0.3110, -0.2417,  0.3779,  ...,  0.1925,  0.5260,  0.3942],\n",
            "          [-0.2479, -0.1440,  0.1132,  ...,  0.5706,  0.7268,  0.6164]],\n",
            "\n",
            "         [[ 0.0627, -0.0534, -0.0496,  ...,  0.3995,  0.3661,  0.2298],\n",
            "          [ 0.3217,  0.1911, -0.1310,  ...,  0.3027, -0.0388, -0.0701],\n",
            "          [ 0.3399,  0.2709, -0.0556,  ...,  0.0430, -0.6439, -0.8772],\n",
            "          ...,\n",
            "          [ 0.7201,  0.3703,  0.1816,  ...,  0.1851,  0.2904, -0.0150],\n",
            "          [ 0.5252,  0.1568, -0.2288,  ..., -0.0256, -0.3654, -0.7973],\n",
            "          [-0.0380, -0.2934, -0.3152,  ..., -0.1650, -0.3389, -0.8761]]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward>) torch.Size([1, 3, 32, 32])\n",
            "output after sigmoid tensor([[[[ 0.5538, -0.3796,  0.0627],\n",
            "          [ 0.9759, -0.2538, -0.0534],\n",
            "          [ 1.0816, -0.1878, -0.0496],\n",
            "          ...,\n",
            "          [-0.0855, -0.1996,  0.0759],\n",
            "          [-0.0908, -0.2973, -0.1572],\n",
            "          [ 0.3174, -0.2440, -0.1792]],\n",
            "\n",
            "         [[ 0.4472, -0.4885,  0.0890],\n",
            "          [ 1.0066, -0.3959, -0.0024],\n",
            "          [ 1.0996, -0.3642, -0.0092],\n",
            "          ...,\n",
            "          [-0.1826, -0.2445,  0.0661],\n",
            "          [-0.1292, -0.3701, -0.1749],\n",
            "          [ 0.3168, -0.2972, -0.2725]],\n",
            "\n",
            "         [[ 0.3250, -0.5562,  0.1317],\n",
            "          [ 1.0391, -0.4761,  0.0527],\n",
            "          [ 1.1239, -0.5715,  0.0500],\n",
            "          ...,\n",
            "          [-0.2870, -0.2107, -0.0217],\n",
            "          [-0.1413, -0.3733, -0.2302],\n",
            "          [ 0.3189, -0.3415, -0.3976]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2740, -0.2853,  0.0966],\n",
            "          [-0.1882, -0.7167,  0.3106],\n",
            "          [-0.2212, -0.5750,  0.5435],\n",
            "          ...,\n",
            "          [-0.2932,  0.3738, -0.1103],\n",
            "          [-0.5459,  0.4671, -0.3205],\n",
            "          [-0.4849,  0.4284, -0.6329]],\n",
            "\n",
            "         [[-0.2942, -0.2581,  0.2141],\n",
            "          [-0.1435, -0.7427,  0.4779],\n",
            "          [-0.2313, -0.7514,  0.6727],\n",
            "          ...,\n",
            "          [-0.3763,  0.4820, -0.1118],\n",
            "          [-0.4738,  0.5965, -0.3125],\n",
            "          [-0.4325,  0.5312, -0.7553]],\n",
            "\n",
            "         [[-0.3189, -0.1801,  0.3463],\n",
            "          [-0.1309, -0.7201,  0.6378],\n",
            "          [-0.2544, -0.8780,  0.7460],\n",
            "          ...,\n",
            "          [-0.4251,  0.5706, -0.1650],\n",
            "          [-0.3884,  0.7268, -0.3389],\n",
            "          [-0.3734,  0.6164, -0.8761]]]], device='cuda:0',\n",
            "       grad_fn=<CopySlices>) torch.Size([1, 64, 16, 3])\n",
            "y size torch.Size([1, 65, 384])\n",
            "y size torch.Size([1, 65, 384])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-eb275f46c6fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Calculate gradients for G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Update G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYS2aTJxZfjR"
      },
      "source": [
        "# Results\n",
        "\n",
        "Finally, lets check out how we did. Here, we will look at three\n",
        "different results. First, we will see how D and G’s losses changed\n",
        "during training. Second, we will visualize G’s output on the fixed_noise\n",
        "batch for every epoch. And third, we will look at a batch of real data\n",
        "next to a batch of fake data from G.\n",
        "\n",
        "**Loss versus training iteration**\n",
        "\n",
        "Below is a plot of D & G’s losses versus training iterations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fHlZrfTZfjS"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nytxGFRZfjT"
      },
      "source": [
        "**Visualization of G’s progression**\n",
        "\n",
        "Remember how we saved the generator’s output on the fixed_noise batch\n",
        "after every epoch of training. Now, we can visualize the training\n",
        "progression of G with an animation. Press the play button to start the\n",
        "animation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLBNraojZfjT"
      },
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD94uqOaZfjT"
      },
      "source": [
        "**Real Images vs. Fake Images**\n",
        "\n",
        "Finally, lets take a look at some real images and fake images side by\n",
        "side.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBFyVIZcZfjU"
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhcDdHWZfjU"
      },
      "source": [
        "# Where to Go Next\n",
        "\n",
        "\n",
        "We have reached the end of our journey, but there are several places you\n",
        "could go from here. You could:\n",
        "\n",
        "-  Train for longer to see how good the results get\n",
        "-  Modify this model to take a different dataset and possibly change the\n",
        "   size of the images and the model architecture\n",
        "-  Check out some other cool GAN projects\n",
        "   `here <https://github.com/nashory/gans-awesome-applications>`__\n",
        "-  Create GANs that generate\n",
        "   `music <https://deepmind.com/blog/wavenet-generative-model-raw-audio/>`__\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA-667Xyw4Wj"
      },
      "source": [
        "# Additional Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm9f-Rg-lo1j"
      },
      "source": [
        "noise = torch.randn(2, 3, 2,2, device=device)\n",
        "noise2 = torch.randn(2, 6, device=device)\n",
        "l = torch.matmul(noise, noise2)\n",
        "\n",
        "\n",
        "# print(l.size())\n",
        "# print(noise.size())\n",
        "\n",
        "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "noise = torch.randn(3, 2, 1,1, device=device)\n",
        "layer = nn.Linear(1, 6)      \n",
        "l = layer(noise)\n",
        "# print(l.size())\n",
        "\n",
        "# 아래에서 에러가 나는 이유는 matrix multiplication 작동을 몰라서 그렇다.\n",
        "# 차원이 3개 이상인 경우 맨 마지막 두 차원을 제외한 앞의 차원들은 모두 배치 차원이 된다.\n",
        "# 즉, 뒤에 2 차원들 끼리는 matrix multiplication이 되도록 차원을 맞춰야 한다.\n",
        "\n",
        "noise = torch.randn(3, 2, 1,1, device=device)\n",
        "layer = nn.Linear(2, 6)      \n",
        "l = layer(noise)\n",
        "# print(l.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0usTw61w5c9"
      },
      "source": [
        "H = W = 32\n",
        "h = w = 4\n",
        "nh = H // h\n",
        "nw = W // w\n",
        "Ys = torch.randn(1, nh*nw, D)\n",
        "Ys = Ys.unsqueeze(1).repeat(1, 64, 1, 1)\n",
        "Ys.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAFRwJ6ew5c-"
      },
      "source": [
        "H = W = 32\n",
        "h = w = 4\n",
        "nh = H // h\n",
        "nw = W // w\n",
        "B = 128\n",
        "\n",
        "h_basis = torch.linspace(0, nh-1, nh).div(nh-1).mul(2).sub(1)\n",
        "w_basis = torch.linspace(0, nw-1, nw).div(nw-1).mul(2).sub(1)\n",
        "coords2d = torch.meshgrid(h_basis, w_basis)\n",
        "# print(coords2d)\n",
        "\n",
        "coords2d = torch.stack(coords2d, 0).reshape(2, nh*nw).t()\n",
        "coords2d_patch_position = coords2d.unsqueeze(0).repeat(B, 1, 1)\n",
        "# print(coords2d_patch_position.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfprGJsYw5c-"
      },
      "source": [
        "coords_new = rearrange(coords2d, '(nh h nw w) D -> (nh nw) (h w) D', nh=nh, h=h, nw=nw, w=w, D=2)\n",
        "# print(coords_new.unsqueeze(0).repeat(B, 1, 1, 1).shape)\n",
        "\n",
        "coords_new = repeat(coords2d, '(nh h nw w) D -> B (nh nw) (h w) D', B=B, nh=nh, h=h, nw=nw, w=w, D=2).shape # nh h nw w 인지 어떻게 알까?\n",
        "# print(coords_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSp8uTKo-Nqv"
      },
      "source": [
        "이미지를 불러와서 아래와 같이 reshape 해보고, 이미지 그대로 복원되는지 보기.\n",
        " \n",
        "\n",
        "3,64,64\n",
        "\n",
        "4096, 3\n",
        "\n",
        "3, 64, 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxwpTqXjXV4p"
      },
      "source": [
        "D = 256\n",
        "L = 10 # num tokens\n",
        "B = 4 # num batch\n",
        "net = nn.Linear(D, D)\n",
        "\n",
        "input = torch.randn(B, L, D)\n",
        "input.bmm(input.permute(0, 2, 1)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07k-QxKaqMM"
      },
      "source": [
        "class CustomNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Linear(10, 20),\n",
        "        nn.Conv2d(3, 3, 1, 1, 1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.features2 = nn.Sequential(\n",
        "        nn.Linear(10, 20),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.apply(self.__init_weight__)\n",
        "  def forward(self, x):\n",
        "    return None\n",
        "\n",
        "  def __init_weight__(self, m):\n",
        "    if isinstance(m, (nn.Linear,)):\n",
        "      nn.utils.spectral_norm(m)\n",
        "\n",
        "net = CustomNet()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}